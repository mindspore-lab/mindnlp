# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
# pylint: disable=C0301
# pylint: disable=W4902
"""Test ChatGLM"""
import random
import unittest
import pytest
import numpy as np

import mindspore
from mindspore import Tensor

from mindnlp.models.glm.chatglm import ChatGLMForConditionalGeneration
from mindnlp.transforms.tokenizers import ChatGLMTokenizer

def set_random_seed(seed):
    """set random seed"""
    random.seed(seed)

    # mindspore RNGs
    mindspore.set_seed(seed)

    # numpy RNG
    np.random.seed(seed)


def ids_tensor(shape, vocab_size):
    """Creates a random int32 tensor of the shape within the vocab size"""
    total_dims = 1
    for dim in shape:
        total_dims *= dim

    values = []
    for _ in range(total_dims):
        values.append(random.randint(0, vocab_size - 1))

    return mindspore.Tensor(values, dtype=mindspore.int64).view(shape)


def get_model_and_tokenizer():
    """get model and tokenizer"""
    model = ChatGLMForConditionalGeneration.from_pretrained("chatglm-6b")

    tokenizer = ChatGLMTokenizer.from_pretrained("chatglm-6b")
    return model, tokenizer


class ChatGLMGenerationTest(unittest.TestCase):
    """ChatGLM generation test."""
    @pytest.mark.skipif(True, reason="not ready")
    def test_chat(self):
        """test chat"""
        model, tokenizer = get_model_and_tokenizer()
        prompts = ["ä½ å¥½", "ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦", "å®ƒåˆ›å»ºäºå“ªä¸€å¹´"]
        history = []
        set_random_seed(42)
        expected_responses = [
            'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
            'æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦ï¼Œä½äºä¸­å›½åŒ—äº¬å¸‚æµ·æ·€åŒºï¼Œåˆ›å»ºäº 1911 å¹´ï¼Œå‰èº«æ˜¯æ¸…åå­¦å ‚ã€‚ä½œä¸ºæˆ‘å›½é¡¶å°–é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œæ¸…åå¤§å­¦åœ¨ç§‘å­¦ç ”ç©¶ã€å·¥ç¨‹æŠ€æœ¯ã€ä¿¡æ¯æŠ€æœ¯ã€ç»æµç®¡ç†ç­‰é¢†åŸŸå¤„äºé¢†å…ˆåœ°ä½ï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€è‘—åçš„å·¥ç¨‹å­¦åºœä¹‹ä¸€ã€‚\n\næ¸…åå¤§å­¦æ‹¥æœ‰ä¸–ç•Œä¸€æµçš„æ•™å­¦è®¾æ–½å’Œç§‘å­¦ç ”ç©¶å¹³å°ï¼Œè®¾æœ‰å¤šä¸ªå­¦é™¢å’Œç ”ç©¶ä¸­å¿ƒï¼ŒåŒ…æ‹¬å·¥ç¨‹å­¦é™¢ã€è‡ªç„¶ç§‘å­¦å­¦é™¢ã€ç¤¾ä¼šç§‘å­¦å­¦é™¢ã€äººæ–‡å­¦é™¢ã€æ³•å­¦é™¢ã€ç»æµç®¡ç†å­¦é™¢ç­‰ã€‚å­¦æ ¡æ‹¥æœ‰ä¼—å¤šçŸ¥åæ•™æˆå’Œç ”ç©¶å›¢é˜Ÿï¼Œå…¶ä¸­åŒ…æ‹¬å¤šä½é™¢å£«ã€å›½å®¶æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘è·å¾—è€…ã€é•¿æ±Ÿå­¦è€…ç­‰ã€‚\n\næ¸…åå¤§å­¦çš„æœ¬ç§‘ç”Ÿæ‹›ç”ŸèŒƒå›´ä¸ºå…¨å›½ä¸­å­¦æ¯•ä¸šç”Ÿï¼Œæœ¬ç§‘ç”Ÿå…¥å­¦è¦æ±‚ä¸¥æ ¼ï¼Œè€ƒè¯•æˆç»©ä¼˜ç§€ã€‚åŒæ—¶ï¼Œæ¸…åå¤§å­¦ä¹Ÿæä¾›ç ”ç©¶ç”Ÿå’Œåšå£«ç”Ÿæ‹›ç”Ÿï¼ŒåŒ…æ‹¬ç¡•å£«ç ”ç©¶ç”Ÿå’Œåšå£«ç ”ç©¶ç”Ÿã€‚',
            'æ¸…åå¤§å­¦åˆ›å»ºäº 1911 å¹´ã€‚'
        ]
        for (prompt, expected_response) in zip(prompts, expected_responses):
            response, history = model.chat(tokenizer, prompt, history=history)
            print(repr(response))
            self.assertEquals(expected_response, response)

    @pytest.mark.skipif(True, reason="not ready")
    def test_stream_chat(self):
        """test steam chat"""
        model, tokenizer = get_model_and_tokenizer()
        prompts = ["ä½ å¥½", "ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦", "å®ƒåˆ›å»ºäºå“ªä¸€å¹´"]
        history = []
        expected_responses = [
            'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
            'æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦ï¼Œä½äºä¸­å›½åŒ—äº¬å¸‚æµ·æ·€åŒºï¼Œåˆ›å»ºäº 1911 å¹´ï¼Œå‰èº«æ˜¯æ¸…åå­¦å ‚ã€‚ä½œä¸ºæˆ‘å›½é¡¶å°–é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œæ¸…åå¤§å­¦åœ¨ç§‘å­¦ç ”ç©¶ã€å·¥ç¨‹æŠ€æœ¯ã€ä¿¡æ¯æŠ€æœ¯ã€ç»æµç®¡ç†ç­‰é¢†åŸŸå¤„äºé¢†å…ˆåœ°ä½ï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€è‘—åçš„å·¥ç¨‹å­¦åºœä¹‹ä¸€ã€‚\n\næ¸…åå¤§å­¦æ‹¥æœ‰ä¸–ç•Œä¸€æµçš„æ•™å­¦è®¾æ–½å’Œç§‘å­¦ç ”ç©¶å¹³å°ï¼Œè®¾æœ‰å¤šä¸ªå­¦é™¢å’Œç ”ç©¶ä¸­å¿ƒï¼ŒåŒ…æ‹¬å·¥ç¨‹å­¦é™¢ã€è‡ªç„¶ç§‘å­¦å­¦é™¢ã€ç¤¾ä¼šç§‘å­¦å­¦é™¢ã€äººæ–‡å­¦é™¢ã€æ³•å­¦é™¢ã€ç»æµç®¡ç†å­¦é™¢ç­‰ã€‚å­¦æ ¡æ‹¥æœ‰ä¼—å¤šçŸ¥åæ•™æˆå’Œç ”ç©¶å›¢é˜Ÿï¼Œå…¶ä¸­åŒ…æ‹¬å¤šä½é™¢å£«ã€å›½å®¶æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘è·å¾—è€…ã€é•¿æ±Ÿå­¦è€…ç­‰ã€‚\n\næ¸…åå¤§å­¦çš„æœ¬ç§‘ç”Ÿæ‹›ç”ŸèŒƒå›´ä¸ºå…¨å›½ä¸­å­¦æ¯•ä¸šç”Ÿï¼Œæœ¬ç§‘ç”Ÿå…¥å­¦è¦æ±‚ä¸¥æ ¼ï¼Œè€ƒè¯•æˆç»©ä¼˜ç§€ã€‚åŒæ—¶ï¼Œæ¸…åå¤§å­¦ä¹Ÿæä¾›ç ”ç©¶ç”Ÿå’Œåšå£«ç”Ÿæ‹›ç”Ÿï¼ŒåŒ…æ‹¬ç¡•å£«ç ”ç©¶ç”Ÿå’Œåšå£«ç ”ç©¶ç”Ÿã€‚',
            'æ¸…åå¤§å­¦åˆ›å»ºäº 1911 å¹´ã€‚'
        ]
        set_random_seed(42)
        for prompt, expected_response in zip(prompts, expected_responses):
            response = ""
            for _, (response, history) in enumerate(model.stream_chat(tokenizer, prompt, history=history)):
                pass
            print(repr(response))
            self.assertEquals(expected_response, response)

    @pytest.mark.download
    def test_generation(self):
        """test_generation"""
        model, tokenizer = get_model_and_tokenizer()
        parameters = [
                    ("æ™šä¸Šç¡ä¸ç€æ€ä¹ˆåŠ", False, 2048, 1, True, 4),
                    ("ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦", False, 64, 1, False, 1),
                    ("æ¨èå‡ ä¸ªç”µå½±", False, 2048, 1, True, 4),
                    ("æ€ä¹ˆç”¨Pytorchå†™ä¸€ä¸ªæ¨¡å‹ï¼Ÿ", False, 2048, 1, True, 4),
                    #   (True, 2048, 1),
                    #   (True, 64, 1),
                    #   (True, 2048, 4)
                      ]
        for sentence, do_sample, max_length, num_beams, use_bucket, bucket_num in parameters:
            set_random_seed(42)
            inputs = tokenizer(sentence)
            inputs = Tensor([inputs])
            outputs = model.generate(
                inputs,
                do_sample=do_sample,
                max_length=max_length,
                num_beams=num_beams,
                jit=True,
                use_bucket=use_bucket,
                bucket_num=bucket_num
            )

            outputs = outputs.asnumpy().tolist()[0]
            out_sentence = tokenizer.decode(outputs, skip_special_tokens=True)
            print(out_sentence)

    @pytest.mark.skipif(True, reason="not ready")
    def test_batch_generation(self):
        """test batch generation"""
        model, tokenizer = get_model_and_tokenizer()
        sentences = [
            "ä½ å¥½",
            "ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦"
        ]
        parameters = [(False, 2048, 1),
                      (False, 64, 1),
                      (True, 2048, 1),
                      (True, 64, 1),
                      (True, 2048, 4)]
        expected_out_sentences = [
            ['ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
             'ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦ æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§å¤§å­¦,ä½äºåŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯30å·,å…¶å†å²å¯ä»¥è¿½æº¯åˆ°1911å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚,1925å¹´æ›´åä¸ºæ¸…åå­¦æ ¡,1937å¹´æŠ—æ—¥æˆ˜äº‰å…¨é¢çˆ†å‘åå—è¿é•¿æ²™,1946å¹´è¿å›æ¸…åå›­ã€‚æ–°ä¸­å›½æˆç«‹å,æ¸…åå­¦æ ¡æ›´åä¸ºæ¸…åå¤§å­¦ã€‚\n\næ¸…åå¤§å­¦æ˜¯ä¸­å›½æœ€é¡¶å°–çš„å¤§å­¦ä¹‹ä¸€,åœ¨å·¥ç¨‹ã€ç§‘å­¦ã€æŠ€æœ¯ã€ç»æµã€ç®¡ç†ç­‰é¢†åŸŸéƒ½æœ‰å¾ˆé«˜çš„å­¦æœ¯å£°èª‰å’Œå½±å“åŠ›ã€‚å­¦æ ¡æ‹¥æœ‰ä¸–ç•Œä¸€æµçš„æ•™å­¦è®¾æ–½å’Œç§‘å­¦ç ”ç©¶å¹³å°,æœ‰å¤šä¸ªå­¦é™¢å’Œç ”ç©¶ä¸­å¿ƒ,åŒ…æ‹¬å·¥ç¨‹å­¦é™¢ã€è‡ªç„¶ç§‘å­¦å­¦é™¢ã€äººæ–‡å­¦é™¢ã€ç¤¾ä¼šç§‘å­¦å­¦é™¢ã€ç»æµç®¡ç†å­¦é™¢ã€æ³•å­¦é™¢ã€ç¾æœ¯å­¦é™¢ã€åŒ»å­¦é™¢ã€å™¨å­¦é™¢ç­‰ã€‚\n\næ¸…åå¤§å­¦çš„æœ¬ç§‘ç”Ÿæ‹›ç”Ÿå§‹äº2000å¹´,å®è¡Œå…¨é¢äºŒå­©æ”¿ç­–å,æœ¬ç§‘ç”Ÿæ‹›ç”Ÿè§„æ¨¡ä¸æ–­æ‰©å¤§ã€‚æˆªè‡³2022å¹´,æ¸…åå¤§å­¦å…±æœ‰æœ¬ç§‘ç”Ÿè¿‘3ä¸‡äºº,ç ”ç©¶ç”Ÿè¿‘2ä¸‡äºº,å…¶ä¸­å›½é™…å­¦ç”Ÿå æ¯”çº¦ä¸º10%ã€‚æ¸…åå¤§å­¦çš„æœ¬ç§‘ç”Ÿæ•™è‚²æ³¨é‡é€šè¯†æ•™è‚²å’Œä¸ªæ€§åŒ–åŸ¹å…»,å¼ºè°ƒå®è·µã€åˆ›æ–°ã€å›½é™…åŒ–å’Œç»¼åˆç´ è´¨ã€‚'],
            [
                'ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
                'ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦ æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§å¤§å­¦,ä½äºåŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯30å·,å…¶å†å²å¯ä»¥è¿½æº¯åˆ°1911å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚,1925å¹´æ›´åä¸ºæ¸…åå­¦æ ¡,1937å¹´æŠ—æ—¥æˆ˜äº‰å…¨é¢çˆ†å‘åå—è¿é•¿æ²™,1946å¹´è¿å›'
            ],
            [
                'ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
                'ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦ æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦,ä½äºåŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯ 30 å·,å…¶æº¯æºäº 1911 å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚, 1925 å¹´æ›´åä¸ºæ¸…åå­¦æ ¡, 1937 å¹´ç§‹æŠ—æ—¥æˆ˜äº‰å…¨é¢çˆ†å‘åé—­æ ¡ã€‚1949 å¹´ 10 æœˆå¼€å­¦å¤æ ¡,æˆä¸ºæˆ‘å›½ç¬¬ä¸€ä¸ªç¤¾ä¼šä¸»ä¹‰å¤§å­¦ç”Ÿæ´»äº†çš„é«˜æ ¡ã€‚æˆªè‡³ 2023 å¹´,æ¸…åå­¦æ ¡å…±ç®¡è¾– 2 ä¸ªå­¦é™¢ã€13 ä¸ªç³»,æœ‰æœ¬ç§‘ä¸“ä¸š 60 ä¸ª,ç ”ç©¶ç”Ÿä¸“ä¸š 190 ä¸ªã€‚'
            ],
            [
                'ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
                'ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦ æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦,ä½äºåŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯ 30 å·,å…¶æº¯æºäº 1911 å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚, 1925 å¹´æ›´åä¸ºæ¸…åå­¦æ ¡, 1937 å¹´ç§‹æŠ—æ—¥æˆ˜äº‰å…¨é¢çˆ†å‘å'
            ],
            [
                'ä½ å¥½ ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',
                'ä»‹ç»ä¸€ä¸‹æ¸…åå¤§å­¦ æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦,ä½äºåŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯30å·,å…¶å†å²å¯ä»¥è¿½æº¯åˆ°1911å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚,1925å¹´æ›´åä¸ºæ¸…åå­¦æ ¡,1937å¹´æŠ—æ—¥æˆ˜äº‰å…¨é¢çˆ†å‘åå—è¿é•¿æ²™,ä¸åŒ—äº¬å¤§å­¦ã€å—å¼€å¤§å­¦ç»„å»ºå›½ç«‹é•¿æ²™ä¸´æ—¶å¤§å­¦,1938å¹´è¿è‡³ æ˜†æ˜æ”¹åä¸ºå›½ç«‹è¥¿å—è”åˆå¤§å­¦,1946å¹´è¿å›åŒ—äº¬ã€‚æ–°ä¸­å›½æˆç«‹å,æ¸…åå­¦æ ¡æ›´åä¸ºæ¸…åå¤§å­¦ã€‚'
            ]
        ]
        for (do_sample, max_length, num_beams), expected_output_sentence in zip(parameters, expected_out_sentences):
            set_random_seed(42)
            inputs = tokenizer(sentences, padding=True)
            inputs = Tensor(inputs)
            outputs = model.generate(
                **inputs,
                do_sample=do_sample,
                max_length=max_length,
                num_beams=num_beams
            )

            batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            print(batch_out_sentence)
            self.assertListEqual(expected_output_sentence, batch_out_sentence)
