"""
OCRæ¨¡å‹è¯„ä¼°æµ‹è¯• - è®¡ç®—CERå’ŒWERæŒ‡æ ‡

ä½¿ç”¨æ–¹æ³•:
    pytest tests/ocr/test_evaluate_model.py
    python tests/ocr/test_evaluate_model.py --model_name "Qwen/Qwen2-VL-7B-Instruct" --test_data test_data.json
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import logging
import time
from tqdm import tqdm

# æ·»åŠ mindnlpåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from mindnlp.ocr.core.engine import VLMOCREngine

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def calculate_cer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate)
    
    CER = (S + D + I) / N
    å…¶ä¸­: S=æ›¿æ¢æ•°, D=åˆ é™¤æ•°, I=æ’å…¥æ•°, N=å‚è€ƒæ–‡æœ¬å­—ç¬¦æ•°
    """
    if not reference:
        return 1.0 if hypothesis else 0.0
    
    # ä½¿ç”¨ç¼–è¾‘è·ç¦»ç®—æ³• (Levenshteinè·ç¦»)
    len_ref = len(reference)
    len_hyp = len(hypothesis)
    
    # åˆå§‹åŒ–è·ç¦»çŸ©é˜µ
    dp = [[0] * (len_hyp + 1) for _ in range(len_ref + 1)]
    
    # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
    for i in range(len_ref + 1):
        dp[i][0] = i
    for j in range(len_hyp + 1):
        dp[0][j] = j
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»
    for i in range(1, len_ref + 1):
        for j in range(1, len_hyp + 1):
            if reference[i-1] == hypothesis[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # åˆ é™¤
                    dp[i][j-1] + 1,    # æ’å…¥
                    dp[i-1][j-1] + 1   # æ›¿æ¢
                )
    
    edit_distance = dp[len_ref][len_hyp]
    cer = edit_distance / len_ref if len_ref > 0 else 0.0
    
    return cer


def calculate_wer(reference: str, hypothesis: str) -> float:
    """
    è®¡ç®—è¯é”™è¯¯ç‡ (Word Error Rate)
    
    WER = (S + D + I) / N
    å…¶ä¸­: S=æ›¿æ¢æ•°, D=åˆ é™¤æ•°, I=æ’å…¥æ•°, N=å‚è€ƒæ–‡æœ¬è¯æ•°
    """
    ref_words = reference.split()
    hyp_words = hypothesis.split()
    
    if not ref_words:
        return 1.0 if hyp_words else 0.0
    
    len_ref = len(ref_words)
    len_hyp = len(hyp_words)
    
    # åˆå§‹åŒ–è·ç¦»çŸ©é˜µ
    dp = [[0] * (len_hyp + 1) for _ in range(len_ref + 1)]
    
    # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
    for i in range(len_ref + 1):
        dp[i][0] = i
    for j in range(len_hyp + 1):
        dp[0][j] = j
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»
    for i in range(1, len_ref + 1):
        for j in range(1, len_hyp + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # åˆ é™¤
                    dp[i][j-1] + 1,    # æ’å…¥
                    dp[i-1][j-1] + 1   # æ›¿æ¢
                )
    
    edit_distance = dp[len_ref][len_hyp]
    wer = edit_distance / len_ref if len_ref > 0 else 0.0
    
    return wer


def normalize_text(text: str) -> str:
    """
    å½’ä¸€åŒ–æ–‡æœ¬ - ç§»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦ç­‰
    """
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = ' '.join(text.split())
    return text.strip()


def load_test_data(test_data_path: str) -> List[Dict]:
    """
    åŠ è½½æµ‹è¯•æ•°æ®
    """
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    logger.info(f"âœ… åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return test_data


def evaluate_model(
    model_name: str,
    test_data: List[Dict],
    lora_path: str = None,
    device: str = "npu:0"
) -> Dict:
    """
    è¯„ä¼°OCRæ¨¡å‹æ€§èƒ½
    
    Returns:
        {
            "average_cer": float,
            "average_wer": float,
            "total_samples": int,
            "total_time": float,
            "avg_time_per_sample": float,
            "detailed_results": List[Dict]
        }
    """
    logger.info(f"ğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
    if lora_path:
        logger.info(f"ğŸ”§ ä½¿ç”¨LoRAæƒé‡: {lora_path}")
    
    # åˆå§‹åŒ–OCRå¼•æ“
    logger.info("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    engine = VLMOCREngine(
        model_name=model_name,
        lora_weights_path=lora_path,
        device=device
    )
    logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è¯„ä¼°ç»“æœ
    results = {
        "total_samples": len(test_data),
        "successful_samples": 0,
        "failed_samples": 0,
        "total_cer": 0.0,
        "total_wer": 0.0,
        "total_time": 0.0,
        "detailed_results": []
    }
    
    # é€ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°
    for idx, sample in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
        image_path = sample["image_path"]
        ground_truth = normalize_text(sample["ground_truth"])
        
        try:
            # æ‰§è¡ŒOCRè¯†åˆ«
            start_time = time.time()
            prediction = engine.inference(
                image_path=image_path,
                prompt="è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹"
            )
            inference_time = time.time() - start_time
            
            # å½’ä¸€åŒ–é¢„æµ‹ç»“æœ
            prediction = normalize_text(prediction)
            
            # è®¡ç®—CERå’ŒWER
            cer = calculate_cer(ground_truth, prediction)
            wer = calculate_wer(ground_truth, prediction)
            
            # è®°å½•ç»“æœ
            results["total_cer"] += cer
            results["total_wer"] += wer
            results["total_time"] += inference_time
            results["successful_samples"] += 1
            
            results["detailed_results"].append({
                "sample_id": idx,
                "image_path": image_path,
                "ground_truth": ground_truth,
                "prediction": prediction,
                "cer": cer,
                "wer": wer,
                "inference_time": inference_time
            })
            
        except Exception as e:
            logger.error(f"âŒ æ ·æœ¬ {idx} è¯„ä¼°å¤±è´¥: {str(e)}")
            results["failed_samples"] += 1
            results["detailed_results"].append({
                "sample_id": idx,
                "image_path": image_path,
                "error": str(e)
            })
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    if results["successful_samples"] > 0:
        results["average_cer"] = results["total_cer"] / results["successful_samples"]
        results["average_wer"] = results["total_wer"] / results["successful_samples"]
        results["avg_time_per_sample"] = results["total_time"] / results["successful_samples"]
    else:
        results["average_cer"] = 1.0
        results["average_wer"] = 1.0
        results["avg_time_per_sample"] = 0.0
    
    return results


def print_results(results: Dict, output_file: str = None):
    """
    æ‰“å°è¯„ä¼°ç»“æœ
    """
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*80)
    print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
    print(f"æˆåŠŸæ ·æœ¬: {results['successful_samples']}")
    print(f"å¤±è´¥æ ·æœ¬: {results['failed_samples']}")
    print(f"\nå¹³å‡CER (å­—ç¬¦é”™è¯¯ç‡): {results['average_cer']:.4f} ({results['average_cer']*100:.2f}%)")
    print(f"å¹³å‡WER (è¯é”™è¯¯ç‡): {results['average_wer']:.4f} ({results['average_wer']*100:.2f}%)")
    print(f"\næ€»æ¨ç†æ—¶é—´: {results['total_time']:.2f} ç§’")
    print(f"å¹³å‡æ¯æ ·æœ¬æ—¶é—´: {results['avg_time_per_sample']:.2f} ç§’")
    print("="*80)
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def compare_models(base_results: Dict, lora_results: Dict):
    """
    æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒLoRAå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½
    """
    print("\n" + "="*80)
    print("ğŸ“ˆ æ¨¡å‹å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    base_cer = base_results['average_cer']
    lora_cer = lora_results['average_cer']
    cer_improvement = (base_cer - lora_cer) / base_cer * 100
    
    base_wer = base_results['average_wer']
    lora_wer = lora_results['average_wer']
    wer_improvement = (base_wer - lora_wer) / base_wer * 100
    
    print(f"åŸºç¡€æ¨¡å‹ CER: {base_cer:.4f} ({base_cer*100:.2f}%)")
    print(f"LoRAæ¨¡å‹ CER: {lora_cer:.4f} ({lora_cer*100:.2f}%)")
    print(f"CERæ”¹è¿›: {cer_improvement:+.2f}%")
    
    print(f"\nåŸºç¡€æ¨¡å‹ WER: {base_wer:.4f} ({base_wer*100:.2f}%)")
    print(f"LoRAæ¨¡å‹ WER: {lora_wer:.4f} ({lora_wer*100:.2f}%)")
    print(f"WERæ”¹è¿›: {wer_improvement:+.2f}%")
    
    base_time = base_results['avg_time_per_sample']
    lora_time = lora_results['avg_time_per_sample']
    time_change = (lora_time - base_time) / base_time * 100
    
    print(f"\nåŸºç¡€æ¨¡å‹å¹³å‡æ—¶é—´: {base_time:.2f}ç§’")
    print(f"LoRAæ¨¡å‹å¹³å‡æ—¶é—´: {lora_time:.2f}ç§’")
    print(f"æ—¶é—´å˜åŒ–: {time_change:+.2f}%")
    
    print("="*80)
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³Issue #2379è¦æ±‚
    print("\nâœ… Issue #2379 è¦æ±‚æ£€æŸ¥:")
    if cer_improvement >= 20:
        print(f"âœ… CERé™ä½ {cer_improvement:.2f}% >= 20% (æ»¡è¶³è¦æ±‚)")
    else:
        print(f"âŒ CERé™ä½ {cer_improvement:.2f}% < 20% (æœªæ»¡è¶³è¦æ±‚)")


def main():
    parser = argparse.ArgumentParser(description="OCRæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model_name", type=str, required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--lora_path", type=str, default=None, help="LoRAæƒé‡è·¯å¾„")
    parser.add_argument("--test_data", type=str, required=True, help="æµ‹è¯•æ•°æ®JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, default="npu:0", help="è®¾å¤‡ (npu:0 or cpu)")
    parser.add_argument("--output", type=str, default=None, help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--compare_with", type=str, default=None, help="ä¸å¦ä¸€ä¸ªç»“æœæ–‡ä»¶å¯¹æ¯”")
    
    args = parser.parse_args()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(args.test_data)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(
        model_name=args.model_name,
        test_data=test_data,
        lora_path=args.lora_path,
        device=args.device
    )
    
    # æ‰“å°å’Œä¿å­˜ç»“æœ
    output_file = args.output or f"eval_results_{int(time.time())}.json"
    print_results(results, output_file)
    
    # å¦‚æœæŒ‡å®šäº†å¯¹æ¯”æ–‡ä»¶ï¼Œè¿›è¡Œå¯¹æ¯”åˆ†æ
    if args.compare_with:
        with open(args.compare_with, 'r', encoding='utf-8') as f:
            compare_results = json.load(f)
        compare_models(compare_results, results)


if __name__ == "__main__":
    main()
