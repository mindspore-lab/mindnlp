"""
å¤šåœºæ™¯OCRæµ‹è¯• - æµ‹è¯•è¡¨æ ¼ã€å…¬å¼ã€æ‰‹å†™ä½“ç­‰åœºæ™¯

ä½¿ç”¨æ–¹æ³•:
    pytest tests/ocr/test_multi_scenario.py
    python tests/ocr/test_multi_scenario.py --model_name "Qwen/Qwen2-VL-7B-Instruct" --scenario table
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import logging
import time
from datetime import datetime

# æ·»åŠ mindnlpåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from mindnlp.ocr.core.engine import VLMOCREngine

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MultiScenarioTester:
    """å¤šåœºæ™¯OCRæµ‹è¯•å™¨"""
    
    SCENARIO_CONFIGS = {
        "table": {
            "name": "è¡¨æ ¼è¯†åˆ«",
            "description": "æµ‹è¯•ç»“æ„åŒ–è¡¨æ ¼çš„è¯†åˆ«å‡†ç¡®ç‡",
            "prompt": "è¯·è¯†åˆ«å›¾åƒä¸­çš„è¡¨æ ¼å†…å®¹ï¼Œä¿æŒè¡¨æ ¼ç»“æ„",
            "target_accuracy": 0.95,  # Issue #2379è¦æ±‚: è¡¨æ ¼è¯†åˆ«ç²¾åº¦æå‡è‡³95%ä»¥ä¸Š
            "metrics": ["structure_accuracy", "content_accuracy"]
        },
        "formula": {
            "name": "å…¬å¼è¯†åˆ«",
            "description": "æµ‹è¯•æ•°å­¦å…¬å¼ã€LaTeXè¡¨è¾¾å¼çš„è¯†åˆ«",
            "prompt": "è¯·è¯†åˆ«å›¾åƒä¸­çš„æ•°å­¦å…¬å¼ï¼Œè¾“å‡ºLaTeXæ ¼å¼",
            "target_accuracy": 0.90,  # Issue #2379è¦æ±‚: å…¬å¼è¯†åˆ«ç²¾åº¦æå‡è‡³90%ä»¥ä¸Š
            "metrics": ["latex_accuracy", "symbol_accuracy"]
        },
        "handwriting": {
            "name": "æ‰‹å†™ä½“è¯†åˆ«",
            "description": "æµ‹è¯•æ‰‹å†™æ–‡å­—çš„è¯†åˆ«èƒ½åŠ›",
            "prompt": "è¯·è¯†åˆ«å›¾åƒä¸­çš„æ‰‹å†™æ–‡å­—",
            "target_accuracy": 0.85,
            "metrics": ["cer", "wer"]
        },
        "mixed": {
            "name": "æ··åˆåœºæ™¯",
            "description": "æµ‹è¯•åŒ…å«å¤šç§è¯­è¨€ã€æ ¼å¼æ··åˆçš„å¤æ‚æ–‡æ¡£",
            "prompt": "è¯·è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡å­—ã€æ•°å­—ã€ç¬¦å·",
            "target_accuracy": 0.90,
            "metrics": ["overall_accuracy"]
        },
        "business_doc": {
            "name": "å•†ä¸šæ–‡æ¡£",
            "description": "æµ‹è¯•è¥ä¸šæ‰§ç…§ã€å‘ç¥¨ã€åˆåŒç­‰å•†ä¸šæ–‡æ¡£",
            "prompt": "è¯·è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰ä¿¡æ¯å­—æ®µ",
            "target_accuracy": 0.95,
            "metrics": ["field_extraction", "accuracy"]
        }
    }
    
    def __init__(self, model_name: str, lora_path: str = None, device: str = "npu:0"):
        self.model_name = model_name
        self.lora_path = lora_path
        self.device = device
        self.engine = None
        self.results = {}
    
    def load_model(self):
        """åŠ è½½OCRæ¨¡å‹"""
        logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        if self.lora_path:
            logger.info(f"ğŸ”§ ä½¿ç”¨LoRAæƒé‡: {self.lora_path}")
        
        self.engine = VLMOCREngine(
            model_name=self.model_name,
            lora_weights_path=self.lora_path,
            device=self.device
        )
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def calculate_cer(self, reference: str, hypothesis: str) -> float:
        """è®¡ç®—å­—ç¬¦é”™è¯¯ç‡"""
        if not reference:
            return 1.0 if hypothesis else 0.0
        
        len_ref = len(reference)
        len_hyp = len(hypothesis)
        
        dp = [[0] * (len_hyp + 1) for _ in range(len_ref + 1)]
        
        for i in range(len_ref + 1):
            dp[i][0] = i
        for j in range(len_hyp + 1):
            dp[0][j] = j
        
        for i in range(1, len_ref + 1):
            for j in range(1, len_hyp + 1):
                if reference[i-1] == hypothesis[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + 1)
        
        return dp[len_ref][len_hyp] / len_ref if len_ref > 0 else 0.0
    
    def test_table_recognition(self, test_data: List[Dict]) -> Dict:
        """æµ‹è¯•è¡¨æ ¼è¯†åˆ«"""
        logger.info("ğŸ“Š å¼€å§‹æµ‹è¯•è¡¨æ ¼è¯†åˆ«...")
        
        results = {
            "scenario": "table",
            "total_samples": len(test_data),
            "successful": 0,
            "failed": 0,
            "average_accuracy": 0.0,
            "target_accuracy": self.SCENARIO_CONFIGS["table"]["target_accuracy"],
            "detailed_results": []
        }
        
        total_accuracy = 0.0
        
        for idx, sample in enumerate(test_data):
            try:
                image_path = sample["image_path"]
                ground_truth = sample["ground_truth"]
                
                # æ‰§è¡ŒOCR
                start_time = time.time()
                prediction = self.engine.inference(
                    image_path=image_path,
                    prompt=self.SCENARIO_CONFIGS["table"]["prompt"]
                )
                inference_time = time.time() - start_time
                
                # è®¡ç®—å‡†ç¡®ç‡ (åŸºäºCERçš„è¡¥æ•°)
                cer = self.calculate_cer(ground_truth, prediction)
                accuracy = 1.0 - cer
                total_accuracy += accuracy
                
                results["successful"] += 1
                results["detailed_results"].append({
                    "sample_id": idx,
                    "image_path": image_path,
                    "accuracy": accuracy,
                    "cer": cer,
                    "inference_time": inference_time,
                    "prediction_length": len(prediction)
                })
                
                logger.info(f"  æ ·æœ¬ {idx}: å‡†ç¡®ç‡={accuracy:.2%}, CER={cer:.4f}")
                
            except Exception as e:
                logger.error(f"âŒ è¡¨æ ¼æ ·æœ¬ {idx} æµ‹è¯•å¤±è´¥: {str(e)}")
                results["failed"] += 1
        
        if results["successful"] > 0:
            results["average_accuracy"] = total_accuracy / results["successful"]
        
        # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        meets_target = results["average_accuracy"] >= results["target_accuracy"]
        results["meets_target"] = meets_target
        
        logger.info(f"âœ… è¡¨æ ¼è¯†åˆ«å®Œæˆ: å¹³å‡å‡†ç¡®ç‡={results['average_accuracy']:.2%} (ç›®æ ‡: {results['target_accuracy']:.0%})")
        if meets_target:
            logger.info("âœ… è¾¾åˆ°Issue #2379è¦æ±‚!")
        else:
            logger.warning(f"âš ï¸  æœªè¾¾åˆ°è¦æ±‚ï¼Œå·®è·: {(results['target_accuracy'] - results['average_accuracy'])*100:.2f}%")
        
        return results
    
    def test_formula_recognition(self, test_data: List[Dict]) -> Dict:
        """æµ‹è¯•å…¬å¼è¯†åˆ«"""
        logger.info("ğŸ”¢ å¼€å§‹æµ‹è¯•å…¬å¼è¯†åˆ«...")
        
        results = {
            "scenario": "formula",
            "total_samples": len(test_data),
            "successful": 0,
            "failed": 0,
            "average_accuracy": 0.0,
            "target_accuracy": self.SCENARIO_CONFIGS["formula"]["target_accuracy"],
            "detailed_results": []
        }
        
        total_accuracy = 0.0
        
        for idx, sample in enumerate(test_data):
            try:
                image_path = sample["image_path"]
                ground_truth = sample["ground_truth"]  # LaTeXæ ¼å¼
                
                # æ‰§è¡ŒOCR
                start_time = time.time()
                prediction = self.engine.inference(
                    image_path=image_path,
                    prompt=self.SCENARIO_CONFIGS["formula"]["prompt"]
                )
                inference_time = time.time() - start_time
                
                # è®¡ç®—å‡†ç¡®ç‡
                cer = self.calculate_cer(ground_truth, prediction)
                accuracy = 1.0 - cer
                total_accuracy += accuracy
                
                results["successful"] += 1
                results["detailed_results"].append({
                    "sample_id": idx,
                    "image_path": image_path,
                    "accuracy": accuracy,
                    "cer": cer,
                    "inference_time": inference_time,
                    "ground_truth": ground_truth,
                    "prediction": prediction
                })
                
                logger.info(f"  æ ·æœ¬ {idx}: å‡†ç¡®ç‡={accuracy:.2%}, CER={cer:.4f}")
                
            except Exception as e:
                logger.error(f"âŒ å…¬å¼æ ·æœ¬ {idx} æµ‹è¯•å¤±è´¥: {str(e)}")
                results["failed"] += 1
        
        if results["successful"] > 0:
            results["average_accuracy"] = total_accuracy / results["successful"]
        
        meets_target = results["average_accuracy"] >= results["target_accuracy"]
        results["meets_target"] = meets_target
        
        logger.info(f"âœ… å…¬å¼è¯†åˆ«å®Œæˆ: å¹³å‡å‡†ç¡®ç‡={results['average_accuracy']:.2%} (ç›®æ ‡: {results['target_accuracy']:.0%})")
        if meets_target:
            logger.info("âœ… è¾¾åˆ°Issue #2379è¦æ±‚!")
        else:
            logger.warning(f"âš ï¸  æœªè¾¾åˆ°è¦æ±‚ï¼Œå·®è·: {(results['target_accuracy'] - results['average_accuracy'])*100:.2f}%")
        
        return results
    
    def test_handwriting_recognition(self, test_data: List[Dict]) -> Dict:
        """æµ‹è¯•æ‰‹å†™ä½“è¯†åˆ«"""
        logger.info("âœï¸  å¼€å§‹æµ‹è¯•æ‰‹å†™ä½“è¯†åˆ«...")
        
        results = {
            "scenario": "handwriting",
            "total_samples": len(test_data),
            "successful": 0,
            "failed": 0,
            "average_cer": 0.0,
            "target_accuracy": self.SCENARIO_CONFIGS["handwriting"]["target_accuracy"],
            "detailed_results": []
        }
        
        total_cer = 0.0
        
        for idx, sample in enumerate(test_data):
            try:
                image_path = sample["image_path"]
                ground_truth = sample["ground_truth"]
                
                # æ‰§è¡ŒOCR
                start_time = time.time()
                prediction = self.engine.inference(
                    image_path=image_path,
                    prompt=self.SCENARIO_CONFIGS["handwriting"]["prompt"]
                )
                inference_time = time.time() - start_time
                
                # è®¡ç®—CER
                cer = self.calculate_cer(ground_truth, prediction)
                total_cer += cer
                
                results["successful"] += 1
                results["detailed_results"].append({
                    "sample_id": idx,
                    "image_path": image_path,
                    "cer": cer,
                    "accuracy": 1.0 - cer,
                    "inference_time": inference_time
                })
                
                logger.info(f"  æ ·æœ¬ {idx}: CER={cer:.4f}, å‡†ç¡®ç‡={1.0-cer:.2%}")
                
            except Exception as e:
                logger.error(f"âŒ æ‰‹å†™ä½“æ ·æœ¬ {idx} æµ‹è¯•å¤±è´¥: {str(e)}")
                results["failed"] += 1
        
        if results["successful"] > 0:
            results["average_cer"] = total_cer / results["successful"]
            results["average_accuracy"] = 1.0 - results["average_cer"]
        
        meets_target = results.get("average_accuracy", 0) >= results["target_accuracy"]
        results["meets_target"] = meets_target
        
        logger.info(f"âœ… æ‰‹å†™ä½“è¯†åˆ«å®Œæˆ: å¹³å‡å‡†ç¡®ç‡={results.get('average_accuracy', 0):.2%}, CER={results['average_cer']:.4f}")
        
        return results
    
    def test_scenario(self, scenario: str, test_data: List[Dict]) -> Dict:
        """æµ‹è¯•æŒ‡å®šåœºæ™¯"""
        if scenario == "table":
            return self.test_table_recognition(test_data)
        elif scenario == "formula":
            return self.test_formula_recognition(test_data)
        elif scenario == "handwriting":
            return self.test_handwriting_recognition(test_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åœºæ™¯: {scenario}")
    
    def run_all_tests(self, test_data_dir: str) -> Dict:
        """è¿è¡Œæ‰€æœ‰åœºæ™¯æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰åœºæ™¯æµ‹è¯•...")
        
        all_results = {
            "model_name": self.model_name,
            "lora_path": self.lora_path,
            "test_time": datetime.now().isoformat(),
            "scenarios": {}
        }
        
        # æµ‹è¯•æ¯ä¸ªåœºæ™¯
        for scenario_name in ["table", "formula", "handwriting"]:
            test_file = Path(test_data_dir) / f"{scenario_name}_test.json"
            
            if not test_file.exists():
                logger.warning(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
                continue
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            logger.info(f"\n{'='*80}")
            logger.info(f"æµ‹è¯•åœºæ™¯: {self.SCENARIO_CONFIGS[scenario_name]['name']}")
            logger.info(f"æè¿°: {self.SCENARIO_CONFIGS[scenario_name]['description']}")
            logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
            logger.info(f"{'='*80}")
            
            # è¿è¡Œæµ‹è¯•
            results = self.test_scenario(scenario_name, test_data)
            all_results["scenarios"][scenario_name] = results
        
        return all_results
    
    def generate_report(self, results: Dict, output_file: str):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“Š å¤šåœºæ™¯æµ‹è¯•æŠ¥å‘Š")
        logger.info(f"{'='*80}")
        logger.info(f"æ¨¡å‹: {results['model_name']}")
        if results['lora_path']:
            logger.info(f"LoRA: {results['lora_path']}")
        logger.info(f"æµ‹è¯•æ—¶é—´: {results['test_time']}")
        logger.info(f"{'='*80}\n")
        
        summary = []
        
        for scenario_name, scenario_results in results["scenarios"].items():
            config = self.SCENARIO_CONFIGS[scenario_name]
            
            logger.info(f"åœºæ™¯: {config['name']}")
            logger.info(f"  æ€»æ ·æœ¬æ•°: {scenario_results['total_samples']}")
            logger.info(f"  æˆåŠŸ: {scenario_results['successful']}")
            logger.info(f"  å¤±è´¥: {scenario_results['failed']}")
            
            if "average_accuracy" in scenario_results:
                accuracy = scenario_results["average_accuracy"]
                target = scenario_results["target_accuracy"]
                meets = scenario_results["meets_target"]
                
                logger.info(f"  å¹³å‡å‡†ç¡®ç‡: {accuracy:.2%}")
                logger.info(f"  ç›®æ ‡å‡†ç¡®ç‡: {target:.0%}")
                logger.info(f"  æ˜¯å¦è¾¾æ ‡: {'âœ… æ˜¯' if meets else 'âŒ å¦'}")
                
                summary.append({
                    "scenario": config['name'],
                    "accuracy": accuracy,
                    "target": target,
                    "meets_target": meets
                })
            
            logger.info("")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        # æ‰“å°æ±‡æ€»
        logger.info(f"\n{'='*80}")
        logger.info("ğŸ“ˆ Issue #2379 è¾¾æ ‡æƒ…å†µæ±‡æ€»")
        logger.info(f"{'='*80}")
        
        for item in summary:
            status = "âœ…" if item["meets_target"] else "âŒ"
            logger.info(f"{status} {item['scenario']}: {item['accuracy']:.2%} (ç›®æ ‡: {item['target']:.0%})")
        
        logger.info(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(description="å¤šåœºæ™¯OCRæµ‹è¯•è„šæœ¬")
    parser.add_argument("--model_name", type=str, required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--lora_path", type=str, default=None, help="LoRAæƒé‡è·¯å¾„")
    parser.add_argument("--scenario", type=str, default="all", 
                       choices=["all", "table", "formula", "handwriting"],
                       help="æµ‹è¯•åœºæ™¯")
    parser.add_argument("--test_data_dir", type=str, required=True, 
                       help="æµ‹è¯•æ•°æ®ç›®å½• (åŒ…å«table_test.json, formula_test.jsonç­‰)")
    parser.add_argument("--device", type=str, default="npu:0", help="è®¾å¤‡")
    parser.add_argument("--output", type=str, default=None, help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = MultiScenarioTester(
        model_name=args.model_name,
        lora_path=args.lora_path,
        device=args.device
    )
    
    # åŠ è½½æ¨¡å‹
    tester.load_model()
    
    # è¿è¡Œæµ‹è¯•
    if args.scenario == "all":
        results = tester.run_all_tests(args.test_data_dir)
    else:
        test_file = Path(args.test_data_dir) / f"{args.scenario}_test.json"
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        results = {
            "model_name": args.model_name,
            "lora_path": args.lora_path,
            "test_time": datetime.now().isoformat(),
            "scenarios": {
                args.scenario: tester.test_scenario(args.scenario, test_data)
            }
        }
    
    # ç”ŸæˆæŠ¥å‘Š
    output_file = args.output or f"multi_scenario_report_{int(time.time())}.json"
    tester.generate_report(results, output_file)


if __name__ == "__main__":
    main()
