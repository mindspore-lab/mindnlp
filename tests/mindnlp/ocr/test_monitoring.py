"""
æµ‹è¯•ç›‘æ§ã€æ—¥å¿—å’Œè¿½è¸ªç³»ç»Ÿ
"""
import sys
import os
import tempfile
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root / "src"))


def test_structured_logging():
    """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ")
    print("="*60)
    
    from mindnlp.ocr.utils.structured_logging import (
        setup_structured_logging,
        LogContext,
        get_request_logger,
        get_performance_logger
    )
    
    # åˆ›å»ºä¸´æ—¶æ—¥å¿—æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False) as f:
        log_file = f.name
    
    try:
        # 1. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ (Consoleæ ¼å¼ä¾¿äºæµ‹è¯•)
        print("\n[1.1] åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ...")
        setup_structured_logging(
            log_level="INFO",
            log_file=log_file,
            json_format=False  # Consoleæ ¼å¼ä¾¿äºæŸ¥çœ‹
        )
        print("âœ“ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æµ‹è¯•è¯·æ±‚æ—¥å¿—
        print("\n[1.2] æµ‹è¯•è¯·æ±‚æ—¥å¿—...")
        request_logger = get_request_logger()
        
        request_logger.log_request(
            method="POST",
            endpoint="/api/v1/ocr/predict",
            request_id="test-request-123",
            status_code=200,
            latency_ms=250.5
        )
        print("âœ“ è¯·æ±‚æ—¥å¿—è®°å½•æˆåŠŸ")
        
        # 3. æµ‹è¯•ä¸Šä¸‹æ–‡ç»‘å®š
        print("\n[1.3] æµ‹è¯•æ—¥å¿—ä¸Šä¸‹æ–‡ç»‘å®š...")
        with LogContext(request_id="test-ctx-456", user_id="user-789"):
            request_logger.log_inference(
                request_id="test-ctx-456",
                model_name="ocr_model",
                inference_time_ms=180.2,
                batch_size=4
            )
        print("âœ“ ä¸Šä¸‹æ–‡ç»‘å®šæˆåŠŸ")
        
        # 4. æµ‹è¯•æ€§èƒ½æ—¥å¿—
        print("\n[1.4] æµ‹è¯•æ€§èƒ½æ—¥å¿—...")
        perf_logger = get_performance_logger()
        
        perf_logger.log_resource_usage(
            cpu_percent=65.2,
            memory_mb=1024.5,
            gpu_utilization=82.3
        )
        
        perf_logger.log_queue_metrics(
            queue_size=15,
            queue_capacity=100,
            avg_wait_time_ms=50.0
        )
        print("âœ“ æ€§èƒ½æ—¥å¿—è®°å½•æˆåŠŸ")
        
        # 5. æµ‹è¯•JSONæ ¼å¼
        print("\n[1.5] æµ‹è¯•JSONæ ¼å¼æ—¥å¿—...")
        with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False) as f:
            json_log_file = f.name
        
        setup_structured_logging(
            log_level="INFO",
            log_file=json_log_file,
            json_format=True
        )
        
        request_logger = get_request_logger()
        request_logger.log_request(
            method="GET",
            endpoint="/api/v1/health",
            request_id="test-json-001",
            status_code=200,
            latency_ms=10.5
        )
        
        # éªŒè¯JSONæ ¼å¼
        if os.path.exists(json_log_file):
            with open(json_log_file, 'r', encoding='utf-8') as f:
                last_line = f.readlines()[-1]
                log_entry = json.loads(last_line)
                assert log_entry['event'] == 'http_request'
                assert log_entry['request_id'] == 'test-json-001'
                print("âœ“ JSONæ ¼å¼éªŒè¯æˆåŠŸ")
        
        print("\nâœ… ç»“æ„åŒ–æ—¥å¿—æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in [log_file, json_log_file]:
            if os.path.exists(temp_file):
                os.unlink(temp_file)


def test_tracing():
    """æµ‹è¯•åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ")
    print("="*60)
    
    from mindnlp.ocr.utils.tracing import (
        TracingConfig,
        setup_tracing,
        get_tracer
    )
    
    # 1. æµ‹è¯•Consoleå¯¼å‡ºå™¨ (ä¸éœ€è¦Jaeger)
    print("\n[2.1] åˆå§‹åŒ–è¿½è¸ªç³»ç»Ÿ (Consoleå¯¼å‡º)...")
    config = TracingConfig(
        enabled=True,
        service_name="ocr-api-test",
        sampling_rate=1.0,  # 100%é‡‡æ ·ç”¨äºæµ‹è¯•
        exporter_type="console"
    )
    
    provider = setup_tracing(config)
    print("âœ“ è¿½è¸ªç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    
    # 2. æµ‹è¯•è¯·æ±‚è¿½è¸ª
    print("\n[2.2] æµ‹è¯•HTTPè¯·æ±‚è¿½è¸ª...")
    tracer = get_tracer()
    
    with tracer.trace_request(
        request_id="trace-req-001",
        endpoint="/api/v1/ocr/predict"
    ) as span:
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†
        span.set_attribute("http.status_code", 200)
    print("âœ“ è¯·æ±‚è¿½è¸ªæˆåŠŸ")
    
    # 3. æµ‹è¯•é¢„å¤„ç†è¿½è¸ª
    print("\n[2.3] æµ‹è¯•é¢„å¤„ç†è¿½è¸ª...")
    with tracer.trace_preprocessing(
        image_size=(1920, 1080),
        image_format="JPEG"
    ):
        time.sleep(0.05)
    print("âœ“ é¢„å¤„ç†è¿½è¸ªæˆåŠŸ")
    
    # 4. æµ‹è¯•æ¨ç†è¿½è¸ª
    print("\n[2.4] æµ‹è¯•æ¨ç†è¿½è¸ª...")
    with tracer.trace_inference(
        model_name="ocr_model_v1",
        batch_size=4
    ):
        time.sleep(0.15)
    print("âœ“ æ¨ç†è¿½è¸ªæˆåŠŸ")
    
    # 5. æµ‹è¯•åå¤„ç†è¿½è¸ª
    print("\n[2.5] æµ‹è¯•åå¤„ç†è¿½è¸ª...")
    with tracer.trace_postprocessing(output_format="json"):
        time.sleep(0.02)
    print("âœ“ åå¤„ç†è¿½è¸ªæˆåŠŸ")
    
    # 6. æµ‹è¯•åµŒå¥—Span
    print("\n[2.6] æµ‹è¯•åµŒå¥—Spanè¿½è¸ª...")
    with tracer.trace_request("nested-req-001", "/api/v1/ocr/batch"):
        with tracer.trace_preprocessing((800, 600), "PNG"):
            time.sleep(0.03)
        with tracer.trace_inference("model_v2", 2):
            time.sleep(0.08)
        with tracer.trace_postprocessing("xml"):
            time.sleep(0.01)
    print("âœ“ åµŒå¥—è¿½è¸ªæˆåŠŸ")
    
    # å…³é—­provider
    if provider:
        provider.shutdown()
    
    print("\nâœ… åˆ†å¸ƒå¼è¿½è¸ªæµ‹è¯•å…¨éƒ¨é€šè¿‡!")


def test_profiling():
    """æµ‹è¯•æ€§èƒ½åˆ†æå·¥å…·"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: æ€§èƒ½Profilingå·¥å…·")
    print("="*60)
    
    from mindnlp.ocr.utils.profiling import (
        get_profiling_manager,
        CPUProfiler,
        MemoryProfiler,
        PerformanceTimer
    )
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        
        # 1. æµ‹è¯•CPU Profiling
        print("\n[3.1] æµ‹è¯•CPU Profiling...")
        cpu_profiler = CPUProfiler(output_dir=temp_dir)
        
        with cpu_profiler.profile("test_cpu"):
            # æ¨¡æ‹ŸCPUå¯†é›†å‹æ“ä½œ
            result = sum([i**2 for i in range(10000)])
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        prof_files = list(Path(temp_dir).glob("test_cpu_*.prof"))
        assert len(prof_files) > 0, "CPU profilingæ–‡ä»¶æœªç”Ÿæˆ"
        print(f"âœ“ CPU ProfilingæˆåŠŸï¼Œç”Ÿæˆæ–‡ä»¶: {prof_files[0].name}")
        
        # 2. æµ‹è¯•Memory Profiling
        print("\n[3.2] æµ‹è¯•Memory Profiling...")
        mem_profiler = MemoryProfiler()
        
        with mem_profiler.profile("test_memory"):
            # æ¨¡æ‹Ÿå†…å­˜åˆ†é…
            data = [i for i in range(100000)]
            time.sleep(0.1)
        print("âœ“ Memory ProfilingæˆåŠŸ")
        
        # 3. æµ‹è¯•Performance Timer
        print("\n[3.3] æµ‹è¯•Performance Timer...")
        timer = PerformanceTimer()
        
        with timer.measure("operation_1"):
            time.sleep(0.1)
        
        elapsed = timer.get_elapsed("operation_1")
        assert elapsed >= 100, f"è®¡æ—¶ä¸å‡†ç¡®: {elapsed}ms"
        print(f"âœ“ Performance TimeræˆåŠŸï¼Œè€—æ—¶: {elapsed:.2f}ms")
        
        # 4. æµ‹è¯•Profiling Manager
        print("\n[3.4] æµ‹è¯•Profiling Manager...")
        manager = get_profiling_manager()
        manager.output_dir = temp_dir
        
        with manager.profile_cpu("manager_test"):
            result = sum([i**3 for i in range(5000)])
        
        prof_files = list(Path(temp_dir).glob("manager_test_*.prof"))
        assert len(prof_files) > 0
        print("âœ“ Profiling ManageræˆåŠŸ")
        
        # 5. æµ‹è¯•ç»„åˆProfiling
        print("\n[3.5] æµ‹è¯•ç»„åˆProfiling...")
        with manager.profile_cpu("combined_cpu"):
            with manager.profile_memory("combined_memory"):
                data = [i**2 for i in range(50000)]
                time.sleep(0.05)
        print("âœ“ ç»„åˆProfilingæˆåŠŸ")
        
    print("\nâœ… æ€§èƒ½Profilingæµ‹è¯•å…¨éƒ¨é€šè¿‡!")


def test_integration():
    """é›†æˆæµ‹è¯•: åŒæ—¶ä½¿ç”¨æ—¥å¿—ã€è¿½è¸ªå’ŒProfiling"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: é›†æˆæµ‹è¯•")
    print("="*60)
    
    from mindnlp.ocr.utils.structured_logging import (
        setup_structured_logging,
        LogContext,
        get_request_logger
    )
    from mindnlp.ocr.utils.tracing import (
        TracingConfig,
        setup_tracing,
        get_tracer
    )
    from mindnlp.ocr.utils.profiling import get_profiling_manager
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        log_file = os.path.join(temp_dir, "integration.log")
        
        # åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿ
        print("\n[4.1] åˆå§‹åŒ–æ‰€æœ‰ç›‘æ§ç³»ç»Ÿ...")
        setup_structured_logging(log_level="INFO", log_file=log_file, json_format=True)
        
        trace_config = TracingConfig(
            enabled=True,
            service_name="ocr-integration-test",
            sampling_rate=1.0,
            exporter_type="console"
        )
        setup_tracing(trace_config)
        
        profiler = get_profiling_manager()
        profiler.output_dir = temp_dir
        print("âœ“ æ‰€æœ‰ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„OCRè¯·æ±‚å¤„ç†
        print("\n[4.2] æ¨¡æ‹Ÿå®Œæ•´OCRè¯·æ±‚å¤„ç†...")
        request_id = "integration-req-001"
        request_logger = get_request_logger()
        tracer = get_tracer()
        
        with LogContext(request_id=request_id):
            with tracer.trace_request(request_id, "/api/v1/ocr/predict"):
                with profiler.profile_cpu("full_pipeline"):
                    
                    # é¢„å¤„ç†
                    with tracer.trace_preprocessing((1024, 768), "JPEG"):
                        time.sleep(0.05)
                        request_logger.log_inference(
                            request_id=request_id,
                            model_name="preprocessing",
                            inference_time_ms=50.0,
                            batch_size=1
                        )
                    
                    # æ¨ç†
                    with tracer.trace_inference("ocr_model", 1):
                        time.sleep(0.15)
                        request_logger.log_inference(
                            request_id=request_id,
                            model_name="ocr_model",
                            inference_time_ms=150.0,
                            batch_size=1
                        )
                    
                    # åå¤„ç†
                    with tracer.trace_postprocessing("json"):
                        time.sleep(0.02)
                
                # è®°å½•æœ€ç»ˆè¯·æ±‚
                request_logger.log_request(
                    method="POST",
                    endpoint="/api/v1/ocr/predict",
                    request_id=request_id,
                    status_code=200,
                    latency_ms=220.0
                )
        
        print("âœ“ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ")
        
        # éªŒè¯ç»“æœ
        print("\n[4.3] éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶...")
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
        assert os.path.exists(log_file), "æ—¥å¿—æ–‡ä»¶æœªç”Ÿæˆ"
        with open(log_file, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()
            assert len(log_lines) > 0, "æ—¥å¿—ä¸ºç©º"
            # éªŒè¯JSONæ ¼å¼
            for line in log_lines:
                json.loads(line)  # åº”è¯¥èƒ½æˆåŠŸè§£æ
        print(f"âœ“ æ—¥å¿—æ–‡ä»¶éªŒè¯æˆåŠŸ ({len(log_lines)} æ¡æ—¥å¿—)")
        
        # æ£€æŸ¥Profilingæ–‡ä»¶
        prof_files = list(Path(temp_dir).glob("*.prof"))
        assert len(prof_files) > 0, "Profilingæ–‡ä»¶æœªç”Ÿæˆ"
        print(f"âœ“ Profilingæ–‡ä»¶éªŒè¯æˆåŠŸ ({len(prof_files)} ä¸ªæ–‡ä»¶)")
    
    print("\nâœ… é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡!")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "â–ˆ"*60)
    print("â–ˆ  OCR ç›‘æ§ã€æ—¥å¿—å’Œæ€§èƒ½åˆ†æç³»ç»Ÿ - åŠŸèƒ½æµ‹è¯•")
    print("â–ˆ"*60)
    
    try:
        test_structured_logging()
        test_tracing()
        test_profiling()
        test_integration()
        
        print("\n" + "â–ˆ"*60)
        print("â–ˆ  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ")
        print("â–ˆ"*60)
        print("\næµ‹è¯•æ€»ç»“:")
        print("  âœ… ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ - 6é¡¹æµ‹è¯•é€šè¿‡")
        print("  âœ… åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ - 6é¡¹æµ‹è¯•é€šè¿‡")
        print("  âœ… æ€§èƒ½Profiling - 5é¡¹æµ‹è¯•é€šè¿‡")
        print("  âœ… é›†æˆæµ‹è¯• - 3é¡¹æµ‹è¯•é€šè¿‡")
        print("\næ€»è®¡: 20é¡¹æµ‹è¯•å…¨éƒ¨é€šè¿‡\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
