# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
# pylint: disable=missing-function-docstring
# pylint: disable=missing-class-docstring
# pylint: disable=invalid-name
# pylint: disable=unused-variable
# pylint: disable=unused-argument
# pylint: disable=global-statement
# pylint: disable=unspecified-encoding
# pylint: disable=eval-used
# pylint: disable=attribute-defined-outside-init
# pylint: disable=import-outside-toplevel
"""
Serialization utils
"""
import os
import io
import sys
import pickle
import shutil
import zipfile
import tarfile
import pathlib
import warnings
import tempfile
import operator

from contextlib import closing, contextmanager
from enum import Enum
from typing import Dict, Union, Optional, Any
from functools import reduce
from dataclasses import dataclass
from ml_dtypes import bfloat16

import numpy as np
import mindspore
from mindspore import Tensor

from . import logging

logger = logging.get_logger(__name__)

MAGIC_NUMBER = 0x1950a86a20f9469cfc6c
PROTOCOL_VERSION = 1001

@contextmanager
def mkdtemp():
    path = tempfile.mkdtemp()
    try:
        yield path
    finally:
        shutil.rmtree(path)

class PyTorchFileReader:
    """
    Class to allow PackageImporter to operate on unzipped packages. Methods
    copy the behavior of the internal PyTorchFileReader class (which is used for
    accessing packages in all other cases).

    N.B.: ScriptObjects are not depickleable or accessible via this DirectoryReader
    class due to ScriptObjects requiring an actual PyTorchFileReader instance.
    """

    def __init__(self, file):
        self.file = zipfile.ZipFile(file)
        self.directory = self.file.namelist()[0].split('/')[0]

    def open_record(self, name):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            return self.file.open(filename)
        return None

    def read_record(self, name):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            return self.file.read(filename)
        return None


    def has_record(self, name):
        filename = f"{self.directory}/{name}"
        return filename in self.file.namelist()

    def get_all_records(
        self,
    ):
        files = [name.replace(self.directory + '/' , '')for name in self.file.namelist()]
        return files

    def get_record_offset(self, name):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            return self.file.getinfo(filename).header_offset
        return None

class LoadEndianness(Enum):
    NATIVE = 1
    LITTLE = 2
    BIG = 3

_default_load_endian: Optional[LoadEndianness] = None

def get_default_load_endianness() -> Optional[LoadEndianness]:
    '''
    Get fallback byte order for loading files

    If byteorder mark is not present in saved checkpoint,
    this byte order is used as fallback.
    By default, it's "native" byte order.

    Returns:
        default_load_endian: Optional[LoadEndianness]
    '''
    return _default_load_endian

def set_default_load_endianness(endianness):
    '''
    Set fallback byte order for loading files

    If byteorder mark is not present in saved checkpoint,
    this byte order is used as fallback.
    By default, it's "native" byte order.

    Args:
        endianness: the new fallback byte order
    '''
    global _default_load_endian
    if not isinstance(endianness, LoadEndianness) and endianness is not None:
        raise TypeError("Invalid argument type in function set_default_load_endianness")
    _default_load_endian = endianness

def _is_zipfile(f) -> bool:
    # This is a stricter implementation than zipfile.is_zipfile().
    # zipfile.is_zipfile() is True if the magic number appears anywhere in the
    # binary. Since we expect the files here to be generated by torch.save or
    # torch.jit.save, it's safe to only check the start bytes and avoid
    # collisions and assume the zip has only 1 file.
    # See bugs.python.org/issue28494.

    # Read the first 4 bytes of the file
    read_bytes = []
    start = f.tell()

    byte = f.read(1)
    while byte != b"":
        read_bytes.append(byte)
        if len(read_bytes) == 4:
            break
        byte = f.read(1)
    f.seek(start)

    local_header_magic_number = [b'P', b'K', b'\x03', b'\x04']
    return read_bytes == local_header_magic_number

def _check_seekable(f) -> bool:

    def raise_err_msg(patterns, e):
        for p in patterns:
            if p in str(e):
                msg = (str(e) + ". You can only torch.load from a file that is seekable."
                                + " Please pre-load the data into a buffer like io.BytesIO and"
                                + " try to load from it instead.")
                raise type(e)(msg)
        raise e

    try:
        f.seek(f.tell())
        return True
    except (io.UnsupportedOperation, AttributeError) as e:
        raise_err_msg(["seek", "tell"], e)
    return False

def _is_compressed_file(f) -> bool:
    compress_modules = ['gzip']
    try:
        return f.__module__ in compress_modules
    except AttributeError:
        return False

def _should_read_directly(f):
    """
    Checks if f is a file that should be read directly. It should be read
    directly if it is backed by a real file (has a fileno) and is not a
    a compressed file (e.g. gzip)
    """
    if _is_compressed_file(f):
        return False
    try:
        return f.fileno() >= 0
    except io.UnsupportedOperation:
        return False
    except AttributeError:
        return False


def _is_path(name_or_buffer):
    return isinstance(name_or_buffer, (str, pathlib.Path))

def _is_torchscript_zip(zip_file):
    return 'constants.pkl' in zip_file.get_all_records()

class _opener:
    def __init__(self, file_like):
        self.file_like = file_like

    def __enter__(self):
        return self.file_like

    def __exit__(self, *args):
        pass


class _open_file(_opener):
    def __init__(self, name, mode):
        super().__init__(open(name, mode))

    def __exit__(self, *args):
        self.file_like.close()


class _open_buffer_reader(_opener):
    def __init__(self, buffer):
        super().__init__(buffer)
        _check_seekable(buffer)


class _open_buffer_writer(_opener):
    def __exit__(self, *args):
        self.file_like.flush()


def _open_file_like(name_or_buffer, mode):
    if _is_path(name_or_buffer):
        return _open_file(name_or_buffer, mode)
    if 'w' in mode:
        return _open_buffer_writer(name_or_buffer)
    if 'r' in mode:
        return _open_buffer_reader(name_or_buffer)
    raise RuntimeError(f"Expected 'r' or 'w' in mode but got {mode}")

class _open_zipfile_reader(_opener):
    def __init__(self, name_or_buffer) -> None:
        super().__init__(PyTorchFileReader(name_or_buffer))

def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata=None):
    if size == ():
        size = (1,)
        stride = (1,)
    num_elemets = reduce(operator.mul, size)
    array = storage[storage_offset: storage_offset + num_elemets]

    if array.dtype == bfloat16:
        logger.warning_once("MindSpore do not support bfloat16 dtype, we will automaticlly convert to float16")
        array = array.astype(np.float16)

    if stride is not None and len(stride) > 1 and stride[0] == 1 and stride[1] > 1:
        stride = tuple((s * 4 for s in stride))
        array = np.lib.stride_tricks.as_strided(array, size, stride)
    else:
        order = "C"
        array = array.reshape(size, order=order)
    param = mindspore.Parameter(array, requires_grad=requires_grad)
    return param

@dataclass
class FakeParameter:
    storage: np.ndarray = None
    storage_offset: int = None
    size: tuple = None
    stride: tuple = None
    requires_grad: bool = None

def _rebuild_tensor_legacy(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata=None):
    return FakeParameter(storage, storage_offset, size, stride, requires_grad)

def _maybe_decode_ascii(bytes_str: Union[bytes, str]) -> str:
    # When using encoding='bytes' in Py3, some **internal** keys stored as
    # strings in Py2 are loaded as bytes. This function decodes them with
    # ascii encoding, one that Py3 uses by default.
    #
    # NOTE: This should only be used on internal keys (e.g., `typename` and
    #       `location` in `persistent_load` below!
    if isinstance(bytes_str, bytes):
        return bytes_str.decode('ascii')
    return bytes_str


dtype_map = {
    "HalfStorage": np.float16,
    "FloatStorage": np.float32,
    'BFloat16Storage': bfloat16,
    'LongStorage': np.int64,
    'ByteStorage': np.uint8,
    'BoolStorage': np.bool_
}

element_size_map = {
    "HalfStorage": 2,
    "FloatStorage": 3,
    'BFloat16Storage': 2,
    'LongStorage': 4,
    'ByteStorage': 1,
    'BoolStorage': 1
}

def load(f, pickle_module=pickle, *, mmap=None, **pickle_load_args):
    if pickle_module is None:
        pickle_module = pickle

    # make flipping default BC-compatible
    if mmap is None:
        mmap = False

    if 'encoding' not in pickle_load_args:
        pickle_load_args['encoding'] = 'utf-8'

    with _open_file_like(f, 'rb') as opened_file:
        if _is_zipfile(opened_file):
            # The zipfile reader is going to advance the current file position.
            # If we want to actually tail call to torch.jit.load, we need to
            # reset back to the original position.
            overall_storage = None
            with _open_zipfile_reader(opened_file, ) as opened_zipfile:
                if _is_torchscript_zip(opened_zipfile):
                    raise ValueError('do not support torchscript now')
                if mmap:
                    if not isinstance(f, str):
                        raise ValueError("f must be a string filename in order to use mmap argument")
                    overall_storage = f

                return _load(opened_zipfile,
                             pickle_module,
                             overall_storage=overall_storage,
                             **pickle_load_args)
        if mmap:
            raise RuntimeError("mmap can only be used with files saved with ",
                               "`torch.save(_use_new_zipfile_serialization=True), "
                               "please torch.save your checkpoint with this option in order to use mmap.")

        return _legacy_load(opened_file, pickle_module, **pickle_load_args)

def _legacy_load(f, pickle_module, **pickle_load_args):
    deserialized_objects: Dict[int, Any] = {}

    class UnpicklerWrapper(pickle_module.Unpickler):  # type: ignore[name-defined]

        def find_class(self, mod_name, name):
            if name == '_rebuild_tensor_v2':
                name = '_rebuild_tensor_legacy'
            if mod_name == 'torch._utils':
                return eval(name)
            if mod_name == 'torch':
                return str(name)
            return super().find_class(mod_name, name)

    def legacy_load(f):
        deserialized_objects: Dict[int, Any] = {}

        def persistent_load(saved_id):
            if isinstance(saved_id, tuple):
                # Ignore containers that don't have any sources saved
                return saved_id[0]
            return deserialized_objects[int(saved_id)]

        with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
                mkdtemp() as tmpdir:
            raise ValueError('do not support legacy load for Pytorch.')


    deserialized_objects = {}

    def persistent_load(saved_id):
        assert isinstance(saved_id, tuple)
        typename = _maybe_decode_ascii(saved_id[0])
        data = saved_id[1:]

        if typename == 'module':
            # Ignore containers that don't have any sources saved
            return data[0]
        if typename == 'storage':
            storage_type, root_key, location, numel, view_metadata = data
            location = _maybe_decode_ascii(location)

            if root_key not in deserialized_objects:
                typed_storage = np.empty(numel, dtype_map[storage_type])
                deserialized_objects[root_key] = typed_storage
            else:
                typed_storage = deserialized_objects[root_key]

            if view_metadata is not None:
                view_key, offset, view_size = view_metadata
                if view_key not in deserialized_objects:
                    # TODO: Once we decide to break serialization FC, we can
                    # stop wrapping with TypedStorage
                    deserialized_objects[view_key] = typed_storage[offset: offset + view_size]
                res = deserialized_objects[view_key]
            else:
                res = typed_storage
            return res
        raise RuntimeError(f"Unknown saved id type: {saved_id[0]}")

    _check_seekable(f)
    f_should_read_directly = _should_read_directly(f)

    if f_should_read_directly and f.tell() == 0:
        # legacy_load requires that f has fileno()
        # only if offset is zero we can attempt the legacy tar file loader
        try:
            return legacy_load(f)
        except tarfile.TarError:
            if _is_zipfile(f):
                # .zip is used for torch.jit.save and will throw an un-pickling error here
                raise RuntimeError(
                    f"{f.name} is a zip archive (did you mean to use torch.jit.load()?)") from None
            # if not a tarfile, reset file offset and proceed
            f.seek(0)

    if not hasattr(f, 'readinto') and (3, 8, 0) <= sys.version_info < (3, 8, 2):
        raise RuntimeError(
            "torch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. "
            f"Received object of type \"{type(f)}\". Please update to Python 3.8.2 or newer to restore this "
            "functionality.")

    magic_number = pickle_module.load(f, **pickle_load_args)
    if magic_number != MAGIC_NUMBER:
        raise RuntimeError("Invalid magic number; corrupt file?")
    protocol_version = pickle_module.load(f, **pickle_load_args)
    if protocol_version != PROTOCOL_VERSION:
        raise RuntimeError(f"Invalid protocol version: {protocol_version}")

    _sys_info = pickle_module.load(f, **pickle_load_args)
    unpickler = UnpicklerWrapper(f, **pickle_load_args)
    unpickler.persistent_load = persistent_load
    result = unpickler.load()

    deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)

    offset = f.tell() if f_should_read_directly else None
    for key in deserialized_storage_keys:
        assert key in deserialized_objects
        typed_storage = deserialized_objects[key]
        f.read(8) # trick for read
        array = np.frombuffer(f.read(typed_storage.nbytes), typed_storage.dtype)
        typed_storage[:] = array
        assert np.allclose(typed_storage, array)
        if offset is not None:
            offset = f.tell()

    new_result = {}
    for k, v in result.items():
        num_elemets = reduce(operator.mul, v.size)
        array = v.storage[v.storage_offset: v.storage_offset + num_elemets]
        stride = v.stride
        size = v.size
        if stride is not None and len(stride) > 1 and stride[0] == 1 and stride[1] > 1:
            stride = tuple((s * 4 for s in stride))
            array = np.lib.stride_tricks.as_strided(array, size, stride)
        else:
            order = "C"
            array = array.reshape(size, order=order)
        if array.dtype == bfloat16:
            logger.warning_once("MindSpore do not support bfloat16 dtype, we will automaticlly convert to float16")
            array = array.astype(np.float16)
        new_result[k] = mindspore.Parameter(array, requires_grad=v.requires_grad)

    return new_result

def _load(zip_file, pickle_module, overall_storage=None, pickle_file='data.pkl', **pickle_load_args):
    loaded_storages = {}
    # check if byteswapping is needed
    byteordername = 'byteorder'
    byteorderdata = None
    if zip_file.has_record(byteordername):
        byteorderdata = zip_file.read_record(byteordername)
        if byteorderdata not in [b'little', b'big']:
            raise ValueError('Unknown endianness type: ' + byteorderdata.decode())
    elif get_default_load_endianness() == LoadEndianness.LITTLE or \
            get_default_load_endianness() is None:
        byteorderdata = b'little'
    elif get_default_load_endianness() == LoadEndianness.BIG:
        byteorderdata = b'big'
    elif get_default_load_endianness() == LoadEndianness.NATIVE:
        pass
    else:
        raise ValueError('Invalid load endianness type')

    if not zip_file.has_record(byteordername) and \
            get_default_load_endianness() is None and \
            sys.byteorder == 'big':
        # Default behaviour was changed
        # See https://github.com/pytorch/pytorch/issues/101688
        warnings.warn("The default load endianness for checkpoints without a byteorder mark "
                      "on big endian machines was changed from 'native' to 'little' endian, "
                      "to avoid this behavior please use "
                      "torch.serialization.set_default_load_endianness to set "
                      "the desired default load endianness",
                      UserWarning)

    def persistent_load(saved_id):
        assert isinstance(saved_id, tuple)
        typename = _maybe_decode_ascii(saved_id[0])
        data = saved_id[1:]

        assert typename == 'storage', \
            f"Unknown typename for persistent_load, expected 'storage' but got '{typename}'"
        storage_type, key, location, numel = data

        name = f'data/{key}'
        if name in loaded_storages:
            return loaded_storages[name]

        if overall_storage is not None:
            array = np.memmap(overall_storage, dtype=dtype_map[storage_type], offset=zip_file.open_record(name)._fileobj.tell(), shape=(numel,))
        else:
            array = np.frombuffer(zip_file.read_record(name), dtype_map[storage_type])
        loaded_storages[name] = array
        return array

    load_module_mapping: Dict[str, str] = {
        # See https://github.com/pytorch/pytorch/pull/51633
        'torch.tensor': 'torch._tensor'
    }

    # Need to subclass Unpickler instead of directly monkey-patching the find_class method
    # because it's marked readonly in pickle.
    # The type: ignore is because mypy can't statically determine the type of this class.
    class UnpicklerWrapper(pickle_module.Unpickler):  # type: ignore[name-defined]
        # from https://stackoverflow.com/questions/13398462/unpickling-python-objects-with-a-changed-module-path/13405732
        # Lets us override the imports that pickle uses when unpickling an object.
        # This is useful for maintaining BC if we change a module path that tensor instantiation relies on.
        def find_class(self, mod_name, name):
            if mod_name == 'torch._utils':
                return eval(name)
            if mod_name == 'torch':
                return str(name)

            mod_name = load_module_mapping.get(mod_name, mod_name)
            return super().find_class(mod_name, name)

    # Load the data (which may in turn use `persistent_load` to load tensors)
    data_file = zip_file.open_record(pickle_file)

    unpickler = UnpicklerWrapper(data_file, **pickle_load_args)
    unpickler.persistent_load = persistent_load
    result = unpickler.load()

    return result

def convert_torch_to_mindspore(pth_file):
    """convert torch checkpoint to mindspore"""
    try:
        import torch # pylint: disable=import-error
    except Exception as exc:
        raise ImportError("'import torch' failed, please install torch by "
                        "`pip install torch` or instructions from 'https://pytorch.org'") \
                        from exc
    if pth_file.endswith(".safetensors"):
        from safetensors.torch import load_file
        state_dict = load_file(pth_file)
        ms_ckpt_path = pth_file.replace('model-', 'mindspore-')
        ms_ckpt_path = ms_ckpt_path.replace('.safetensors', '.ckpt')

    else:
        ms_ckpt_path = pth_file.replace('pytorch_model', 'mindspore')
        ms_ckpt_path = ms_ckpt_path.replace('.bin', '.ckpt')

        state_dict = torch.load(pth_file, map_location='cpu')

    if os.path.exists(ms_ckpt_path):
        return ms_ckpt_path

    ms_ckpt = []
    logger.info('Starting checkpoint conversion.')

    has_bf16 = False
    for key, value in state_dict.items():
        if value.dtype == torch.bfloat16:
            data = Tensor(value.to(torch.float).numpy(), dtype=mindspore.float16)
            if not has_bf16:
                has_bf16 = True
        else:
            data = Tensor(value.numpy())
        ms_ckpt.append({'name': key, 'data': data})

    if has_bf16:
        logger.warning("MindSpore do not support bfloat16 dtype, we will automaticlly convert to float16")

    try:
        mindspore.save_checkpoint(ms_ckpt, ms_ckpt_path)
    except Exception as exc:
        raise RuntimeError(f'Save checkpoint to {ms_ckpt_path} failed, '
                            f'please checkout the path.') from exc

    return ms_ckpt_path
