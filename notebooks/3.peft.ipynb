{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6213b0-0a5a-4f1a-abe8-a3012234c40d",
   "metadata": {},
   "source": [
    "# PEFT\n",
    "\n",
    "PEFT (Parameter-Efficient Fine-Tuning) is a technique that fine-tunes large pre-trained models with minimal parameter updates to reduce computational costs and preserve generalization. Within PEFT, LoRA ([Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)) uses low-rank matrices to efficiently adapt parts of a neural network with minimal extra parameters. This technique enables you to train large models that would typically be inaccessible on consumer devices.\n",
    "\n",
    "In this tutorial, we'll explore this technique using MindNLP. As an example, we will use the the mT0 model, which is a mT5 model finetuned on multilingual tasks. You'll learn how to initialize, modify, and train the model, gaining hands-on experience in efficient fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c62952",
   "metadata": {},
   "source": [
    "## Load the model and add PEFT adapter\n",
    "First, we load the pretrained model by supplying the model name to the model loader `AutoModelForSeq2SeqLM`. Then add a PEFT adapter to the model using `get_peft_model`, which allows the model to maintain much of its pre-trained parameters while efficiently adapting to new tasks with a focused set of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce388e-d581-4425-946f-8aa416f9311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForSeq2SeqLM\n",
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Get the model with a PEFT adapter\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44acc1ab",
   "metadata": {},
   "source": [
    "`LoraConfig` specifies how the PEFT adapters should be configured:\n",
    "\n",
    "* `task_type`: Defines the type of task, which in this case is TaskType.SEQ_2_SEQ_LM for sequence-to-sequence language modeling.\n",
    "* `inference_mode`: A boolean that should be set to False when training to enable the training-specific features of the adapters.\n",
    "* `r`: Represents the rank of the low-rank matrices that are part of the adapter. A lower rank means less complexity and fewer parameters to train.\n",
    "* `lora_alpha`: LoRA alpha is the scaling factor for the weight matrices. A higher alpha value assigns more weight to the LoRA activations.\n",
    "* `lora_dropout`: Sets the dropout rate within the adapter layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e945fff",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "To fine-tune the model, let's use the [financial_phrasebank](https://huggingface.co/datasets/takala/financial_phrasebank) dataset. The financial_phrasebank dataset is specifically designed for sentiment analysis tasks within the financial sector. It contains sentences extracted from financial news articles, which are categorized based on the sentiment expressed — negative, neutral or positive.\n",
    "\n",
    "Although the dataset is designed for sentiment classification task, we use it here for a sequence-to-sequence task for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d6121",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Load the dataset with `load_dataset` from MindNLP.\n",
    "\n",
    "The data is then shuffled and split, allocating 90% for training and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60b4d3-71c5-4c43-9a9a-1705970610be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.dataset import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "train_dataset, validation_dataset = dataset.shuffle(64).split([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe2311",
   "metadata": {},
   "source": [
    "### Add text label\n",
    "Since we are training a sequence-to-sequence model, the output of the model needs to be text, which in our case is \"negative\", \"neutral\" or \"positive\". Therefore, we need to add a text label to in addition to the numeric label (0, 1 or 2) in each entry. This is achieved through the `add_text_label` function. The function is mapped onto each entry in the training and validation datasets through the `map` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba10a3-e071-40c6-885e-5873300abe34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:42:57.768743Z",
     "iopub.status.busy": "2024-06-06T10:42:57.768504Z",
     "iopub.status.idle": "2024-06-06T10:42:57.772619Z",
     "shell.execute_reply": "2024-06-06T10:42:57.772207Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = dataset.source.ds.features[\"label\"].names\n",
    "def add_text_label(sentence, label):\n",
    "    return sentence, label, classes[label.item()]\n",
    "\n",
    "train_dataset = train_dataset.map(add_text_label, ['sentence', 'label'], ['sentence', 'label', 'text_label'])\n",
    "validation_dataset = validation_dataset.map(add_text_label, ['sentence', 'label'], ['sentence', 'label', 'text_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ca5a9",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We then tokenize the text with the tokenizer associated with the mT0 model. First, load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baa568-da2e-4584-8019-29ad9802681c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:42:57.774699Z",
     "iopub.status.busy": "2024-06-06T10:42:57.774369Z",
     "iopub.status.idle": "2024-06-06T10:42:59.670167Z",
     "shell.execute_reply": "2024-06-06T10:42:59.669601Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c7046",
   "metadata": {},
   "source": [
    "Next, modify the `BaseMapFunction` from MindNLP to wrap up the tokenization steps.\n",
    "\n",
    "Note that both the `sentence` and the `text_label` columns needs to be tokenized.\n",
    "\n",
    "In addition, to avoid unexpected behavior due to multiple threads attempting to tokenize data at the same time, we use `Lock` from the `threading` module to ensure only one thread can perform the tokenization at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5a09d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:42:59.673012Z",
     "iopub.status.busy": "2024-06-06T10:42:59.672758Z",
     "iopub.status.idle": "2024-06-06T10:42:59.677202Z",
     "shell.execute_reply": "2024-06-06T10:42:59.676763Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from mindnlp.dataset import BaseMapFunction\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "max_length = 128\n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, sentence, label, text_label):\n",
    "        lock.acquire()\n",
    "        model_inputs = tokenizer(sentence, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        labels = tokenizer(text_label, max_length=3, padding=\"max_length\", truncation=True)\n",
    "        lock.release()\n",
    "        labels = labels['input_ids']\n",
    "        labels = np.where(np.equal(labels, tokenizer.pad_token_id), -100, labels)\n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845f693",
   "metadata": {},
   "source": [
    "Next, we apply the map function, shuffle the dataset if necessary and batch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f976db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:42:59.679364Z",
     "iopub.status.busy": "2024-06-06T10:42:59.679004Z",
     "iopub.status.idle": "2024-06-06T10:42:59.684676Z",
     "shell.execute_reply": "2024-06-06T10:42:59.684255Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(dataset, tokenizer, batch_size=None, shuffle=True):\n",
    "    input_colums=['sentence', 'label', 'text_label']\n",
    "    output_columns=['input_ids', 'attention_mask', 'labels']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(64)\n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = get_dataset(train_dataset, tokenizer, batch_size=batch_size)\n",
    "eval_dataset = get_dataset(validation_dataset, tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafa827",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now we have the model and datasets ready, let's prepare for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7baa03f",
   "metadata": {},
   "source": [
    "### Optimizer and learning rate scheduler\n",
    "\n",
    "We set up the optimizer for updating the model parameters, alongside a learning rate scheduler that manages the learning rate throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9840cd1-3d3b-4968-a259-fd9c75e6aff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:42:59.686712Z",
     "iopub.status.busy": "2024-06-06T10:42:59.686448Z",
     "iopub.status.idle": "2024-06-06T10:43:00.042733Z",
     "shell.execute_reply": "2024-06-06T10:43:00.042240Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindnlp.modules.optimization import get_linear_schedule_with_warmup\n",
    "import mindspore.experimental.optim as optim\n",
    "\n",
    "# Setting up optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(model.trainable_params(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3 # Number of iterations over the entire training dataset\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_dataset) * num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3872a1d",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "Next, define the function that controls each step of training.\n",
    "\n",
    "Define `forward_fn` which executes the model's forward pass to compute the loss.\n",
    "\n",
    "Then pass `forward_fn` to `mindspore.value_and_grad` to create `grad_fn` that computes both the loss and gradients needed for parameter updates.\n",
    "\n",
    "Define `train_step` that updates the model's parameters according to the computed gradients, which will be called in each step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1a27b-3adb-4d45-9796-016eed922b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T10:43:00.045048Z",
     "iopub.status.busy": "2024-06-06T10:43:00.044819Z",
     "iopub.status.idle": "2024-06-06T10:43:00.054462Z",
     "shell.execute_reply": "2024-06-06T10:43:00.054036Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import ops\n",
    "\n",
    "# Forward function to compute the loss\n",
    "def forward_fn(**batch):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Gradient function to compute gradients for optimization\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "# Define the training step function\n",
    "def train_step(**batch):\n",
    "    loss, grads = grad_fn(**batch)\n",
    "    optimizer(grads)  # Apply gradients to optimizer for updating model parameters\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c9bcb",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Now everything is ready, let's implement the training and evaluation loop and excute the training process.\n",
    "\n",
    "This process optimizes the model's parameters through multiple iterations over the dataset, i.e. multiple epochs, and assesses its performance on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0ad0f-0459-48a7-ba3d-7b20250e70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop across epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train(True)\n",
    "    total_loss = 0\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    # Iterate over each entry in the training dataset\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "        loss = train_step(**batch)\n",
    "        total_loss += loss.float()  # Accumulate loss for monitoring\n",
    "        lr_scheduler.step()  # Update learning rate based on scheduler\n",
    "\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    # Iterate over each entry in the evaluation dataset\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        with mindspore._no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(ops.argmax(outputs.logits, -1).asnumpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataset)\n",
    "    eval_ppl = ops.exp(eval_epoch_loss) # Perplexity\n",
    "    train_epoch_loss = total_loss / len(train_dataset)\n",
    "    train_ppl = ops.exp(train_epoch_loss) # Perplexity\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2dbf04",
   "metadata": {},
   "source": [
    "Let's break down the training loop implementation and understand the key components:\n",
    "\n",
    "* Model Training Mode\n",
    "\n",
    "    Before the training starts, the model is set to training mode by `model.set_train(True)`. Before the evaluation, the training-specific behaviour of the model is disabled by `model.set_train(False)`.\n",
    "\n",
    "* Loss and Perplexity\n",
    "\n",
    "    `total_loss = 0` initializes and `total_loss += loss.float()` accumulates the total loss for each batch within an epoch. This accumulation is crucial for monitoring the model’s performance.\n",
    "\n",
    "    The average loss and the perplexity (PPL), which is a common metric for language models, are reported in the printed message.\n",
    "\n",
    "* Learning Rate Scheduler\n",
    "\n",
    "    `lr_scheduler.step()` adjusts the learning rate after processing each batch, according to the predefined schedule. This is vital for effective learning, helping to converge faster or escape local minima.\n",
    "\n",
    "* Evaluation Loop\n",
    "\n",
    "    During evaluation, in addition to `model.set_train(False)`, `mindspore._no_grad()` ensures that gradients are not computed during the evaluation phase, which conserves memory and computations.\n",
    "    The `tokenizer.batch_decode()` function converts the output logits from the model back into readable text. This is useful for inspecting what the model predicts and for further qualitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb965d",
   "metadata": {},
   "source": [
    "## After training\n",
    "Now that we have finished the training, we can assess its performance and save the trained model for future use.\n",
    "\n",
    "### Accuracy compuation and check the predicited results\n",
    "\n",
    "Let's comupute the accuracy of the predictions made on the validation dataset. Accuracy is a direct measure of how often the model's predictions match the actual labels, providing a straightforward metric to reflect the model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8650a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:00:43.459859Z",
     "iopub.status.busy": "2024-06-06T11:00:43.459473Z",
     "iopub.status.idle": "2024-06-06T11:00:43.749715Z",
     "shell.execute_reply": "2024-06-06T11:00:43.749242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize counters for correct predictions and total predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# List to store actual labels for comparison\n",
    "ground_truth = []\n",
    "\n",
    "# Compare each predicted label with the true label\n",
    "for pred, data in zip(eval_preds, validation_dataset.create_dict_iterator(output_numpy=True)):\n",
    "    true = str(data['text_label'])\n",
    "    ground_truth.append(true)\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Calculate the percentage of correct predictions\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Output the accuracy and sample predictions for review\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "print(f\"{eval_preds[:10]=}\")\n",
    "print(f\"{ground_truth[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf47e4",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "If you are satisfied with the result, you can save the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f697d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:00:43.751930Z",
     "iopub.status.busy": "2024-06-06T11:00:43.751562Z",
     "iopub.status.idle": "2024-06-06T11:00:45.290504Z",
     "shell.execute_reply": "2024-06-06T11:00:45.289942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_id = f\"../../output/{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6364ef",
   "metadata": {},
   "source": [
    "## Use the model for inference\n",
    "\n",
    "Now let's load the saved model and demonstrate how to use it for making predictions on new data.\n",
    "\n",
    "To load the model that has been trained with PEFT, we first load the base model with `AutoModelForSeq2SeqLM.from_pretrained`. On top of it, we add the trained PEFT adapter to the model with `PeftModel.from_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cedbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:00:45.292955Z",
     "iopub.status.busy": "2024-06-06T11:00:45.292504Z",
     "iopub.status.idle": "2024-06-06T11:00:54.917224Z",
     "shell.execute_reply": "2024-06-06T11:00:54.916619Z"
    }
   },
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForSeq2SeqLM\n",
    "from mindnlp.peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = f\"../../output/{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "# Load the model configuration\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the pretrained adapter\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e9537",
   "metadata": {},
   "source": [
    "Next, retrieve an entry from the validation dataset, or alternatively create an entry on your own.\n",
    "\n",
    "We tokenize the `'sentence'` in this entry and use it as inputs into the model. Excute it and be curious about what the model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cdde4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T11:00:54.920036Z",
     "iopub.status.busy": "2024-06-06T11:00:54.919534Z",
     "iopub.status.idle": "2024-06-06T11:00:56.663376Z",
     "shell.execute_reply": "2024-06-06T11:00:56.662798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve an entry from the validation dataset.\n",
    "# example = next(validation_dataset.create_dict_iterator(output_numpy=True)) # Get an example entry from the validation dataset\n",
    "# print(example['sentence'])\n",
    "# print(example['text_label'])\n",
    "\n",
    "# Alternatively, create your own text\n",
    "example = {'sentence': 'Nvidia Tops $3 Trillion in Market Value, Leapfrogging Apple.'}\n",
    "\n",
    "inputs = tokenizer(example['sentence'], return_tensors=\"ms\") # Get the tokenized text label\n",
    "print(inputs)\n",
    "\n",
    "model.set_train(False)\n",
    "with mindspore._no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10) # Predict the text label using the trained model\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.asnumpy(), skip_special_tokens=True)) # Print decoded text label from the prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
