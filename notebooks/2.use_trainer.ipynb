{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f084b20-9e1a-4d3b-9b8f-212dd578a77e",
   "metadata": {},
   "source": [
    "# Use Trainer\n",
    "\n",
    "In the [Quick Start](./0.quick_start.ipynb) tutorial, we have learned using the `Traienr` API to fine-tune a model.\n",
    "\n",
    "This tutorial will give you a comprehensive description of configure `Trainer` to train your model for optimal outcomes.\n",
    "\n",
    "The `TrainingArguments` and `Trainer` classes from MindNLP streamline the process of training machine learning models. `TrainingArguments` allows you to easily configure essential training parameters. `Trainer` then leverages these configurations to efficiently handle the entire training loop. Together, these tools abstract away much of the complexity of the training task, enabling both novices and experts to effectively optimize their models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64650eb2-aab4-417e-b0ab-6f5b0288ee4b",
   "metadata": {},
   "source": [
    "## Configure training parameters\n",
    "By creating a `TrainingArguments` object, you can specify the desired configuration for the training process.\n",
    "\n",
    "The following is a code snippet that instantiate a `TrainingArugments` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ff04f-7d96-4770-a6c3-dc476fb39397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../output\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1a0f0-9da1-43af-9948-2f51698c7c85",
   "metadata": {},
   "source": [
    "Let's break down the code to understand each argument in detail.\n",
    "### Basic parameters\n",
    "* `output_dir`\n",
    "\n",
    "  This parameter specifies the directory where the model's checkpoints and training outputs will be saved.\n",
    "* `num_train_epochs`\n",
    "\n",
    "  This parameter defines the total number of training cycles through the entire dataset that the model will undergo.\n",
    "\n",
    "  Adjusting the number of training epochs directly impacts how well the model learns from the dataset. A higher number of epochs allows the model to learn more from the training data and achieve better results. However, setting too many epochs can lead to overfitting, where the model performs well on training data but poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dac375-ee69-4ad6-a069-fd470740efba",
   "metadata": {},
   "source": [
    "### Optimizer parameters\n",
    "`TrainingArguments` allows you to specify parameters of the optimizer, these include:\n",
    "\n",
    "* `optim`\n",
    "\n",
    "  This parameter specifies the optimizer to be used for training the model. So far, MindNLP supports AdamW and SGD. You can choose the optimizer by setting this parameter to `\"adamw\"` or `\"sgd\"`. By default, `TrainingArguments` chooses AdamW.\n",
    "\n",
    "* `learning_rate`\n",
    "  \n",
    "  This patameter sets the initial learning rate for the optimizer, determining the step size at each iteration during loss minimization.\n",
    "\n",
    "  This is one of first parameters to check if your training process fails to converge properly.\n",
    "      \n",
    "  A higher learning rate can converge faster, but if too high, it may overshoot the minimum, causing instability by bouncing around or diverging from the optimal weights. Conversely, a too-low learning rate can lead to slow convergence, potentially getting stuck in local minima.\n",
    "\n",
    "* Advanced parameters for optimizer\n",
    "\n",
    "  Using default values of the following advanced parameters suffices for most of the training. Interested readers and experts can adjust them to achieve better training result.\n",
    "\n",
    "\n",
    "    - `weight_decay`: This parameter helps prevent overfitting by penalizing large weights. Weight decay is a regularization term added to the loss function, which effectively reduces the magnitude of weights in the model.\n",
    "\n",
    "    - `adam_beta1` and `adam_beta2`: These parameters are specific to the AdamW optimizer. `adam_beta1` controls the exponential decay rate for the first moment estimates (similar to the momentum term), and `adam_beta2` controls the exponential decay rate for the second-moment estimates (related to the adaptive learning rate).\n",
    "\n",
    "    - `adam_epsilon`: This is a very small number to prevent any division by zero in the implementation of the Adam optimizer. It's used to improve numerical stability.\n",
    "\n",
    "    - `max_grad_norm`: This is used for gradient clipping, a technique to prevent exploding gradients in deep neural networks. Clipping the gradients at a specified norm helps in stabilizing the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4015b7-6534-4f61-ab24-8eb1b27d4875",
   "metadata": {},
   "source": [
    "### Batch size parameters\n",
    "Parameters related to batch size allow you to control how many examples are processed at a time during training and evaluation phases. Here's a summary of these parameters:\n",
    "\n",
    "* `per_device_train_batch_size`\n",
    "\n",
    "  This parameter sets the batch size for each training step.\n",
    "\n",
    "  A large batch size can speed up training and make updates more consistent, but it might need more memory and could potentially converge to suboptimal minima.\n",
    "\n",
    "  On the other hand, a smaller batch size requires less memory and might help the model learn better, though it could slow down the training process.\n",
    "\n",
    "* `per_device_eval_batch_size`\n",
    "\n",
    "  This parameter sets the batch size for each step in the evaluation.\n",
    "\n",
    "Note: if you already batched your dataset before hand, by calling `dataset.batch()` for example, you would want to set the batch size in the `TrainingArguments` to 1, so the `Trainer` will not further batch on top your already batched dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a4d80-e69c-44ae-86fb-130b3f39eb70",
   "metadata": {},
   "source": [
    "### Strategies for evaluation, saving and logging\n",
    "\n",
    "The `TrainingArguments` allows you to define the strategies for evaluation, saving and logging during the training process.\n",
    "\n",
    "#### Evaluation strategy\n",
    "\n",
    "The `evaluation_strategy` parameter determines when the model should be evaluated during the training process. Evalutation is essential for monitoring model performance on a validation dataset, which is normally different from the training dataset. \n",
    "\n",
    "The strategy of performing evaluation can be:\n",
    "- \"no\": No evaluation is performed.\n",
    "- \"steps\": Evaluation occurs at specified intervals in terms of training steps.\n",
    "  If the \"steps\" strategy is chosen, `eval_steps` needs to specified to control how many steps of training should occur between each evaluation.\n",
    "- \"epoch\": Evaluation happens at the end of each epoch.\n",
    "\n",
    "#### Save strategy\n",
    "\n",
    "The `save_strategy` parameter controls when the model's state should be saved during the training process. Saving is crucial for preserving model checkpoints at different stages of training, which can be useful for recovery or further fine-tuning.\n",
    "\n",
    "The strategy for saving can be:\n",
    "- \"no\": No saving is performed.\n",
    "- \"steps\": Saving occurs at specified intervals in terms of training steps.\n",
    "  If the \"steps\" strategy is chosen, `save_steps` needs to be specified to control how many steps of training should occur between each saved checkpoint.\n",
    "- \"epoch\": Saving happens at the end of each epoch.\n",
    "\n",
    "\n",
    "#### Logging strategy\n",
    "The `logging_strategy` parameter determines when the model's training metrics should be logged during the training process. Logging is important for tracking progress, understanding model behavior, and diagnosing issues during training.\n",
    "\n",
    "The strategy for logging can be:\n",
    "- \"no\": No logging is performed.\n",
    "- \"steps\": Logging occurs at specified intervals in terms of training steps.\n",
    "  If the \"steps\" strategy is chosen, the logging_steps needs to be specified to control how many steps of training should occur between each logging event.\n",
    "- \"epoch\": Logging happens at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6e2d8-711b-427b-b298-f7576d5ed67f",
   "metadata": {},
   "source": [
    "## Create the trainer\n",
    "\n",
    "The `Trainer` in MindNLP accepts configurations from a `TrainingArgument` object and handle the entire training loop.\n",
    "\n",
    "Assume you have defined the `model`, `dataset_train`, `dataset_val` and a function `compute_metrics`, for example as in the [Quick Start](./0.quick_start.ipynb) tutorial, a `Trainer` object can be created using the following code:\n",
    "\n",
    "```python\n",
    "from mindnlp.engine import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f79ebe-0f72-45a9-baf8-053de49c2348",
   "metadata": {},
   "source": [
    "Here's an explanation of key arguments used to customize training behavior:\n",
    "\n",
    "* `model`: Pass the model instance you plan to train. This is the primary object that will undergo the training process.\n",
    "* `args`: Your `TrainingArgument` object that sets up configurations for training.\n",
    "* `train_dataset`, `eval_dataset`: These are the datasets on which the model will be trained or evaluated, respectively. Rember to preprocess the dataset as in the [Data Preprocess](./1.data_preprocess.ipynb) tutorial.\n",
    "\n",
    "* `compute_metrics`: A function that calculates specific performance metrics from the model's predictions. It takes a `mindnlp.engine.utils.EvalPrediction` object, which contains the predictions and labels, and returns the metric results.\n",
    "\n",
    "  An example of `compute_metrics` function can be defined as follows:\n",
    "  ```python\n",
    "  import evaluate\n",
    "  import numpy as np\n",
    "  from mindnlp.engine.utils import EvalPrediction\n",
    "\n",
    "  metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "  def compute_metrics(eval_pred: EvalPrediction):\n",
    "      logits, labels = eval_pred\n",
    "      predictions = np.argmax(logits, axis=-1)\n",
    "      return metric.compute(predictions=predictions, references=labels)\n",
    "  ```\n",
    "\n",
    "  Note that at the moment we still need to load the accuracy metric fromthe Hugging Face evaluate module.\n",
    "\n",
    "Once the trainer is created, run `trainer.train()` to start your training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
