{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DreamBooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text-to-image models like Stable Diffusion given just a few (3-5) images of a subject. It allows the model to generate contextualized images of the subject in different scenes, poses, and views.\n",
    "\n",
    "![Dreambooth examples from the project's blog](https://dreambooth.github.io/DreamBooth_files/teaser_static.jpg)\n",
    "<small>Dreambooth examples from the <a href=\"https://dreambooth.github.io\">project's blog.</a></small>\n",
    "\n",
    "This guide will show you how to finetune DreamBooth with the [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) model for various GPU sizes, and with Flax. All the training scripts for DreamBooth used in this guide can be found [here](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) if you're interested in digging deeper and seeing how things work.\n",
    "\n",
    "Before running the scripts, make sure you install the library's training dependencies. We also recommend installing ðŸ§¨ Diffusers from the `main` GitHub branch:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/diffusers\n",
    "pip install -U -r diffusers/examples/dreambooth/requirements.txt\n",
    "```\n",
    "\n",
    "xFormers is not part of the training requirements, but we recommend you [install](https://huggingface.co/docs/diffusers/main/en/training/../optimization/xformers) it if you can because it could make your training faster and less memory intensive.\n",
    "\n",
    "After all the dependencies have been set up, initialize a [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate/) environment with:\n",
    "\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "To setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n",
    "\n",
    "```bash\n",
    "accelerate config default\n",
    "```\n",
    "\n",
    "Or if your environment doesn't support an interactive shell like a notebook, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, download a [few images of a dog](https://huggingface.co/datasets/diffusers/dog-example) to DreamBooth with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./dog\"\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example\",\n",
    "    local_dir=local_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own dataset, take a look at the [Create a dataset for training](https://huggingface.co/docs/diffusers/main/en/training/create_dataset) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip warning={true}>\n",
    "\n",
    "DreamBooth finetuning is very sensitive to hyperparameters and easy to overfit. We recommend you take a look at our [in-depth analysis](https://huggingface.co/blog/dreambooth) with recommended settings for different subjects to help you choose the appropriate hyperparameters.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Set the `INSTANCE_DIR` environment variable to the path of the directory containing the dog images.\n",
    "\n",
    "Specify the `MODEL_NAME` environment variable (either a Hub model repository id or a path to the directory containing the model weights) and pass it to the `pretrained_model_name_or_path` argument. The `instance_prompt` argument is a text prompt that contains a unique identifier, such as `sks`, and the class the image belongs to, which in this example is `a photo of a sks dog`.\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export OUTPUT_DIR=\"path_to_saved_model\"\n",
    "```\n",
    "\n",
    "Then you can launch the training script (you can find the full training script [here](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py)) with the following command:\n",
    "\n",
    "```bash\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=400 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with prior-preserving loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior preservation is used to avoid overfitting and language-drift (check out the [paper](https://arxiv.org/abs/2208.12242) to learn more if you're interested). For prior preservation, you use other images of the same class as part of the training process. The nice thing is that you can generate those images using the Stable Diffusion model itself! The training script will save the generated images to a local path you specify.\n",
    "\n",
    "The authors recommend generating `num_epochs * num_samples` images for prior preservation. In most cases, 200-300 images work well.\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export CLASS_DIR=\"path_to_class_images\"\n",
    "export OUTPUT_DIR=\"path_to_saved_model\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --class_prompt=\"a photo of dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=200 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the text encoder and UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script also allows you to finetune the `text_encoder` along with the `unet`. In our experiments (check out the [Training Stable Diffusion with DreamBooth using ðŸ§¨ Diffusers](https://huggingface.co/blog/dreambooth) post for more details), this yields much better results, especially when generating images of faces.\n",
    "\n",
    "<Tip warning={true}>\n",
    "\n",
    "Training the text encoder requires additional memory and it won't fit on a 16GB GPU. You'll need at least 24GB VRAM to use this option.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Pass the `--train_text_encoder` argument to the training script to enable finetuning the `text_encoder` and `unet`:\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export CLASS_DIR=\"path_to_class_images\"\n",
    "export OUTPUT_DIR=\"path_to_saved_model\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --class_prompt=\"a photo of dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_checkpointing \\\n",
    "  --learning_rate=2e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=200 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Low-Rank Adaptation of Large Language Models (LoRA), a fine-tuning technique for accelerating training large models, on DreamBooth. For more details, take a look at the [LoRA training](https://huggingface.co/docs/diffusers/main/en/training/./lora#dreambooth) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving checkpoints while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to overfit while training with Dreambooth, so sometimes it's useful to save regular checkpoints during the training process. One of the intermediate checkpoints might actually work better than the final model! Pass the following argument to the training script to enable saving checkpoints:\n",
    "\n",
    "```bash\n",
    "  --checkpointing_steps=500\n",
    "```\n",
    "\n",
    "This saves the full training state in subfolders of your `output_dir`. Subfolder names begin with the prefix `checkpoint-`, followed by the number of steps performed so far; for example, `checkpoint-1500` would be a checkpoint saved after 1500 training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training from a saved checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to resume training from any of the saved checkpoints, you can pass the argument `--resume_from_checkpoint` to the script and specify the name of the checkpoint you want to use. You can also use the special string `\"latest\"` to resume from the last saved checkpoint (the one with the largest number of steps). For example, the following would resume training from the checkpoint saved after 1500 steps:\n",
    "\n",
    "```bash\n",
    "  --resume_from_checkpoint=\"checkpoint-1500\"\n",
    "```\n",
    "\n",
    "This is a good opportunity to tweak some of your hyperparameters if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from a saved checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved checkpoints are stored in a format suitable for resuming training. They not only include the model weights, but also the state of the optimizer, data loaders, and learning rate.\n",
    "\n",
    "If you have **`\"accelerate>=0.16.0\"`** installed, use the following code to run \n",
    "inference from an intermediate checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel\n",
    "import torch\n",
    "\n",
    "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/unet\")\n",
    "\n",
    "# if you have trained with `--args.train_text_encoder` make sure to also load the text encoder\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/text_encoder\")\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id, unet=unet, text_encoder=text_encoder, dtype=torch.float16)\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "# Perform inference, or save, or push to the hub\n",
    "pipeline.save_pretrained(\"dreambooth-pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have **`\"accelerate<0.16.0\"`** installed, you need to convert it to an inference pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Use text_encoder if `--train_text_encoder` was used for the initial training\n",
    "unet, text_encoder = accelerator.prepare(pipeline.unet, pipeline.text_encoder)\n",
    "\n",
    "# Restore state from a checkpoint path. You have to use the absolute path here.\n",
    "accelerator.load_state(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100\")\n",
    "\n",
    "# Rebuild the pipeline with the unwrapped models (assignment to .unet and .text_encoder should work too)\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    unet=accelerator.unwrap_model(unet),\n",
    "    text_encoder=accelerator.unwrap_model(text_encoder),\n",
    ")\n",
    "\n",
    "# Perform inference, or save, or push to the hub\n",
    "pipeline.save_pretrained(\"dreambooth-pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations for different GPU sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your hardware, there are a few different ways to optimize DreamBooth on GPUs from 16GB to just 8GB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xFormers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[xFormers](https://github.com/facebookresearch/xformers) is a toolbox for optimizing Transformers, and it includes a [memory-efficient attention](https://facebookresearch.github.io/xformers/components/ops.html#module-xformers.ops) mechanism that is used in ðŸ§¨ Diffusers. You'll need to [install xFormers](https://huggingface.co/docs/diffusers/main/en/training/./optimization/xformers) and then add the following argument to your training script:\n",
    "\n",
    "```bash\n",
    "  --enable_xformers_memory_efficient_attention\n",
    "```\n",
    "\n",
    "xFormers is not available in Flax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set gradients to none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way you can lower your memory footprint is to [set the gradients](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) to `None` instead of zero. However, this may change certain behaviors, so if you run into any issues, try removing this argument. Add the following argument to your training script to set the gradients to `None`:\n",
    "\n",
    "```bash\n",
    "  --set_grads_to_none\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16GB GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of gradient checkpointing and [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizer, it's possible to train DreamBooth on a 16GB GPU. Make sure you have bitsandbytes installed:\n",
    "\n",
    "```bash\n",
    "pip install bitsandbytes\n",
    "```\n",
    "\n",
    "Then pass the `--use_8bit_adam` option to the training script:\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export CLASS_DIR=\"path_to_class_images\"\n",
    "export OUTPUT_DIR=\"path_to_saved_model\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --class_prompt=\"a photo of dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=2 --gradient_checkpointing \\\n",
    "  --use_8bit_adam \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=200 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12GB GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run DreamBooth on a 12GB GPU, you'll need to enable gradient checkpointing, the 8-bit optimizer, xFormers, and set the gradients to `None`: \n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export CLASS_DIR=\"path-to-class-images\"\n",
    "export OUTPUT_DIR=\"path-to-save-model\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --class_prompt=\"a photo of dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "  --use_8bit_adam \\\n",
    "  --enable_xformers_memory_efficient_attention \\\n",
    "  --set_grads_to_none \\\n",
    "  --learning_rate=2e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=200 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 GB GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 8GB GPUs, you'll need the help of [DeepSpeed](https://www.deepspeed.ai/) to offload some\n",
    "tensors from the VRAM to either the CPU or NVME, enabling training with less GPU memory.\n",
    "\n",
    "Run the following command to configure your ðŸ¤— Accelerate environment:\n",
    "\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "During configuration, confirm that you want to use DeepSpeed. Now it's possible to train on under 8GB VRAM by combining DeepSpeed stage 2, fp16 mixed precision, and offloading the model parameters and the optimizer state to the CPU. The drawback is that this requires more system RAM, about 25 GB. See [the DeepSpeed documentation](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) for more configuration options.\n",
    "\n",
    "You should also change the default Adam optimizer to DeepSpeed's optimized version of Adam\n",
    "[`deepspeed.ops.adam.DeepSpeedCPUAdam`](https://deepspeed.readthedocs.io/en/latest/optimizers.html#adam-cpu) for a substantial speedup. Enabling `DeepSpeedCPUAdam` requires your system's CUDA toolchain version to be the same as the one installed with PyTorch. \n",
    "\n",
    "8-bit optimizers don't seem to be compatible with DeepSpeed at the moment.\n",
    "\n",
    "Launch training with the following command:\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
    "export INSTANCE_DIR=\"./dog\"\n",
    "export CLASS_DIR=\"path_to_class_images\"\n",
    "export OUTPUT_DIR=\"path_to_saved_model\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --class_data_dir=$CLASS_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --class_prompt=\"a photo of dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --sample_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=200 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --mixed_precision=fp16 \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have trained a model, specify the path to where the model is saved, and use it for inference in the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline). Make sure your prompts include the special `identifier` used during training (`sks` in the previous examples).\n",
    "\n",
    "If you have **`\"accelerate>=0.16.0\"`** installed, you can use the following code to run \n",
    "inference from an intermediate checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"path_to_saved_model\"\n",
    "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"A photo of sks dog in a bucket\"\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "image.save(\"dog-bucket.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also run inference from any of the [saved training checkpoints](#inference-from-a-saved-checkpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the lora and full dreambooth scripts to train the text to image [IF model](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) and the stage II upscaler \n",
    "[IF model](https://huggingface.co/DeepFloyd/IF-II-L-v1.0).\n",
    "\n",
    "Note that IF has a predicted variance, and our finetuning scripts only train the models predicted error, so for finetuned IF models we switch to a fixed\n",
    "variance schedule. The full finetuning scripts will update the scheduler config for the full saved model. However, when loading saved LoRA weights, you\n",
    "must also update the pipeline's scheduler config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\")\n",
    "\n",
    "pipe.load_lora_weights(\"<lora weights path>\")\n",
    "\n",
    "# Update scheduler config to fixed variance schedule\n",
    "pipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type=\"fixed_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, a few alternative cli flags are needed for IF.\n",
    "\n",
    "`--resolution=64`: IF is a pixel space diffusion model. In order to operate on un-compressed pixels, the input images are of a much smaller resolution. \n",
    "\n",
    "`--pre_compute_text_embeddings`: IF uses [T5](https://huggingface.co/docs/transformers/model_doc/t5) for its text encoder. In order to save GPU memory, we pre compute all text embeddings and then de-allocate\n",
    "T5.\n",
    "\n",
    "`--tokenizer_max_length=77`: T5 has a longer default text length, but the default IF encoding procedure uses a smaller number.\n",
    "\n",
    "`--text_encoder_use_attention_mask`: T5 passes the attention mask to the text encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips and Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find LoRA to be sufficient for finetuning the stage I model as the low resolution of the model makes representing finegrained detail hard regardless.\n",
    "\n",
    "For common and/or not-visually complex object concepts, you can get away with not-finetuning the upscaler. Just be sure to adjust the prompt passed to the\n",
    "upscaler to remove the new token from the instance prompt. I.e. if your stage I prompt is \"a sks dog\", use \"a dog\" for your stage II prompt.\n",
    "\n",
    "For finegrained detail like faces that aren't present in the original training set, we find that full finetuning of the stage II upscaler is better than \n",
    "LoRA finetuning stage II.\n",
    "\n",
    "For finegrained detail like faces, we find that lower learning rates along with larger batch sizes work best.\n",
    "\n",
    "For stage II, we find that lower learning rates are also needed.\n",
    "\n",
    "We found experimentally that the DDPM scheduler with the default larger number of denoising steps to sometimes work better than the DPM Solver scheduler\n",
    "used in the training scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage II additional validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage II validation requires images to upscale, we can download a downsized version of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./dog_downsized\"\n",
    "snapshot_download(\n",
    "    \"diffusers/dog-example-downsized\",\n",
    "    local_dir=local_dir,\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=\".gitattributes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF stage I LoRA Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training configuration requires ~28 GB VRAM.\n",
    "\n",
    "```sh\n",
    "export MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\n",
    "export INSTANCE_DIR=\"dog\"\n",
    "export OUTPUT_DIR=\"dreambooth_dog_lora\"\n",
    "\n",
    "accelerate launch train_dreambooth_lora.py \\\n",
    "  --report_to wandb \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a sks dog\" \\\n",
    "  --resolution=64 \\\n",
    "  --train_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --scale_lr \\\n",
    "  --max_train_steps=1200 \\\n",
    "  --validation_prompt=\"a sks dog\" \\\n",
    "  --validation_epochs=25 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --pre_compute_text_embeddings \\\n",
    "  --tokenizer_max_length=77 \\\n",
    "  --text_encoder_use_attention_mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF stage II LoRA Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--validation_images`: These images are upscaled during validation steps.\n",
    "\n",
    "`--class_labels_conditioning=timesteps`: Pass additional conditioning to the UNet needed for stage II.\n",
    "\n",
    "`--learning_rate=1e-6`: Lower learning rate than stage I.\n",
    "\n",
    "`--resolution=256`: The upscaler expects higher resolution inputs\n",
    "\n",
    "```sh\n",
    "export MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\n",
    "export INSTANCE_DIR=\"dog\"\n",
    "export OUTPUT_DIR=\"dreambooth_dog_upscale\"\n",
    "export VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n",
    "\n",
    "python train_dreambooth_lora.py \\\n",
    "    --report_to wandb \\\n",
    "    --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "    --instance_data_dir=$INSTANCE_DIR \\\n",
    "    --output_dir=$OUTPUT_DIR \\\n",
    "    --instance_prompt=\"a sks dog\" \\\n",
    "    --resolution=256 \\\n",
    "    --train_batch_size=4 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --learning_rate=1e-6 \\ \n",
    "    --max_train_steps=2000 \\\n",
    "    --validation_prompt=\"a sks dog\" \\\n",
    "    --validation_epochs=100 \\\n",
    "    --checkpointing_steps=500 \\\n",
    "    --pre_compute_text_embeddings \\\n",
    "    --tokenizer_max_length=77 \\\n",
    "    --text_encoder_use_attention_mask \\\n",
    "    --validation_images $VALIDATION_IMAGES \\\n",
    "    --class_labels_conditioning=timesteps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF Stage I Full Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--skip_save_text_encoder`: When training the full model, this will skip saving the entire T5 with the finetuned model. You can still load the pipeline\n",
    "with a T5 loaded from the original model.\n",
    "\n",
    "`use_8bit_adam`: Due to the size of the optimizer states, we recommend training the full XL IF model with 8bit adam. \n",
    "\n",
    "`--learning_rate=1e-7`: For full dreambooth, IF requires very low learning rates. With higher learning rates model quality will degrade. Note that it is \n",
    "likely the learning rate can be increased with larger batch sizes.\n",
    "\n",
    "Using 8bit adam and a batch size of 4, the model can be trained in ~48 GB VRAM.\n",
    "\n",
    "```sh\n",
    "export MODEL_NAME=\"DeepFloyd/IF-I-XL-v1.0\"\n",
    "\n",
    "export INSTANCE_DIR=\"dog\"\n",
    "export OUTPUT_DIR=\"dreambooth_if\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --resolution=64 \\\n",
    "  --train_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-7 \\\n",
    "  --max_train_steps=150 \\\n",
    "  --validation_prompt \"a photo of sks dog\" \\\n",
    "  --validation_steps 25 \\\n",
    "  --text_encoder_use_attention_mask \\\n",
    "  --tokenizer_max_length 77 \\\n",
    "  --pre_compute_text_embeddings \\\n",
    "  --use_8bit_adam \\\n",
    "  --set_grads_to_none \\\n",
    "  --skip_save_text_encoder \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF Stage II Full Dreambooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--learning_rate=5e-6`: With a smaller effective batch size of 4, we found that we required learning rates as low as\n",
    "1e-8.\n",
    "\n",
    "`--resolution=256`: The upscaler expects higher resolution inputs\n",
    "\n",
    "`--train_batch_size=2` and `--gradient_accumulation_steps=6`: We found that full training of stage II particularly with\n",
    "faces required large effective batch sizes.\n",
    "\n",
    "```sh\n",
    "export MODEL_NAME=\"DeepFloyd/IF-II-L-v1.0\"\n",
    "export INSTANCE_DIR=\"dog\"\n",
    "export OUTPUT_DIR=\"dreambooth_dog_upscale\"\n",
    "export VALIDATION_IMAGES=\"dog_downsized/image_1.png dog_downsized/image_2.png dog_downsized/image_3.png dog_downsized/image_4.png\"\n",
    "\n",
    "accelerate launch train_dreambooth.py \\\n",
    "  --report_to wandb \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a sks dog\" \\\n",
    "  --resolution=256 \\\n",
    "  --train_batch_size=2 \\\n",
    "  --gradient_accumulation_steps=6 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --max_train_steps=2000 \\\n",
    "  --validation_prompt=\"a sks dog\" \\\n",
    "  --validation_steps=150 \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --pre_compute_text_embeddings \\\n",
    "  --tokenizer_max_length=77 \\\n",
    "  --text_encoder_use_attention_mask \\\n",
    "  --validation_images $VALIDATION_IMAGES \\\n",
    "  --class_labels_conditioning timesteps \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
