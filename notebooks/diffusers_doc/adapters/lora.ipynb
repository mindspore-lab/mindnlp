{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Rank Adaptation of Large Language Models (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip warning={true}>\n",
    "\n",
    "Currently, LoRA is only supported for the attention layers of the `UNet2DConditionalModel`. We also \n",
    "support fine-tuning the text encoder for DreamBooth with LoRA in a limited capacity. Fine-tuning the text encoder for DreamBooth generally yields better results, but it can increase compute usage.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "[Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) is a training method that accelerates the training of large models while consuming less memory. It adds pairs of rank-decomposition weight matrices (called **update matrices**) to existing weights, and **only** trains those newly added weights. This has a couple of advantages:\n",
    "\n",
    "- Previous pretrained weights are kept frozen so the model is not as prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114).\n",
    "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
    "- LoRA matrices are generally added to the attention layers of the original model. 🧨 Diffusers provides the [load_attn_procs()](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs) method to load the LoRA weights into a model's attention layers. You can control the extent to which the model is adapted toward new training images via a `scale` parameter. \n",
    "- The greater memory-efficiency allows you to run fine-tuning on consumer GPUs like the Tesla T4, RTX 3080 or even the RTX 2080 Ti! GPUs like the T4 are free and readily accessible in Kaggle or Google Colab notebooks.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "💡 LoRA is not only limited to attention layers. The authors found that amending\n",
    "the attention layers of a language model is sufficient to obtain good downstream performance with great efficiency. This is why it's common to just add the LoRA weights to the attention layers of a model. Check out the [Using LoRA for efficient Stable Diffusion fine-tuning](https://huggingface.co/blog/lora) blog for more information about how LoRA works!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "[cloneofsimo](https://github.com/cloneofsimo) was the first to try out LoRA training for Stable Diffusion in the popular [lora](https://github.com/cloneofsimo/lora) GitHub repository. 🧨 Diffusers now supports finetuning with LoRA for [text-to-image generation](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image#training-with-lora) and [DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora). This guide will show you how to do both.\n",
    "\n",
    "If you'd like to store or share your model with the community, login to your Hugging Face account (create [one](https://huggingface.co/docs/diffusers/main/en/training/hf.co/join) if you don't have one already):\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning a model like Stable Diffusion, which has billions of parameters, can be slow and difficult. With LoRA, it is much easier and faster to finetune a diffusion model. It can run on hardware with as little as 11GB of GPU RAM without resorting to tricks such as 8-bit optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training[[text-to-image-training]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune [`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) on the [Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset to generate your own Pokémon.\n",
    "\n",
    "Specify the `MODEL_NAME` environment variable (either a Hub model repository id or a path to the directory containing the model weights) and pass it to the [`pretrained_model_name_or_path`](https://huggingface.co/docs/diffusers/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained.pretrained_model_name_or_path) argument. You'll also need to set the `DATASET_NAME` environment variable to the name of the dataset you want to train on. To use your own dataset, take a look at the [Create a dataset for training](https://huggingface.co/docs/diffusers/main/en/training/create_dataset) guide.\n",
    "\n",
    "The `OUTPUT_DIR` and `HUB_MODEL_ID` variables are optional and specify where to save the model to on the Hub:\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export OUTPUT_DIR=\"/sddata/finetune/lora/pokemon\"\n",
    "export HUB_MODEL_ID=\"pokemon-lora\"\n",
    "export DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n",
    "```\n",
    "\n",
    "There are some flags to be aware of before you start training:\n",
    "\n",
    "* `--push_to_hub` stores the trained LoRA embeddings on the Hub.\n",
    "* `--report_to=wandb` reports and logs the training results to your Weights & Biases dashboard (as an example, take a look at this [report](https://wandb.ai/pcuenq/text2image-fine-tune/runs/b4k1w0tn?workspace=user-pcuenq)).\n",
    "* `--learning_rate=1e-04`, you can afford to use a higher learning rate than you normally would with LoRA.\n",
    "\n",
    "Now you're ready to launch the training (you can find the full training script [here](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py)). Training takes about 5 hours on a 2080 Ti GPU with 11GB of RAM, and it'll create and save model checkpoints and the `pytorch_lora_weights` in your repository.\n",
    "\n",
    "```bash\n",
    "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --dataset_name=$DATASET_NAME \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 --center_crop --random_flip \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-04 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --output_dir=${OUTPUT_DIR} \\\n",
    "  --push_to_hub \\\n",
    "  --hub_model_id=${HUB_MODEL_ID} \\\n",
    "  --report_to=wandb \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --validation_prompt=\"A pokemon with blue eyes.\" \\\n",
    "  --seed=1337\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference[[text-to-image-inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the model for inference by loading the base model in the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) and then the [DPMSolverMultistepScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "model_base = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRA weights from your finetuned model *on top of the base model weights*, and then move the pipeline to a GPU for faster inference. When you merge the LoRA weights with the frozen pretrained model weights, you can optionally adjust how much of the weights to merge with the `scale` parameter:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "💡 A `scale` value of `0` is the same as not using your LoRA weights and you're only using the base model weights, and a `scale` value of `1` means you're only using the fully finetuned LoRA weights. Values between `0` and `1` interpolates between the two weights.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    \"A pokemon with blue eyes.\", num_inference_steps=25, guidance_scale=7.5, cross_attention_kwargs={\"scale\": 0.5}\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\"A pokemon with blue eyes.\", num_inference_steps=25, guidance_scale=7.5).images[0]\n",
    "image.save(\"blue_pokemon.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you are loading the LoRA parameters from the Hub and if the Hub repository has\n",
    "a `base_model` tag (such as [this](https://huggingface.co/sayakpaul/sd-model-finetuned-lora-t4/blob/main/README.md?code=true#L4)), then\n",
    "you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.repocard import RepoCard\n",
    "\n",
    "lora_model_id = \"sayakpaul/sd-model-finetuned-lora-t4\"\n",
    "card = RepoCard.load(lora_model_id)\n",
    "base_model_id = card.data.to_dict()[\"base_model\"]\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DreamBooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DreamBooth](https://arxiv.org/abs/2208.12242) is a finetuning technique for personalizing a text-to-image model like Stable Diffusion to generate photorealistic images of a subject in different contexts, given a few images of the subject. However, DreamBooth is very sensitive to hyperparameters and it is easy to overfit. Some important hyperparameters to consider include those that affect the training time (learning rate, number of training steps), and inference time (number of steps, scheduler type).\n",
    "\n",
    "<Tip>\n",
    "\n",
    "💡 Take a look at the [Training Stable Diffusion with DreamBooth using 🧨 Diffusers](https://huggingface.co/blog/dreambooth) blog for an in-depth analysis of DreamBooth experiments and recommended settings.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training[[dreambooth-training]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune [`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) with DreamBooth and LoRA with some 🐶 [dog images](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ). Download and save these images to a directory. To use your own dataset, take a look at the [Create a dataset for training](https://huggingface.co/docs/diffusers/main/en/training/create_dataset) guide.\n",
    "\n",
    "To start, specify the `MODEL_NAME` environment variable (either a Hub model repository id or a path to the directory containing the model weights) and pass it to the [`pretrained_model_name_or_path`](https://huggingface.co/docs/diffusers/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained.pretrained_model_name_or_path) argument. You'll also need to set `INSTANCE_DIR` to the path of the directory containing the images. \n",
    "\n",
    "The `OUTPUT_DIR` variables is optional and specifies where to save the model to on the Hub:\n",
    "\n",
    "```bash\n",
    "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "export INSTANCE_DIR=\"path-to-instance-images\"\n",
    "export OUTPUT_DIR=\"path-to-save-model\"\n",
    "```\n",
    "\n",
    "There are some flags to be aware of before you start training:\n",
    "\n",
    "* `--push_to_hub` stores the trained LoRA embeddings on the Hub.\n",
    "* `--report_to=wandb` reports and logs the training results to your Weights & Biases dashboard (as an example, take a look at this [report](https://wandb.ai/pcuenq/text2image-fine-tune/runs/b4k1w0tn?workspace=user-pcuenq)).\n",
    "* `--learning_rate=1e-04`, you can afford to use a higher learning rate than you normally would with LoRA.\n",
    "\n",
    "Now you're ready to launch the training (you can find the full training script [here](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py)). The script creates and saves model checkpoints and the `pytorch_lora_weights.bin` file in your repository.\n",
    "\n",
    "It's also possible to additionally fine-tune the text encoder with LoRA. This, in most cases, leads\n",
    "to better results with a slight increase in the compute. To allow fine-tuning the text encoder with LoRA,\n",
    "specify the `--train_text_encoder` while launching the `train_dreambooth_lora.py` script.\n",
    "\n",
    "```bash\n",
    "accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --seed=\"0\" \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference[[dreambooth-inference]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the model for inference by loading the base model in the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_base = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRA weights from your finetuned DreamBooth model *on top of the base model weights*, and then move the pipeline to a GPU for faster inference. When you merge the LoRA weights with the frozen pretrained model weights, you can optionally adjust how much of the weights to merge with the `scale` parameter:\n",
    "\n",
    "<Tip>\n",
    "\n",
    "💡 A `scale` value of `0` is the same as not using your LoRA weights and you're only using the base model weights, and a `scale` value of `1` means you're only using the fully finetuned LoRA weights. Values between `0` and `1` interpolates between the two weights.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    \"A picture of a sks dog in a bucket.\",\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    cross_attention_kwargs={\"scale\": 0.5},\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\"A picture of a sks dog in a bucket.\", num_inference_steps=25, guidance_scale=7.5).images[0]\n",
    "image.save(\"bucket-dog.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If your LoRA parameters involve the UNet as well as the Text Encoder, then passing\n",
    "`cross_attention_kwargs={\"scale\": 0.5}` will apply the `scale` value to both the UNet \n",
    "and the Text Encoder. \n",
    "\n",
    "</Tip>\n",
    "\n",
    "Note that the use of [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.load_lora_weights) is preferred to [load_attn_procs()](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs) for loading LoRA parameters. This is because\n",
    "[load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.load_lora_weights) can handle the following situations:\n",
    "\n",
    "* LoRA parameters that don't have separate identifiers for the UNet and the text encoder (such as [`\"patrickvonplaten/lora_dreambooth_dog_example\"`](https://huggingface.co/patrickvonplaten/lora_dreambooth_dog_example)). So, you can just do:\n",
    "\n",
    "  ```py \n",
    "  pipe.load_lora_weights(lora_model_path)\n",
    "  ```\n",
    "\n",
    "* LoRA parameters that have separate identifiers for the UNet and the text encoder such as: [`\"sayakpaul/dreambooth\"`](https://huggingface.co/sayakpaul/dreambooth).\n",
    "\n",
    "**Note** that it is possible to provide a local directory path to [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.load_lora_weights) as well as [load_attn_procs()](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.UNet2DConditionLoadersMixin.load_attn_procs). To know about the supported inputs,\n",
    "refer to the respective docstrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting A1111 themed LoRA checkpoints from Diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide seamless interoperability with A1111 to our users, we support loading A1111 formatted\n",
    "LoRA checkpoints using [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.load_lora_weights) in a limited capacity.\n",
    "In this section, we explain how to load an A1111 formatted LoRA checkpoint from [CivitAI](https://civitai.com/)\n",
    "in Diffusers and perform inference with it. \n",
    "\n",
    "First, download a checkpoint. We'll use\n",
    "[this one](https://civitai.com/models/13239/light-and-shadow) for demonstration purposes. \n",
    "\n",
    "```bash\n",
    "wget https://civitai.work/api/download/models/15603 -O light_and_shadow.safetensors\n",
    "```\n",
    "\n",
    "Next, we initialize a [~DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindnlp\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"gsdf/Counterfeit-V2.5\", torch_dtype=torch.float16, safety_checker=None\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "    pipeline.scheduler.config, use_karras_sigmas=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the checkpoint downloaded from CivitAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_lora_weights(\".\", weight_name=\"light_and_shadow.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip warning={true}>\n",
    "\n",
    "If you're loading a checkpoint in the `safetensors` format, please ensure you have `safetensors` installed.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "And then it's time for running inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"masterpiece, best quality, 1girl, at dusk\"\n",
    "negative_prompt = (\"(low quality, worst quality:1.4), (bad anatomy), (inaccurate limb:1.2), \"\n",
    "                   \"bad composition, inaccurate eyes, extra digit, fewer digits, (extra arms:1.2), large breasts\")\n",
    "\n",
    "images = pipeline(prompt=prompt, \n",
    "    negative_prompt=negative_prompt, \n",
    "    width=512, \n",
    "    height=768, \n",
    "    num_inference_steps=15, \n",
    "    num_images_per_prompt=4,\n",
    "    generator=torch.manual_seed(0)\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/utils/multiprocess_util.py\", line 91, in run\n",
      "    key, func, args, kwargs = self.task_q.get(timeout=TIMEOUT)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.11/multiprocessing/managers.py\", line 822, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.11/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "make_image_grid(images, rows=1, cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comparison between the LoRA and the non-LoRA results:\n",
    "\n",
    "![lora_non_lora](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lora_non_lora_comparison.png)\n",
    "\n",
    "You have a similar checkpoint stored on the Hugging Face Hub, you can load it\n",
    "directly with [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.load_lora_weights) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_id = \"sayakpaul/civitai-light-shadow-lora\"\n",
    "lora_filename = \"light_and_shadow.safetensors\"\n",
    "pipeline.load_lora_weights(lora_model_id, weight_name=lora_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
