============================= test session starts =============================
platform win32 -- Python 3.9.19, pytest-7.2.0, pluggy-1.5.0 -- D:\Anaconda\envs\MindSpore\python.exe
cachedir: .pytest_cache
rootdir: E:\Documents\code\mindnlp\tests, configfile: pytest.ini
collecting ... collected 89 items

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_matches_greedy_search_0_random hidden_state shape: (1, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (3, 16)
hidden_state shape: (3, 16)
hidden_state shape: (5, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (6, 16)
hidden_state shape: (2, 16)
hidden_state shape: (3, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (7, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (6, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (2, 16)
hidden_state shape: (4, 16)
hidden_state shape: (6, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_matches_greedy_search_1_same hidden_state shape: (1, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (6, 16)
hidden_state shape: (2, 16)
hidden_state shape: (3, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (3, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (6, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (3, 16)
hidden_state shape: (3, 16)
hidden_state shape: (6, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_sample hidden_state shape: (1, 7, 16)
hidden_state shape: (1, 16)
hidden_state shape: (6, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_attention_outputs hidden_state shape: (13, 7, 16)
hidden_state shape: (66, 16)
hidden_state shape: (69, 16)
hidden_state shape: (21, 16)
hidden_state shape: (26, 16)
hidden_state shape: (39, 16)
hidden_state shape: (25, 16)
hidden_state shape: (60, 16)
hidden_state shape: (58, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_batching_equivalence hidden_state shape: (13, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_sample_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (8, 16)
hidden_state shape: (3, 16)
hidden_state shape: (14, 16)
hidden_state shape: (4, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_sample_generate_dict_output hidden_state shape: (2, 7, 16)
hidden_state shape: (10, 16)
hidden_state shape: (1, 16)
hidden_state shape: (12, 16)
hidden_state shape: (5, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (10, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate_dict_output hidden_state shape: (2, 7, 16)
hidden_state shape: (13, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate_dict_outputs_use_cache hidden_state shape: (2, 7, 16)
hidden_state shape: (6, 16)
hidden_state shape: (1, 16)
hidden_state shape: (9, 16)
hidden_state shape: (12, 16)
hidden_state shape: (4, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_low_memory hidden_state shape: (2, 7, 16)
hidden_state shape: (10, 16)
hidden_state shape: (12, 16)
hidden_state shape: (3, 16)
hidden_state shape: (3, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_can_use_safetensors PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_config PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_constrained_beam_search_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (0,)
hidden_state shape: (2, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (7, 16)
hidden_state shape: (8, 16)
hidden_state shape: (10, 16)
hidden_state shape: (0,)
hidden_state shape: (2, 7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (12, 16)
hidden_state shape: (14, 16)
hidden_state shape: (0,)
hidden_state shape: (2, 7, 16)
hidden_state shape: (0,)
hidden_state shape: (2, 7, 16)
hidden_state shape: (14, 16)
hidden_state shape: (7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_constrained_beam_search_generate_dict_output hidden_state shape: (2, 7, 16)
hidden_state shape: (4, 16)
hidden_state shape: (1, 16)
hidden_state shape: (14, 16)
hidden_state shape: (9, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (5, 16)
hidden_state shape: (12, 16)
hidden_state shape: (11, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate_dict_outputs_use_cache hidden_state shape: (2, 7, 16)
hidden_state shape: (6, 16)
hidden_state shape: (9, 16)
hidden_state shape: (13, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate_low_memory hidden_state shape: (1, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_correct_missing_keys PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_custom_4d_attention_mask SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_decoder_model_past_with_large_inputs hidden_state shape: (13, 7, 16)
hidden_state shape: (13, 7, 16)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_determinism hidden_state shape: (13, 7, 16)
hidden_state shape: (65, 16)
hidden_state shape: (26, 16)
hidden_state shape: (91, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_dola_decoding_sample SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_encoder_decoder_model_standalone hidden_state shape: (13, 7, 16)
hidden_state shape: (54, 16)
hidden_state shape: (49, 16)
hidden_state shape: (52, 16)
hidden_state shape: (27, 16)
hidden_state shape: (3, 16)
hidden_state shape: (91, 16)
hidden_state shape: (7, 16)
hidden_state shape: (81, 16)
hidden_state shape: (52, 16)
hidden_state shape: (43, 16)
hidden_state shape: (53, 16)
hidden_state shape: (34, 16)
hidden_state shape: (13, 7, 16)
hidden_state shape: (54, 16)
hidden_state shape: (49, 16)
hidden_state shape: (52, 16)
hidden_state shape: (27, 16)
hidden_state shape: (3, 16)
hidden_state shape: (91, 16)
hidden_state shape: (7, 16)
hidden_state shape: (81, 16)
hidden_state shape: (52, 16)
hidden_state shape: (43, 16)
hidden_state shape: (53, 16)
hidden_state shape: (34, 16)
PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_feed_forward_chunking hidden_state shape: (13, 7, 16)
hidden_state shape: (58, 16)
hidden_state shape: (13, 16)
hidden_state shape: (72, 16)
hidden_state shape: (39, 16)
hidden_state shape: (37, 16)
hidden_state shape: (43, 16)
hidden_state shape: (15, 16)
hidden_state shape: (87, 16)
hidden_state shape: (39, 16)
hidden_state shape: (71, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_from_pretrained_no_checkpoint PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_continue_from_past_key_values hidden_state shape: (13, 7, 16)
hidden_state shape: (91, 16)
hidden_state shape: (69, 16)
hidden_state shape: (1, 16)
hidden_state shape: (21, 16)
hidden_state shape: (18, 16)
hidden_state shape: (53, 16)
hidden_state shape: (30, 16)
hidden_state shape: (81, 16)
hidden_state shape: (52, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_fp16 hidden_state shape: (13, 7, 16)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_from_inputs_embeds_decoder_only PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_with_head_masking hidden_state shape: (2, 7, 16)
hidden_state shape: (12, 16)
hidden_state shape: (3, 16)
hidden_state shape: (13, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_without_input_ids hidden_state shape: (1, 1, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_get_loss hidden_state shape: (13, 7, 16)
hidden_state shape: (45, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_gradient_checkpointing_backward_compatibility PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_gradient_checkpointing_enable_disable PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (14, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate_dict_outputs hidden_state shape: (2, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate_dict_outputs_use_cache hidden_state shape: (2, 7, 16)
hidden_state shape: (5, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_group_beam_search_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (2, 16)
hidden_state shape: (14, 16)
hidden_state shape: (12, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_group_beam_search_generate_dict_output hidden_state shape: (2, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_head_pruning SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_head_pruning_integration SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_head_pruning_save_load_from_config_init SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_head_pruning_save_load_from_pretrained SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_headmasking hidden_state shape: (13, 7, 16)
hidden_state shape: (91, 16)
hidden_state shape: (65, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_hidden_states_output hidden_state shape: (13, 7, 16)
hidden_state shape: (49, 16)
hidden_state shape: (42, 16)
hidden_state shape: (2, 16)
hidden_state shape: (89, 16)
hidden_state shape: (76, 16)
hidden_state shape: (33, 16)
hidden_state shape: (40, 16)
hidden_state shape: (33, 16)
hidden_state shape: (79, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_initialization PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_inputs_embeds hidden_state shape: (13, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_inputs_embeds_matches_input_ids hidden_state shape: (13, 7, 16)
hidden_state shape: (10, 16)
hidden_state shape: (26, 16)
hidden_state shape: (91, 16)
hidden_state shape: (55, 16)
hidden_state shape: (10, 16)
hidden_state shape: (19, 16)
hidden_state shape: (62, 16)
hidden_state shape: (91, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_keep_in_fp32_modules SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_left_padding_compatibility SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_load_save_without_tied_weights SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_load_with_mismatched_shapes PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_mindspore_save_load SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_mismatched_shapes_have_properly_initialized_weights PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_get_set_embeddings PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_is_small PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_main_input_name PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_outputs_equivalence hidden_state shape: (13, 7, 16)
hidden_state shape: (60, 16)
hidden_state shape: (69, 16)
hidden_state shape: (29, 16)
hidden_state shape: (24, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_weights_reload_no_missing_tied_weights PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_new_cache_format_0 SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_new_cache_format_1 SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_new_cache_format_2 SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_past_key_values_format hidden_state shape: (13, 7, 16)
hidden_state shape: (4, 16)
hidden_state shape: (69, 16)
hidden_state shape: (21, 16)
hidden_state shape: (88, 16)
hidden_state shape: (72, 16)
hidden_state shape: (28, 16)
hidden_state shape: (77, 16)
hidden_state shape: (5, 16)
hidden_state shape: (6, 16)
hidden_state shape: (45, 16)
hidden_state shape: (61, 16)
hidden_state shape: (70, 16)
PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_problem_types PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_prompt_lookup_decoding_matches_greedy_search hidden_state shape: (1, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (3, 16)
hidden_state shape: (7, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (7, 16)
hidden_state shape: (1, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (7, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (0,)
hidden_state shape: (1, 7, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_prompt_lookup_decoding_stops_at_eos PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_resize_embeddings_untied hidden_state shape: (13, 7, 16)
hidden_state shape: (22, 16)
hidden_state shape: (69, 16)
hidden_state shape: (64, 16)
hidden_state shape: (27, 16)
hidden_state shape: (5, 16)
hidden_state shape: (44, 16)
hidden_state shape: (59, 16)
hidden_state shape: (74, 16)
hidden_state shape: (7, 16)
hidden_state shape: (57, 16)
hidden_state shape: (46, 16)
hidden_state shape: (72, 16)
hidden_state shape: (13, 7, 16)
hidden_state shape: (18, 16)
hidden_state shape: (70, 16)
hidden_state shape: (66, 16)
hidden_state shape: (28, 16)
hidden_state shape: (14, 16)
hidden_state shape: (49, 16)
hidden_state shape: (52, 16)
hidden_state shape: (67, 16)
hidden_state shape: (6, 16)
hidden_state shape: (50, 16)
hidden_state shape: (54, 16)
hidden_state shape: (72, 16)
PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_resize_position_vector_embeddings SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_resize_tokens_embeddings hidden_state shape: (13, 7, 16)
hidden_state shape: (3, 16)
hidden_state shape: (22, 16)
hidden_state shape: (88, 16)
hidden_state shape: (69, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_sample_generate hidden_state shape: (2, 7, 16)
hidden_state shape: (5, 16)
hidden_state shape: (8, 16)
hidden_state shape: (1, 16)
hidden_state shape: (14, 16)
hidden_state shape: (2, 16)
hidden_state shape: (2, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_sample_generate_dict_output hidden_state shape: (2, 7, 16)
hidden_state shape: (9, 16)
hidden_state shape: (2, 16)
hidden_state shape: (11, 16)
hidden_state shape: (6, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load hidden_state shape: (13, 7, 16)
hidden_state shape: (65, 16)
hidden_state shape: (8, 16)
hidden_state shape: (70, 16)
hidden_state shape: (39, 16)
hidden_state shape: (36, 16)
hidden_state shape: (45, 16)
hidden_state shape: (14, 16)
hidden_state shape: (87, 16)
hidden_state shape: (34, 16)
hidden_state shape: (74, 16)
hidden_state shape: (0,)
FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load_fast_init_from_base SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load_fast_init_to_base SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load_keys_to_ignore_on_save PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load_strict PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_tie_model_weights PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_tied_weights_keys PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_training hidden_state shape: (13, 7, 16)
hidden_state shape: (80, 16)
hidden_state shape: (9, 16)
hidden_state shape: (88, 16)
hidden_state shape: (5, 16)
hidden_state shape: (44, 16)
hidden_state shape: (3, 16)
hidden_state shape: (86, 16)
hidden_state shape: (49, 16)
hidden_state shape: (77, 16)
hidden_state shape: (6, 16)
hidden_state shape: (22, 16)
hidden_state shape: (77, 16)
PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_training_gradient_checkpointing SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_training_gradient_checkpointing_use_reentrant SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_training_gradient_checkpointing_use_reentrant_false SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelIntegrationTests::test_inference_logits SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelIntegrationTests::test_large_logits SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelIntegrationTests::test_seq_to_seq_generation SKIPPED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeRouterTest::test_batch_prioritized_routing FAILED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeRouterTest::test_second_expert_policy PASSED
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeRouterTest::test_top_2_routing token_indices.shape: (40,)
masked_hidden_states[idx, token_indices].shape: (1, 32)
expert_output.shape: (1, 32)
FAILED

================================== FAILURES ===================================
___ NllbMoeModelTest.test_assisted_decoding_matches_greedy_search_0_random ____

a = (<tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_assisted_decoding_matches_greedy_search_0_random>,)
kw = {}

    @wraps(func)
    def standalone_func(*a, **kw):
>       return func(*(a + p.args), **p.kwargs, **kw)

D:\Anaconda\envs\MindSpore\lib\site-packages\parameterized\parameterized.py:620: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\utils\testing_utils.py:1770: in wrapper
    return test_func_ref(*args, **kwargs)
tests\ut\transformers\generation\test_utils.py:1156: in test_assisted_decoding_matches_greedy_search
    output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1964: in generate
    result = self._sample(
mindnlp\transformers\generation\utils.py:2917: in _sample
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
____ NllbMoeModelTest.test_assisted_decoding_matches_greedy_search_1_same _____

a = (<tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_assisted_decoding_matches_greedy_search_1_same>,)
kw = {}

    @wraps(func)
    def standalone_func(*a, **kw):
>       return func(*(a + p.args), **p.kwargs, **kw)

D:\Anaconda\envs\MindSpore\lib\site-packages\parameterized\parameterized.py:620: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\utils\testing_utils.py:1770: in wrapper
    return test_func_ref(*args, **kwargs)
tests\ut\transformers\generation\test_utils.py:1156: in test_assisted_decoding_matches_greedy_search
    output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1964: in generate
    result = self._sample(
mindnlp\transformers\generation\utils.py:2917: in _sample
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_______________ NllbMoeModelTest.test_assisted_decoding_sample ________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_assisted_decoding_sample>

    def test_assisted_decoding_sample(self):
        # In this test we don't check assisted vs non-assisted output -- seeded assisted decoding with sample will not
        # match sample for the same seed, as the forward pass does not return the exact same logits (due to matmul with
        # different shapes, see https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="Stateful models don't support assisted generation")
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
            if any(
                model_name in model_class.__name__.lower()
                for model_name in [
                    "bigbirdpegasus",
                    "led",
                    "mega",
                    "speech2text",
                    "git",
                    "prophetnet",
                    "seamlessm4t",
                    "clvp",
                ]
            ):
                self.skipTest(reason="May fix in the future: need model-specific fixes")
    
            # enable cache
            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)
    
            # NOTE: assisted generation only works with cache on at the moment.
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
    
            config.use_cache = True
            config.is_decoder = True
            model = model_class(config).eval()
            # Sets assisted generation arguments such that:
            # a) no EOS is generated, to ensure generation doesn't break early
            # b) the assistant model always generates two tokens when it is called, to ensure the input preparation of
            #    the assistant model is correct
            # c) there are at least two forward passes in the main model, to ensure the input preparation of
            #    the main model is correct
            assistant_model = model
            assistant_model.generation_config.num_assistant_tokens = 2  # see b)
            assistant_model.generation_config.num_assistant_tokens_schedule = "constant"  # see b)
            generation_kwargs = {
                "eos_token_id": -1,  # see a)
                "max_new_tokens": 4,  # see c)
                "num_beams": 1,
                "do_sample": True,
                "assistant_model": assistant_model,
                "output_scores": True,
                "output_logits": True,
                "output_hidden_states": True,
                "output_attentions": self.has_attentions,
                "return_dict_in_generate": True,
            }
>           output_assisted = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)

tests\ut\transformers\generation\test_utils.py:1342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
___________________ NllbMoeModelTest.test_attention_outputs ___________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_attention_outputs>

    def test_attention_outputs(self):
        if not self.has_attentions:
            self.skipTest(reason="Model does not output attentions")
    
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
        config.return_dict = True
    
        seq_len = getattr(self.model_tester, "seq_length", None)
        decoder_seq_length = getattr(self.model_tester, "decoder_seq_length", seq_len)
        encoder_seq_length = getattr(self.model_tester, "encoder_seq_length", seq_len)
        decoder_key_length = getattr(self.model_tester, "decoder_key_length", decoder_seq_length)
        encoder_key_length = getattr(self.model_tester, "key_length", encoder_seq_length)
        chunk_length = getattr(self.model_tester, "chunk_length", None)
        if chunk_length is not None and hasattr(self.model_tester, "num_hashes"):
            encoder_seq_length = encoder_seq_length * self.model_tester.num_hashes
    
        for model_class in self.all_model_classes:
            inputs_dict["output_attentions"] = True
            inputs_dict["output_hidden_states"] = False
            config.return_dict = True
            model = model_class(config)
            model.eval()
            with no_grad():
>               outputs = model(**self._prepare_for_class(inputs_dict, model_class))

tests\ut\transformers\test_modeling_common.py:843: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_batching_equivalence __________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_batching_equivalence>

    def test_batching_equivalence(self):
        """
        Tests that the model supports batching and that the output is the nearly the same for the same input in
        different batch sizes.
        (Why "nearly the same" not "exactly the same"? Batching uses different matmul shapes, which often leads to
        different results: https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535)
        """
    
        def get_tensor_equivalence_function(batched_input):
            # models operating on continuous spaces have higher abs difference than LMs
            # instead, we can rely on cos distance for image/speech models, similar to `diffusers`
            if "input_ids" not in batched_input:
                return lambda tensor1, tensor2: (
                    1.0 - F.cosine_similarity(tensor1.float().flatten(), tensor2.float().flatten(), dim=0, eps=1e-38)
                )
            return lambda tensor1, tensor2: ops.max(ops.abs(tensor1 - tensor2))
    
        def recursive_check(batched_object, single_row_object, model_name, key):
            if isinstance(batched_object, (list, tuple)):
                for batched_object_value, single_row_object_value in zip(batched_object, single_row_object):
                    recursive_check(batched_object_value, single_row_object_value, model_name, key)
            elif isinstance(batched_object, dict):
                for batched_object_value, single_row_object_value in zip(
                    batched_object.values(), single_row_object.values()
                ):
                    recursive_check(batched_object_value, single_row_object_value, model_name, key)
            # do not compare returned loss (0-dim tensor) / codebook ids (int) / caching objects
            elif batched_object is None or not isinstance(batched_object, mindspore.Tensor):
                return
            elif batched_object.ndim == 0:
                return
            else:
                if isinstance(batched_object.dtype, mindspore.dtype.Int):
                    return
                # indexing the first element does not always work
                # e.g. models that output similarity scores of size (N, M) would need to index [0, 0]
                slice_ids = [slice(0, index) for index in single_row_object.shape]
                batched_row = batched_object[slice_ids]
                self.assertFalse(
                    ops.isnan(batched_row).any(), f"Batched output has `nan` in {model_name} for key={key}"
                )
                self.assertFalse(
                    ops.isinf(batched_row).any(), f"Batched output has `inf` in {model_name} for key={key}"
                )
                self.assertFalse(
                    ops.isnan(single_row_object).any(), f"Single row output has `nan` in {model_name} for key={key}"
                )
                self.assertFalse(
                    ops.isinf(single_row_object).any(), f"Single row output has `inf` in {model_name} for key={key}"
                )
                self.assertTrue(
                    (equivalence(batched_row, single_row_object)) <= 1e-03,
                    msg=(
                        f"Batched and Single row outputs are not equal in {model_name} for key={key}. "
                        f"Difference={equivalence(batched_row, single_row_object)}."
                    ),
                )
    
        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()
        equivalence = get_tensor_equivalence_function(batched_input)
    
        for model_class in self.all_model_classes:
            config.output_hidden_states = True
    
            model_name = model_class.__name__
            if hasattr(self.model_tester, "prepare_config_and_inputs_for_model_class"):
                config, batched_input = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)
    
            batched_input_prepared = self._prepare_for_class(batched_input, model_class)
            model = model_class(config).eval()
    
            batch_size = self.model_tester.batch_size
            single_row_input = {}
            for key, value in batched_input_prepared.items():
                if isinstance(value, mindspore.Tensor) and value.shape[0] % batch_size == 0:
                    # e.g. musicgen has inputs of size (bs*codebooks). in most cases value.shape[0] == batch_size
                    single_batch_shape = value.shape[0] // batch_size
                    single_row_input[key] = value[:single_batch_shape]
                else:
                    single_row_input[key] = value
    
            with no_grad():
>               model_batched_output = model(**batched_input_prepared)

tests\ut\transformers\test_modeling_common.py:729: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1597: in forward
    encoder_outputs = self.encoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_beam_sample_generate __________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_sample_generate>

    def test_beam_sample_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            _, logits_warper_kwargs = self._get_logits_processor_and_warper_kwargs(input_ids.shape[-1])
    
            model = model_class(config).eval()
            beam_kwargs = self._get_beam_kwargs()
    
>           output_generate = self._beam_sample_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_warper_kwargs=logits_warper_kwargs,
            )

tests\ut\transformers\generation\test_utils.py:672: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:300: in _beam_sample_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:2002: in generate
    result = self._beam_search(
mindnlp\transformers\generation\utils.py:3170: in _beam_search
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
___________ NllbMoeModelTest.test_beam_sample_generate_dict_output ____________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_sample_generate_dict_output>

    def test_beam_sample_generate_dict_output(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            # disable cache
            config.use_cache = False
    
            model = model_class(config).eval()
            _, logits_warper_kwargs = self._get_logits_processor_and_warper_kwargs(input_ids.shape[-1])
            beam_kwargs = self._get_beam_kwargs()
    
>           output_generate = self._beam_sample_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_warper_kwargs=logits_warper_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:709: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:300: in _beam_sample_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:2002: in generate
    result = self._beam_search(
mindnlp\transformers\generation\utils.py:3170: in _beam_search
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_beam_search_generate __________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_search_generate>

    def test_beam_search_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            model = model_class(config).eval()
    
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
            beam_kwargs = self._get_beam_kwargs()
    
>           output_generate = self._beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
            )

tests\ut\transformers\generation\test_utils.py:566: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:269: in _beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
___________ NllbMoeModelTest.test_beam_search_generate_dict_output ____________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_search_generate_dict_output>

    def test_beam_search_generate_dict_output(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            # disable cache
            config.use_cache = False
    
            model = model_class(config).eval()
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
            beam_kwargs = self._get_beam_kwargs()
>           output_generate = self._beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:269: in _beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______ NllbMoeModelTest.test_beam_search_generate_dict_outputs_use_cache ______

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_search_generate_dict_outputs_use_cache>

    def test_beam_search_generate_dict_outputs_use_cache(self):
        for model_class in self.all_generative_model_classes:
            # enable cache
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
            if any(model_name in model_class.__name__.lower() for model_name in ["rwkv"]):
                self.skipTest(reason="Won't fix: model with non-standard dictionary output shapes")
    
            model = model_class(config).eval()
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
    
            beam_kwargs = self._get_beam_kwargs()
    
            config.use_cache = True
            config.is_decoder = True
            model = model_class(config).eval()
>           output_generate = self._beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:642: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:269: in _beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:2002: in generate
    result = self._beam_search(
mindnlp\transformers\generation\utils.py:3170: in _beam_search
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
________________ NllbMoeModelTest.test_beam_search_low_memory _________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_beam_search_low_memory>

    def test_beam_search_low_memory(self):
        # Check that choosing 'low_memory' does not change the model output
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="May fix in the future: need custom cache handling")
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
            if any(
                model_name in model_class.__name__.lower()
                for model_name in [
                    "ctrl",
                    "gptbigcode",
                    "transo_xl",
                    "xlnet",
                    "cpm",
                    "jamba",
                ]
            ):
                self.skipTest(reason="May fix in the future: need model-specific fixes")
            config, input_ids, _ = self._get_input_ids_and_config(batch_size=2)
            # batch_size=1 is ok, but batch_size>1 will cause non-identical output
    
            config.use_cache = True
            config.is_decoder = True
    
            # test output equality of low versus high memory
            model = model_class(config).eval()
    
>           low_output = model.generate(input_ids, max_new_tokens=8, num_beams=5, early_stopping=True, low_memory=True)

tests\ut\transformers\generation\test_utils.py:1088: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:2002: in generate
    result = self._beam_search(
mindnlp\transformers\generation\utils.py:3163: in _beam_search
    outputs_per_sub_batch = [
mindnlp\transformers\generation\utils.py:3164: in <listcomp>
    self(**inputs_per_sub_batch, return_dict=True) for inputs_per_sub_batch in inputs_per_sub_batches
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
___________ NllbMoeModelTest.test_constrained_beam_search_generate ____________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_constrained_beam_search_generate>

    @is_flaky()
    def test_constrained_beam_search_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            model = model_class(config).eval()
    
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
    
            # Sample constraints
            min_id = 3
            max_id = config.vocab_size
    
            force_tokens = ops.randint(min_id, max_id, (1, 2)).tolist()[0]
            constraints = [
                PhrasalConstraint(force_tokens),
            ]
    
            beam_kwargs = self._get_constrained_beam_kwargs()
>           output_generate = self._constrained_beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                constraints=constraints,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
            )

tests\ut\transformers\generation\test_utils.py:861: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:361: in _constrained_beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_____ NllbMoeModelTest.test_constrained_beam_search_generate_dict_output ______

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_constrained_beam_search_generate_dict_output>

    def test_constrained_beam_search_generate_dict_output(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            # disable cache
            config.use_cache = False
    
            model = model_class(config).eval()
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
    
            # Sample constraints
            min_id = 3
            max_id = model.config.vocab_size
            force_tokens = ops.randint(min_id, max_id, (1, 2)).tolist()[0]
            constraints = [
                PhrasalConstraint(force_tokens),
            ]
    
            beam_kwargs = self._get_constrained_beam_kwargs()
>           output_generate = self._constrained_beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                constraints=constraints,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:927: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:361: in _constrained_beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:2101: in generate
    result = self._constrained_beam_search(
mindnlp\transformers\generation\utils.py:3727: in _constrained_beam_search
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_contrastive_generate __________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_contrastive_generate>

    def test_contrastive_generate(self):
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="Stateful models don't support contrastive search generation")
    
            # won't fix: FSMT and Reformer have a different cache variable type (and format).
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
    
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            # NOTE: contrastive search only works with cache on at the moment.
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
            config.use_cache = True
            config.is_decoder = True
    
            # test old generation output for backwards compatibility
            model = model_class(config).eval()
>           output_generate = self._contrastive_generate(
                model=model, input_ids=input_ids, attention_mask=attention_mask
            )

tests\ut\transformers\generation\test_utils.py:975: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:401: in _contrastive_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______ NllbMoeModelTest.test_contrastive_generate_dict_outputs_use_cache ______

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_contrastive_generate_dict_outputs_use_cache>

    def test_contrastive_generate_dict_outputs_use_cache(self):
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="Stateful models don't support contrastive search generation")
    
            # won't fix: FSMT and Reformer have a different cache variable type (and format).
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
    
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            # NOTE: contrastive search only works with cache on at the moment.
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
            config.use_cache = True
            config.is_decoder = True
    
            model = model_class(config).eval()
>           output_generate = self._contrastive_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:1001: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:401: in _contrastive_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
____________ NllbMoeModelTest.test_contrastive_generate_low_memory ____________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_contrastive_generate_low_memory>

    def test_contrastive_generate_low_memory(self):
        # Check that choosing 'low_memory' does not change the model output
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="Stateful models don't support contrastive search generation")
    
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer", "speech2text"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
            if any(model_name in model_class.__name__.lower() for model_name in ["gptbigcode"]):
                self.skipTest(reason="TODO: fix me")
    
            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)
    
            # NOTE: contrastive search only works with cache on at the moment.
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
    
            config.use_cache = True
            config.is_decoder = True
    
            # test output equality of low versus high memory
            model = model_class(config).eval()
    
>           low_output = model.generate(
                input_ids,
                top_k=4,
                penalty_alpha=0.6,
                low_memory=True,
                max_new_tokens=self.max_new_tokens,
                attention_mask=attention_mask,
            )

tests\ut\transformers\generation\test_utils.py:1041: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________ NllbMoeModelTest.test_decoder_model_past_with_large_inputs __________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_decoder_model_past_with_large_inputs>

    def test_decoder_model_past_with_large_inputs(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs()
        config.decoder_sparse_step = 0
>       self.model_tester.create_and_check_decoder_model_past_large_inputs(
            config, inputs_dict
        )

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:321: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:220: in create_and_check_decoder_model_past_large_inputs
    next_attention_mask = ops.cat([attention_mask, next_attn_mask], dim=-1)
mindnlp\core\ops\array.py:20: in cat
    return ops.cat(tensors, dim)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\function\array_func.py:300: in cat
    return _concat(tensors)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Concat]<axis=-1>, 'Concat', ([Tensor(shape=[13, 7], dtype=Bool, value=
[[ True,  True,  True ...  True,  True,  ...or(shape=[13, 3], dtype=Int64, value=
[[0, 1, 1],
 [1, 0, 0],
 [0, 0, 1],
 ...
 [1, 1, 1],
 [1, 0, 1],
 [1, 1, 0]])],))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       TypeError: For primitive[Concat], the input type must be same.
E       name:[element0]:Tensor[Bool].
E       name:[element1]:Tensor[Int64].
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\utils\check_convert_utils.cc:1029 mindspore::CheckAndConvertUtils::_CheckTypeSame

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: TypeError
______________________ NllbMoeModelTest.test_determinism ______________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_determinism>

    def test_determinism(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        def check_determinism(first, second):
            out_1 = first.asnumpy()
            out_2 = second.asnumpy()
            out_1 = out_1[~np.isnan(out_1)]
            out_2 = out_2[~np.isnan(out_2)]
            out_1 = out_1[~np.isneginf(out_1)]
            out_2 = out_2[~np.isneginf(out_2)]
            max_diff = np.amax(np.abs(out_1 - out_2))
            self.assertLessEqual(max_diff, 1e-5)
    
        for model_class in self.all_model_classes:
            model = model_class(config)
            model.eval()
            with no_grad():
>               first = model(**self._prepare_for_class(inputs_dict, model_class))[0]

tests\ut\transformers\test_modeling_common.py:638: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1597: in forward
    encoder_outputs = self.encoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_feed_forward_chunking _________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_feed_forward_chunking>

    def test_feed_forward_chunking(self):
        (
            original_config,
            inputs_dict,
        ) = self.model_tester.prepare_config_and_inputs_for_common()
        for model_class in self.all_model_classes:
            set_seed(0)
            config = copy.deepcopy(original_config)
            model = model_class(config)
            model.eval()
    
>           hidden_states_no_chunk = model(**self._prepare_for_class(inputs_dict, model_class))[0]

tests\ut\transformers\test_modeling_common.py:1280: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
________ NllbMoeModelTest.test_generate_continue_from_past_key_values _________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_generate_continue_from_past_key_values>

    def test_generate_continue_from_past_key_values(self):
        # Tests that we can continue generating from past key values, returned from a previous `generate` call
        for model_class in self.all_generative_model_classes:
            if any(model_name in model_class.__name__.lower() for model_name in ["imagegpt"]):
                self.skipTest(reason="Won't fix: old model with unique inputs/caches/other")
            if any(model_name in model_class.__name__.lower() for model_name in ["umt5"]):
                self.skipTest(reason="TODO: needs modeling or test input preparation fixes for compatibility")
    
            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()
    
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
    
            # Let's make it always:
            # 1. use cache (for obvious reasons)
            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which
            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the
            #    continuation would force it to generate beyond an EOS token)
            # 3. ignore `token_type_ids` for simplicity
            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is
            #    active by default on some models
            config.use_cache = True
            if "token_type_ids" in inputs:
                del inputs["token_type_ids"]
    
            model = model_class(config)
            model.eval()
            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1
            model.generation_config.forced_eos_token_id = None
            # If "past_key_values" is not returned, skip the test (e.g. RWKV uses a different cache name and format)
>           outputs = model(**inputs)

tests\ut\transformers\generation\test_utils.py:1633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_____________________ NllbMoeModelTest.test_generate_fp16 _____________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_generate_fp16>

    def test_generate_fp16(self):
        config, input_dict = self.model_tester.prepare_config_and_inputs()
        input_ids = input_dict["input_ids"]
        attention_mask = input_ids.ne(1)
        model = NllbMoeForConditionalGeneration(config).eval()
        model.half()
>       model.generate(input_ids, attention_mask=attention_mask)

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:364: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:538: in forward
    top_1_mask, router_probs = self.router(hidden_states, padding_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:450: in forward
    router_logits = self.classifier(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[91, 16], dtype=Float32, value=
[[ 1.08691406e+00, -8.55712891e-02....59863281e-01,  1.10058594e+00]]), Parameter (name=Parameter, shape=(4, 16), dtype=Float16, requires_grad=True), None))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       TypeError: For primitive[Dense], the input type must be same.
E       name:[w]:Ref[Tensor[Float16]].
E       name:[x]:Tensor[Float32].
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\utils\check_convert_utils.cc:1029 mindspore::CheckAndConvertUtils::_CheckTypeSame

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: TypeError
______________ NllbMoeModelTest.test_generate_with_head_masking _______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_generate_with_head_masking>

    def test_generate_with_head_masking(self):
        """Test designed for encoder-decoder models to ensure the attention head masking is used."""
        attention_names = ["encoder_attentions", "decoder_attentions", "cross_attentions"]
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
            # We want to test only encoder-decoder models
            if not config.is_encoder_decoder:
                continue
            model = model_class(config)
    
            head_masking = {
                "head_mask": ops.zeros(config.encoder_layers, config.encoder_attention_heads),
                "decoder_head_mask": ops.zeros(
                    config.decoder_layers, config.decoder_attention_heads
                ),
                "cross_attn_head_mask": ops.zeros(
                    config.decoder_layers, config.decoder_attention_heads
                ),
            }
    
            signature = inspect.signature(model.forward)
            # We want to test only models where encoder/decoder head masking is implemented
            if not set(head_masking.keys()) < {*signature.parameters.keys()}:
                continue
    
            for attn_name, (name, mask) in zip(attention_names, head_masking.items()):
>               out = model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    num_beams=1,
                    output_attentions=self.has_attentions,
                    return_dict_in_generate=True,
                    remove_invalid_values=True,
                    **{name: mask},
                )

tests\ut\transformers\generation\test_utils.py:1400: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______________ NllbMoeModelTest.test_generate_without_input_ids _______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_generate_without_input_ids>

    def test_generate_without_input_ids(self):
        config, _, _ = self._get_input_ids_and_config()
    
        # if no bos token id => cannot generate from None
        if config.bos_token_id is None:
            self.skipTest(reason="bos_token_id is None")
    
        # hack in case they are equal, otherwise the attn mask will be [0]
        if config.bos_token_id == config.pad_token_id:
            config.pad_token_id = None
    
        for model_class in self.all_generative_model_classes:
            model = model_class(config)
            model.eval()
    
>           output_ids_generate = model.generate(
                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True
            )

tests\ut\transformers\generation\test_utils.py:752: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_______________________ NllbMoeModelTest.test_get_loss ________________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_get_loss>

    def test_get_loss(self):
        config, input_dict = self.model_tester.prepare_config_and_inputs()
        input_dict["output_router_logits"] = True
        input_dict["labels"] = input_dict["input_ids"]
        model = NllbMoeForConditionalGeneration(config).eval()
>       out = model(**input_dict)

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:374: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1597: in forward
    encoder_outputs = self.encoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
____________________ NllbMoeModelTest.test_greedy_generate ____________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_greedy_generate>

    def test_greedy_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            model = model_class(config).eval()
>           output_generate = self._greedy_generate(model=model, input_ids=input_ids, attention_mask=attention_mask)

tests\ut\transformers\generation\test_utils.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:205: in _greedy_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_____________ NllbMoeModelTest.test_greedy_generate_dict_outputs ______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_greedy_generate_dict_outputs>

    def test_greedy_generate_dict_outputs(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            config.use_cache = False
            model = model_class(config).eval()
>           output_generate = self._greedy_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:205: in _greedy_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
________ NllbMoeModelTest.test_greedy_generate_dict_outputs_use_cache _________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_greedy_generate_dict_outputs_use_cache>

    def test_greedy_generate_dict_outputs_use_cache(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
            if any(model_name in model_class.__name__.lower() for model_name in ["rwkv"]):
                self.skipTest(reason="Won't fix: model with non-standard dictionary output shapes")
    
            config.use_cache = True
            config.is_decoder = True
            model = model_class(config).eval()
>           output_generate = self._greedy_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:205: in _greedy_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______________ NllbMoeModelTest.test_group_beam_search_generate _______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_group_beam_search_generate>

    def test_group_beam_search_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            model = model_class(config).eval()
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
    
            # check `generate()` and `group_beam_search()` are equal
            beam_kwargs = self._get_diverse_beam_kwargs()
>           output_generate = self._group_beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
            )

tests\ut\transformers\generation\test_utils.py:770: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:330: in _group_beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
________ NllbMoeModelTest.test_group_beam_search_generate_dict_output _________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_group_beam_search_generate_dict_output>

    def test_group_beam_search_generate_dict_output(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
            config.use_cache = False
    
            model = model_class(config).eval()
            logits_process_kwargs, _ = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                config.forced_bos_token_id,
                config.forced_eos_token_id,
            )
    
            beam_kwargs = self._get_diverse_beam_kwargs()
>           output_generate = self._group_beam_search_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                beam_kwargs=beam_kwargs,
                logits_process_kwargs=logits_process_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:810: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:330: in _group_beam_search_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______________________ NllbMoeModelTest.test_headmasking ______________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_headmasking>

    def test_headmasking(self):
        if not self.test_head_masking:
            self.skipTest(reason="Model does not support head masking")
    
        global_rng.seed(42)
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
        global_rng.seed()
    
        inputs_dict["output_attentions"] = True
        config.output_hidden_states = True
        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan
        for model_class in self.all_model_classes:
            model = model_class(config=configs_no_init)
            model.eval()
    
            # Prepare head_mask
            # Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior)
            head_mask = ops.ones(
                self.model_tester.num_hidden_layers,
                self.model_tester.num_attention_heads,
            )
            head_mask[0, 0] = 0
            head_mask[-1, :-1] = 0
            head_mask.requires_grad =True
            inputs = self._prepare_for_class(inputs_dict, model_class).copy()
            inputs["head_mask"] = head_mask
            if model.config.is_encoder_decoder:
                signature = inspect.signature(model.forward)
                arg_names = [*signature.parameters.keys()]
                if "decoder_head_mask" in arg_names:  # necessary diferentiation because of T5 model
                    inputs["decoder_head_mask"] = head_mask
                if "cross_attn_head_mask" in arg_names:
                    inputs["cross_attn_head_mask"] = head_mask
>           outputs = model(**inputs, return_dict=True)

tests\ut\transformers\test_modeling_common.py:971: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1597: in forward
    encoder_outputs = self.encoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_________________ NllbMoeModelTest.test_hidden_states_output __________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_hidden_states_output>

    def test_hidden_states_output(self):
        def check_hidden_states_output(inputs_dict, config, model_class):
            model = model_class(config)
            model.eval()
    
            with no_grad():
                outputs = model(**self._prepare_for_class(inputs_dict, model_class))
    
            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states
    
            expected_num_layers = getattr(
                self.model_tester, "expected_num_hidden_layers", self.model_tester.num_hidden_layers + 1
            )
            self.assertEqual(len(hidden_states), expected_num_layers)
    
            if hasattr(self.model_tester, "encoder_seq_length"):
                seq_length = self.model_tester.encoder_seq_length
                if hasattr(self.model_tester, "chunk_length") and self.model_tester.chunk_length > 1:
                    seq_length = seq_length * self.model_tester.chunk_length
            else:
                seq_length = self.model_tester.seq_length
    
            self.assertListEqual(
                list(hidden_states[0].shape[-2:]),
                [seq_length, self.model_tester.hidden_size],
            )
    
            if config.is_encoder_decoder:
                hidden_states = outputs.decoder_hidden_states
    
                self.assertIsInstance(hidden_states, (list, tuple))
                self.assertEqual(len(hidden_states), expected_num_layers)
                seq_len = getattr(self.model_tester, "seq_length", None)
                decoder_seq_length = getattr(self.model_tester, "decoder_seq_length", seq_len)
    
                self.assertListEqual(
                    list(hidden_states[0].shape[-2:]),
                    [decoder_seq_length, self.model_tester.hidden_size],
                )
    
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        for model_class in self.all_model_classes:
            inputs_dict["output_hidden_states"] = True
>           check_hidden_states_output(inputs_dict, config, model_class)

tests\ut\transformers\test_modeling_common.py:1203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\test_modeling_common.py:1165: in check_hidden_states_output
    outputs = model(**self._prepare_for_class(inputs_dict, model_class))
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_____________________ NllbMoeModelTest.test_inputs_embeds _____________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_inputs_embeds>

    def test_inputs_embeds(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        for model_class in (NllbMoeModel, NllbMoeForConditionalGeneration):
            model = model_class(config)
            model.eval()
    
            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))
    
            if not self.is_encoder_decoder:
                input_ids = inputs["input_ids"]
                del inputs["input_ids"]
            else:
                encoder_input_ids = inputs["input_ids"]
                decoder_input_ids = inputs.get("decoder_input_ids", encoder_input_ids)
                del inputs["input_ids"]
                inputs.pop("decoder_input_ids", None)
    
            wte = model.get_input_embeddings()
            if not self.is_encoder_decoder:
                inputs["inputs_embeds"] = wte(input_ids)
            else:
                inputs["inputs_embeds"] = wte(encoder_input_ids)
                inputs["decoder_inputs_embeds"] = wte(decoder_input_ids)
    
            with no_grad():
>               model(**inputs)[0]

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:355: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1597: in forward
    encoder_outputs = self.encoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
____________ NllbMoeModelTest.test_inputs_embeds_matches_input_ids ____________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_inputs_embeds_matches_input_ids>

    def test_inputs_embeds_matches_input_ids(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        for model_class in self.all_model_classes:
            if model_class.__name__ not in get_values(MODEL_MAPPING_NAMES):
                continue
            model = model_class(config)
            model.eval()
    
            model_forward_args = inspect.signature(model.forward).parameters
            if "inputs_embeds" not in model_forward_args:
                self.skipTest(reason="This model doesn't use `inputs_embeds`")
    
            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))
            pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1
    
            wte = model.get_input_embeddings()
            if not self.is_encoder_decoder:
                input_ids = inputs["input_ids"]
                # some models infer position ids/attn mask differently when input ids
                # by check if pad_token let's make sure no padding is in input ids
                not_pad_token_id = pad_token_id + 1 if max(0, pad_token_id - 1) == 0 else pad_token_id - 1
                input_ids[input_ids == pad_token_id] = not_pad_token_id
                del inputs["input_ids"]
                inputs_embeds = wte(input_ids)
                with no_grad():
                    out_ids = model(input_ids=input_ids, **inputs)[0]
                    out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]
            else:
                encoder_input_ids = inputs["input_ids"]
                decoder_input_ids = inputs.get("decoder_input_ids", encoder_input_ids)
                encoder_input_ids[encoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)
                decoder_input_ids[decoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)
                del inputs["input_ids"]
                inputs.pop("decoder_input_ids", None)
                inputs_embeds = wte(encoder_input_ids)
                decoder_inputs_embeds = wte(decoder_input_ids)
                with no_grad():
>                   out_ids = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids, **inputs)[0]

tests\ut\transformers\test_modeling_common.py:1954: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_______________ NllbMoeModelTest.test_model_outputs_equivalence _______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_model_outputs_equivalence>

    def test_model_outputs_equivalence(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        def set_nan_tensor_to_zero(t):
            t[t != t] = 0
            return t
    
        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):
            with no_grad():
                tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)
                dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs).to_tuple()
    
                def recursive_check(tuple_object, dict_object):
                    if isinstance(tuple_object, (List, Tuple)):
                        for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):
                            recursive_check(tuple_iterable_value, dict_iterable_value)
                    elif isinstance(tuple_object, Dict):
                        for tuple_iterable_value, dict_iterable_value in zip(
                            tuple_object.values(), dict_object.values()
                        ):
                            recursive_check(tuple_iterable_value, dict_iterable_value)
                    elif tuple_object is None:
                        return
                    else:
                        self.assertTrue(
                            ops.allclose(
                                set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-4
                            ),
                            msg=(
                                "Tuple and dict output are not equal. Difference:"
                                f" {ops.max(ops.abs(tuple_object - dict_object))}. Tuple has `nan`:"
                                f" {ops.isnan(tuple_object).any()} and `inf`: {ops.isinf(tuple_object)}. Dict has"
                                f" `nan`: {ops.isnan(dict_object).any()} and `inf`: {ops.isinf(dict_object)}."
                            ),
                        )
    
                recursive_check(tuple_output, dict_output)
    
        for model_class in self.all_model_classes:
            model = model_class(config)
            model.eval()
    
            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)
            dict_inputs = self._prepare_for_class(inputs_dict, model_class)
>           check_equivalence(model, tuple_inputs, dict_inputs)

tests\ut\transformers\test_modeling_common.py:1796: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\test_modeling_common.py:1761: in check_equivalence
    tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_____ NllbMoeModelTest.test_prompt_lookup_decoding_matches_greedy_search ______

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_prompt_lookup_decoding_matches_greedy_search>

    @is_flaky()
    def test_prompt_lookup_decoding_matches_greedy_search(self):
        # This test ensures that the prompt lookup generation does not introduce output changes over greedy search.
        # This test is mostly a copy of test_assisted_decoding_matches_greedy_search
    
        for model_class in self.all_generative_model_classes:
            if model_class._is_stateful:
                self.skipTest(reason="Stateful models don't support assisted generation")
            if any(model_name in model_class.__name__.lower() for model_name in ["fsmt", "reformer"]):
                self.skipTest(reason="Won't fix: old model with different cache format")
            if any(
                model_name in model_class.__name__.lower()
                for model_name in [
                    "bigbirdpegasus",
                    "led",
                    "mega",
                    "speech2text",
                    "git",
                    "prophetnet",
                    "seamlessm4t",
                    "clvp",
                ]
            ):
                self.skipTest(reason="May fix in the future: need model-specific fixes")
    
            # enable cache
            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)
    
            # NOTE: assisted generation only works with cache on at the moment.
            if not hasattr(config, "use_cache"):
                self.skipTest(reason="This model doesn't support caching")
    
            config.use_cache = True
            config.is_decoder = True
            model = model_class(config).eval()
            # Sets assisted generation arguments such that:
            # a) no EOS is generated, to ensure generation doesn't break early
            # b) the prompt lookup tries to give the model 2 tokens, to ensure the input preparation of
            #    prompt lookup is correct
            # c) there are at least two forward passes in the main model, to ensure the input preparation of
            #    the main model is correct
            generation_kwargs = {
                "eos_token_id": -1,  # see a)
                "max_new_tokens": 4,  # see c)
                "num_beams": 1,
                "do_sample": False,
                "output_scores": True,
                "output_logits": True,
                "output_hidden_states": True,
                "output_attentions": self.has_attentions,
                "return_dict_in_generate": True,
            }
    
>           output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)

tests\ut\transformers\generation\test_utils.py:1228: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1703: in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
mindnlp\transformers\generation\utils.py:537: in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1151: in forward
    layer_outputs = encoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:791: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_______________ NllbMoeModelTest.test_resize_tokens_embeddings ________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_resize_tokens_embeddings>

    def test_resize_tokens_embeddings(self):
        (
            original_config,
            inputs_dict,
        ) = self.model_tester.prepare_config_and_inputs_for_common()
        if not self.test_resize_embeddings:
            self.skipTest(reason="test_resize_embeddings is set to `False`")
    
        for model_class in self.all_model_classes:
            config = copy.deepcopy(original_config)
            model = model_class(config)
            model_embed_pre_resize = model.get_input_embeddings()
            type_model_embed_pre_resize = type(model_embed_pre_resize)
    
            if self.model_tester.is_training is False:
                model.eval()
    
            model_vocab_size = config.text_config.vocab_size if hasattr(config, "text_config") else config.vocab_size
            # Retrieve the embeddings and clone theme
            model_embed = model.resize_token_embeddings(model_vocab_size)
            cloned_embeddings = model_embed.weight.clone()
    
            # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size
            model_embed = model.resize_token_embeddings(model_vocab_size + 10)
            new_model_vocab_size = (
                model.config.text_config.vocab_size
                if hasattr(model.config, "text_config")
                else model.config.vocab_size
            )
            self.assertEqual(new_model_vocab_size, model_vocab_size + 10)
            # Check that it actually resizes the embeddings matrix
            self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)
            # Check to make sure the type of embeddings returned post resizing is same as type of input
            type_model_embed_post_resize = type(model_embed)
            self.assertEqual(type_model_embed_pre_resize, type_model_embed_post_resize)
            # Check that the model can still do a forward pass successfully (every parameter should be resized)
>           model(**self._prepare_for_class(inputs_dict, model_class))

tests\ut\transformers\test_modeling_common.py:1404: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
____________________ NllbMoeModelTest.test_sample_generate ____________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_sample_generate>

    def test_sample_generate(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            model = model_class(config).eval()
            process_kwargs, logits_warper_kwargs = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                forced_bos_token_id=model.config.forced_bos_token_id,
                forced_eos_token_id=model.config.forced_eos_token_id,
            )
    
>           output_generate = self._sample_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                num_return_sequences=1,
                logits_warper_kwargs=logits_warper_kwargs,
                process_kwargs=process_kwargs,
            )

tests\ut\transformers\generation\test_utils.py:500: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:237: in _sample_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1964: in generate
    result = self._sample(
mindnlp\transformers\generation\utils.py:2917: in _sample
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______________ NllbMoeModelTest.test_sample_generate_dict_output ______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_sample_generate_dict_output>

    def test_sample_generate_dict_output(self):
        for model_class in self.all_generative_model_classes:
            config, input_ids, attention_mask = self._get_input_ids_and_config()
    
            config.use_cache = False
            model = model_class(config).eval()
    
            process_kwargs, logits_warper_kwargs = self._get_logits_processor_and_warper_kwargs(
                input_ids.shape[-1],
                forced_bos_token_id=model.config.forced_bos_token_id,
                forced_eos_token_id=model.config.forced_eos_token_id,
            )
>           output_generate = self._sample_generate(
                model=model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                num_return_sequences=2,
                logits_warper_kwargs=logits_warper_kwargs,
                process_kwargs=process_kwargs,
                output_scores=True,
                output_logits=True,
                output_hidden_states=True,
                output_attentions=self.has_attentions,
                return_dict_in_generate=True,
            )

tests\ut\transformers\generation\test_utils.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\ut\transformers\generation\test_utils.py:237: in _sample_generate
    output_generate = model.generate(
D:\Anaconda\envs\MindSpore\lib\contextlib.py:79: in inner
    return func(*args, **kwds)
mindnlp\transformers\generation\utils.py:1964: in generate
    result = self._sample(
mindnlp\transformers\generation\utils.py:2917: in _sample
    outputs = self(**model_inputs, return_dict=True)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1727: in forward
    outputs = self.model(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
_______________________ NllbMoeModelTest.test_save_load _______________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeModelTest testMethod=test_save_load>

    def test_save_load(self):
        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()
    
        def check_save_load(out1, out2):
            # make sure we don't have nans
            out_2 = out2.asnumpy()
            out_2[np.isnan(out_2)] = 0
            out_2 = out_2[~np.isneginf(out_2)]
    
            out_1 = out1.asnumpy()
            out_1[np.isnan(out_1)] = 0
            out_1 = out_1[~np.isneginf(out_1)]
            max_diff = np.amax(np.abs(out_1 - out_2))
            self.assertLessEqual(max_diff, 1e-5)
    
        for model_class in self.all_model_classes:
            model = model_class(config)
            model.eval()
            with no_grad():
>               first = model(**self._prepare_for_class(inputs_dict, model_class))[0]

tests\ut\transformers\test_modeling_common.py:241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1617: in forward
    decoder_outputs = self.decoder(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:1457: in forward
    layer_outputs = decoder_layer(
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:933: in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:547: in forward
    expert_output = expert(masked_hidden_states[idx, token_indices])
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\transformers\models\nllb_moe\modeling_nllb_moe.py:468: in forward
    hidden_states = self.fc1(hidden_states)
mindnlp\core\nn\modules\module.py:338: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
mindnlp\core\nn\modules\module.py:349: in _call_impl
    return forward_call(*args, **kwargs)
mindnlp\core\nn\modules\linear.py:60: in forward
    return F.linear(input, self.weight, self.bias)
mindnlp\core\nn\functional.py:172: in linear
    return dense_(input, weight, bias)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:314: in __call__
    return _run_op(self, self.name, args)
D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\ops\primitive.py:913: in _run_op
    stub = _pynative_executor.run_op_async(obj, op_name, args)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <mindspore.common.api._PyNativeExecutor object at 0x000001DFBE9C1340>
args = (Prim[Dense]<has_bias=True>, 'Dense', (Tensor(shape=[0], dtype=Float32, value= ), Parameter (name=Parameter, shape=(4, 16), dtype=Float32, requires_grad=True), Parameter (name=Parameter, shape=(4,), dtype=Float32, requires_grad=True)))

    def run_op_async(self, *args):
        """
        Run single op async.
    
        Args:
            args (tuple): Op prim and input arguments.
    
        Return:
            StubNode, result of run op.
        """
>       return self._executor.run_op_async(*args)
E       ValueError: The dim of x should be larger than 1 if the dim of w is 2.
E       
E       ----------------------------------------------------
E       - C++ Call Stack: (For framework developers)
E       ----------------------------------------------------
E       mindspore\core\ops\dense.cc:114 mindspore::ops::DenseInfer::InferShape

D:\Anaconda\envs\MindSpore\lib\site-packages\mindspore\common\api.py:1186: ValueError
______________ NllbMoeRouterTest.test_batch_prioritized_routing _______________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeRouterTest testMethod=test_batch_prioritized_routing>

    def test_batch_prioritized_routing(self):
        set_seed(0)
        config = NllbMoeConfig(
            num_experts=4,
            hidden_size=32,
            d_ff=16,
            expert_capacity=4,
            second_expert_policy="random",
        )
        mask = ops.zeros(
            (self.batch_size * self.sequence_length), dtype=mindspore.bool_
        )
        logits = ops.rand((self.batch_size * self.sequence_length, 4))
        config.batch_prioritized_routing = True
        router = NllbMoeTop2Router(config)
        top_1_mask, _ = router.route_tokens(logits, padding_mask=mask)
        # check that the routing is batch first. One of the last token is routed while expert capacity is very small
        # this means that it had a greater probability of being routed
>       assert top_1_mask[-1, 0] == 1
E       assert Tensor(shape=[], dtype=Int64, value= 0) == 1

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:635: AssertionError
____________________ NllbMoeRouterTest.test_top_2_routing _____________________

self = <tests.ut.transformers.models.nllb_moe.test_modeling_nllb_moe.NllbMoeRouterTest testMethod=test_top_2_routing>

    def test_top_2_routing(self):
        # test routing with minimal reproduction
        mask = ops.ones((self.batch_size, self.sequence_length), dtype=mindspore.bool_)
        mask[0][0] = False
        mask[1][0] = False
        mask = mask.reshape(-1)
        set_seed(0)
        hidden_states = ops.rand(
            (self.batch_size, self.sequence_length, self.config.hidden_size)
        )
        classfier = nn.Linear(self.config.hidden_size, self.config.num_experts)
        hf_router = NllbMoeTop2Router(self.config)
    
        _, _, hidden_dim = hidden_states.shape
        logits = classfier(
            hidden_states.reshape((self.batch_size * self.sequence_length), hidden_dim)
        )
        top_1_mask, router_probs = hf_router.route_tokens(logits, padding_mask=mask)
        ops.argmax(top_1_mask, dim=-1)
        router_mask = router_probs.bool()
        set_seed(0)
        experts = [
            nn.Linear(hidden_dim, hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
        ]
        hidden_states = hidden_states.reshape(
            (self.batch_size * self.sequence_length), hidden_dim
        )
        masked_hidden_states = ops.einsum("bm,be->ebm", hidden_states, router_mask)
        for idx, expert in enumerate(experts):
            token_indices = router_mask[:, idx]
    
            # 
            print(f"token_indices.shape: {token_indices.shape}")
            print(f"masked_hidden_states[idx, token_indices].shape: {masked_hidden_states[idx, token_indices].shape}")
    
            combining_weights = router_probs[token_indices, idx]
            expert_output = expert(masked_hidden_states[idx, token_indices])
    
            print(f"expert_output.shape: {expert_output.shape}")
>           exit(0)

tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py:600: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = Use exit() or Ctrl-Z plus Return to exit, code = 0

    def __call__(self, code=None):
        # Shells like IDLE catch the SystemExit, but listen when their
        # stdin wrapper is closed.
        try:
            sys.stdin.close()
        except:
            pass
>       raise SystemExit(code)
E       SystemExit: 0

D:\Anaconda\envs\MindSpore\lib\_sitebuiltins.py:26: SystemExit
=========================== short test summary info ===========================
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_matches_greedy_search_0_random
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_matches_greedy_search_1_same
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_assisted_decoding_sample
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_attention_outputs
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_batching_equivalence
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_sample_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_sample_generate_dict_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate_dict_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_generate_dict_outputs_use_cache
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_beam_search_low_memory
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_constrained_beam_search_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_constrained_beam_search_generate_dict_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate_dict_outputs_use_cache
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_contrastive_generate_low_memory
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_decoder_model_past_with_large_inputs
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_determinism
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_feed_forward_chunking
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_continue_from_past_key_values
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_fp16
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_with_head_masking
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_generate_without_input_ids
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_get_loss
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate_dict_outputs
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_greedy_generate_dict_outputs_use_cache
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_group_beam_search_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_group_beam_search_generate_dict_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_headmasking
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_hidden_states_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_inputs_embeds
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_inputs_embeds_matches_input_ids
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_model_outputs_equivalence
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_prompt_lookup_decoding_matches_greedy_search
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_resize_tokens_embeddings
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_sample_generate
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_sample_generate_dict_output
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeModelTest::test_save_load
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeRouterTest::test_batch_prioritized_routing
FAILED tests\ut\transformers\models\nllb_moe\test_modeling_nllb_moe.py::NllbMoeRouterTest::test_top_2_routing
================= 41 failed, 26 passed, 22 skipped in 34.43s ==================
