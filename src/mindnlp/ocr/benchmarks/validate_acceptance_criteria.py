"""
Issue #2379 éªŒæ”¶æ ‡å‡†è¯„ä¼°è„šæœ¬
è‡ªåŠ¨éªŒè¯æ‰€æœ‰éªŒæ”¶æ ‡å‡†æ˜¯å¦è¾¾æ ‡
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Optional
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨åªæ£€æŸ¥æ–‡ä»¶æ—¶åŠ è½½æ•´ä¸ªæ¨¡å‹
OCREvaluator = None
OCRMetrics = None
VLMOCREngine = None

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def _import_evaluation_modules():
    """å»¶è¿Ÿå¯¼å…¥è¯„ä¼°æ¨¡å—"""
    global OCREvaluator, OCRMetrics, VLMOCREngine
    if OCREvaluator is None:
        try:
            from mindnlp.ocr.core.evaluate import OCREvaluator as _OCREvaluator, OCRMetrics as _OCRMetrics
            from mindnlp.ocr.models.qwen2vl import VLMOCREngine as _VLMOCREngine
            OCREvaluator = _OCREvaluator
            OCRMetrics = _OCRMetrics
            VLMOCREngine = _VLMOCREngine
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥è¯„ä¼°æ¨¡å—: {e}")
            logger.warning("è¯„ä¼°åŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†æ–‡ä»¶æ£€æŸ¥åŠŸèƒ½ä»ç„¶å¯ç”¨")


class AcceptanceCriteriaValidator:
    """éªŒæ”¶æ ‡å‡†éªŒè¯å™¨"""
    
    def __init__(self):
        self.criteria = {
            "cer_reduction": {
                "description": "LoRAå¾®è°ƒåœ¨ç›®æ ‡æ•°æ®é›†ä¸ŠCERé™ä½20%ä»¥ä¸Š",
                "target": 0.20,  # 20%é™ä½
                "passed": False,
                "actual": None
            },
            "table_accuracy": {
                "description": "è¡¨æ ¼è¯†åˆ«ç²¾åº¦æå‡è‡³95%ä»¥ä¸Š",
                "target": 0.95,
                "passed": False,
                "actual": None
            },
            "formula_accuracy": {
                "description": "å…¬å¼è¯†åˆ«ç²¾åº¦æå‡è‡³90%ä»¥ä¸Š",
                "target": 0.90,
                "passed": False,
                "actual": None
            },
            "scripts_provided": {
                "description": "æä¾›å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬",
                "passed": False,
                "files": []
            },
            "model_weights": {
                "description": "æä¾›å¾®è°ƒæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶",
                "passed": False,
                "files": []
            },
            "documentation": {
                "description": "æä¾›è¯¦ç»†çš„å¾®è°ƒæ–‡æ¡£å’Œæœ€ä½³å®è·µ",
                "passed": False,
                "files": []
            }
        }
    
    def validate_cer_reduction(
        self,
        base_model_results: str,
        lora_model_results: str
    ) -> bool:
        """
        éªŒè¯CERé™ä½æ ‡å‡†
        
        Args:
            base_model_results: åŸºç¡€æ¨¡å‹è¯„ä¼°ç»“æœJSONæ–‡ä»¶
            lora_model_results: LoRAæ¨¡å‹è¯„ä¼°ç»“æœJSONæ–‡ä»¶
        """
        logger.info("ğŸ” éªŒè¯CERé™ä½æ ‡å‡†...")
        
        try:
            with open(base_model_results, 'r', encoding='utf-8') as f:
                base_results = json.load(f)
            with open(lora_model_results, 'r', encoding='utf-8') as f:
                lora_results = json.load(f)
            
            base_cer = base_results['metrics']['cer']
            lora_cer = lora_results['metrics']['cer']
            
            reduction = (base_cer - lora_cer) / base_cer
            
            logger.info(f"  åŸºç¡€æ¨¡å‹ CER: {base_cer:.4f}")
            logger.info(f"  LoRAæ¨¡å‹ CER: {lora_cer:.4f}")
            logger.info(f"  CERé™ä½: {reduction*100:.2f}%")
            
            self.criteria["cer_reduction"]["actual"] = reduction
            self.criteria["cer_reduction"]["passed"] = reduction >= self.criteria["cer_reduction"]["target"]
            
            if self.criteria["cer_reduction"]["passed"]:
                logger.info(f"  âœ… CERé™ä½è¾¾æ ‡ (>= {self.criteria['cer_reduction']['target']*100:.0f}%)")
            else:
                logger.warning(f"  âŒ CERé™ä½æœªè¾¾æ ‡ (éœ€è¦ >= {self.criteria['cer_reduction']['target']*100:.0f}%)")
            
            return self.criteria["cer_reduction"]["passed"]
        except Exception as e:
            logger.error(f"  âŒ éªŒè¯CERé™ä½æ—¶å‡ºé”™: {e}")
            return False
    
    def validate_task_accuracy(
        self,
        task_type: str,
        test_data: str,
        model_name: str,
        lora_path: Optional[str] = None,
        device: str = "cpu"
    ) -> bool:
        """
        éªŒè¯ç‰¹å®šä»»åŠ¡çš„è¯†åˆ«ç²¾åº¦
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ (table/formula/handwriting)
            test_data: æµ‹è¯•æ•°æ®JSONæ–‡ä»¶
            model_name: æ¨¡å‹åç§°
            lora_path: LoRAæƒé‡è·¯å¾„
            device: è®¾å¤‡
        """
        logger.info(f"ğŸ” éªŒè¯{task_type}è¯†åˆ«ç²¾åº¦...")
        
        # å¯¼å…¥è¯„ä¼°æ¨¡å—
        _import_evaluation_modules()
        if OCREvaluator is None or VLMOCREngine is None:
            logger.error("  âŒ è¯„ä¼°æ¨¡å—æœªå®‰è£…æˆ–ä¸å¯ç”¨")
            return False
        
        try:
            # åŠ è½½æ¨¡å‹
            engine = VLMOCREngine(
                model_name=model_name,
                device=device,
                lora_path=lora_path
            )
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            evaluator = OCREvaluator(engine)
            results = evaluator.evaluate_dataset(test_data)
            
            # è®¡ç®—å‡†ç¡®ç‡ (1 - CER)
            accuracy = 1 - results['metrics']['cer']
            
            logger.info(f"  {task_type}è¯†åˆ«å‡†ç¡®ç‡: {accuracy*100:.2f}%")
            
            criterion_key = f"{task_type}_accuracy"
            if criterion_key in self.criteria:
                self.criteria[criterion_key]["actual"] = accuracy
                self.criteria[criterion_key]["passed"] = accuracy >= self.criteria[criterion_key]["target"]
                
                if self.criteria[criterion_key]["passed"]:
                    logger.info(f"  âœ… {task_type}è¯†åˆ«ç²¾åº¦è¾¾æ ‡ (>= {self.criteria[criterion_key]['target']*100:.0f}%)")
                else:
                    logger.warning(f"  âŒ {task_type}è¯†åˆ«ç²¾åº¦æœªè¾¾æ ‡ (éœ€è¦ >= {self.criteria[criterion_key]['target']*100:.0f}%)")
                
                return self.criteria[criterion_key]["passed"]
            
            return False
        except Exception as e:
            logger.error(f"  âŒ éªŒè¯{task_type}è¯†åˆ«ç²¾åº¦æ—¶å‡ºé”™: {e}")
            return False
    
    def validate_files_existence(self) -> bool:
        """éªŒè¯å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        logger.info("ğŸ” éªŒè¯å¿…éœ€æ–‡ä»¶...")
        
        project_root = Path(__file__).parent.parent.parent
        
        # æ£€æŸ¥è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
        required_scripts = [
            "src/mindnlp/ocr/finetune/train_lora.py",
            "src/mindnlp/ocr/finetune/dataset.py",
            "src/mindnlp/ocr/finetune/evaluate.py",
            "src/mindnlp/ocr/finetune/prepare_dataset.py",
        ]
        
        # å¯é€‰çš„è¯„ä¼°è„šæœ¬ï¼ˆå¦‚æœå­˜åœ¨ä¼šæ›´å¥½ï¼‰
        optional_scripts = [
            "src/mindnlp/ocr/core/evaluate.py",
            "examples/ocr_eval/evaluate_model.py"
        ]
        
        scripts_found = []
        scripts_missing = []
        
        for script in required_scripts:
            script_path = project_root / script
            if script_path.exists():
                scripts_found.append(script)
            else:
                scripts_missing.append(script)
        
        optional_found = []
        for script in optional_scripts:
            script_path = project_root / script
            if script_path.exists():
                optional_found.append(script)
        
        scripts_found.extend(optional_found)
        
        self.criteria["scripts_provided"]["files"] = scripts_found
        self.criteria["scripts_provided"]["passed"] = len(scripts_missing) == 0
        
        if self.criteria["scripts_provided"]["passed"]:
            logger.info(f"  âœ… å¿…éœ€è„šæœ¬æ–‡ä»¶å·²æä¾› ({len(required_scripts)}/{len(required_scripts)})")
            if optional_found:
                logger.info(f"  âœ… å¯é€‰è„šæœ¬æ–‡ä»¶: {len(optional_found)}ä¸ª")
        else:
            logger.warning(f"  âš ï¸  éƒ¨åˆ†å¿…éœ€è„šæœ¬æ–‡ä»¶ç¼ºå¤±:")
            for missing in scripts_missing:
                logger.warning(f"    - {missing}")
        
        # æ£€æŸ¥æ–‡æ¡£ - ä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…
        required_docs = [
            "docs/ocr_finetuning_guide.md",  # å°å†™ç‰ˆæœ¬
            "docs/OCR_FINETUNING_GUIDE.md",  # å¤§å†™ç‰ˆæœ¬
        ]
        
        optional_docs = [
            "examples/ocr_eval/README.md",
            "docs/ocr_supplement_README.md"
        ]
        
        docs_found = []
        docs_missing = []
        
        # è‡³å°‘è¦æœ‰ä¸€ä¸ªå¾®è°ƒæŒ‡å—
        guide_found = False
        for doc in required_docs:
            doc_path = project_root / doc
            if doc_path.exists():
                docs_found.append(doc)
                guide_found = True
                break
        
        if not guide_found:
            docs_missing.append("docs/ocr_finetuning_guide.md æˆ– OCR_FINETUNING_GUIDE.md")
        
        # æ£€æŸ¥å¯é€‰æ–‡æ¡£
        for doc in optional_docs:
            doc_path = project_root / doc
            if doc_path.exists():
                docs_found.append(doc)
        
        self.criteria["documentation"]["files"] = docs_found
        self.criteria["documentation"]["passed"] = guide_found
        
        if self.criteria["documentation"]["passed"]:
            logger.info(f"  âœ… å¾®è°ƒæ–‡æ¡£å·²æä¾›")
            if len(docs_found) > 1:
                logger.info(f"  âœ… é¢å¤–æ–‡æ¡£: {len(docs_found)-1}ä¸ª")
        else:
            logger.warning(f"  âš ï¸  ç¼ºå°‘å¾®è°ƒæ–‡æ¡£")
        
        return self.criteria["scripts_provided"]["passed"] and self.criteria["documentation"]["passed"]
    
    def validate_model_weights(self, lora_path: str) -> bool:
        """éªŒè¯æ¨¡å‹æƒé‡æ˜¯å¦å­˜åœ¨"""
        logger.info("ğŸ” éªŒè¯æ¨¡å‹æƒé‡...")
        
        lora_path = Path(lora_path)
        
        if not lora_path.exists():
            logger.warning(f"  âŒ LoRAæƒé‡è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
            self.criteria["model_weights"]["passed"] = False
            return False
        
        # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
        required_files = [
            "adapter_model.npz",
            "adapter_config.json"
        ]
        
        files_found = []
        files_missing = []
        
        for file in required_files:
            file_path = lora_path / file
            if file_path.exists():
                files_found.append(str(file_path))
            else:
                files_missing.append(file)
        
        self.criteria["model_weights"]["files"] = files_found
        self.criteria["model_weights"]["passed"] = len(files_missing) == 0
        
        if self.criteria["model_weights"]["passed"]:
            logger.info(f"  âœ… æ¨¡å‹æƒé‡æ–‡ä»¶å®Œæ•´")
        else:
            logger.warning(f"  âŒ éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶ç¼ºå¤±:")
            for missing in files_missing:
                logger.warning(f"    - {missing}")
        
        return self.criteria["model_weights"]["passed"]
    
    def generate_report(self, output_file: Optional[str] = None) -> Dict:
        """ç”ŸæˆéªŒæ”¶æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("Issue #2379 éªŒæ”¶æ ‡å‡†è¯„ä¼°æŠ¥å‘Š")
        logger.info("="*60 + "\n")
        
        total_criteria = len(self.criteria)
        passed_criteria = sum(1 for c in self.criteria.values() if c["passed"])
        
        for key, criterion in self.criteria.items():
            status = "âœ… é€šè¿‡" if criterion["passed"] else "âŒ æœªé€šè¿‡"
            logger.info(f"{status} - {criterion['description']}")
            
            if criterion.get("actual") is not None:
                if isinstance(criterion["actual"], float):
                    logger.info(f"       å®é™…å€¼: {criterion['actual']*100:.2f}%, ç›®æ ‡å€¼: {criterion['target']*100:.0f}%")
            
            if criterion.get("files"):
                logger.info(f"       æ–‡ä»¶æ•°: {len(criterion['files'])}")
        
        logger.info("\n" + "="*60)
        logger.info(f"æ€»ä½“ç»“æœ: {passed_criteria}/{total_criteria} é¡¹è¾¾æ ‡ ({passed_criteria/total_criteria*100:.1f}%)")
        logger.info("="*60 + "\n")
        
        report = {
            "issue": "#2379",
            "timestamp": str(Path.cwd()),
            "total_criteria": total_criteria,
            "passed_criteria": passed_criteria,
            "pass_rate": passed_criteria / total_criteria,
            "criteria": self.criteria,
            "overall_status": "PASSED" if passed_criteria == total_criteria else "FAILED"
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return report


def main():
    parser = argparse.ArgumentParser(description="Issue #2379 éªŒæ”¶æ ‡å‡†è¯„ä¼°")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--mode", type=str, default="files",
                        choices=["files", "cer", "table", "formula", "all"],
                        help="éªŒè¯æ¨¡å¼")
    
    # CERéªŒè¯å‚æ•°
    parser.add_argument("--base_results", type=str,
                        help="åŸºç¡€æ¨¡å‹è¯„ä¼°ç»“æœJSON")
    parser.add_argument("--lora_results", type=str,
                        help="LoRAæ¨¡å‹è¯„ä¼°ç»“æœJSON")
    
    # ä»»åŠ¡ç²¾åº¦éªŒè¯å‚æ•°
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2-VL-7B-Instruct",
                        help="æ¨¡å‹åç§°")
    parser.add_argument("--lora_path", type=str,
                        help="LoRAæƒé‡è·¯å¾„")
    parser.add_argument("--table_test_data", type=str,
                        help="è¡¨æ ¼è¯†åˆ«æµ‹è¯•æ•°æ®")
    parser.add_argument("--formula_test_data", type=str,
                        help="å…¬å¼è¯†åˆ«æµ‹è¯•æ•°æ®")
    parser.add_argument("--device", type=str, default="cpu",
                        help="è®¾å¤‡")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output", type=str, default="acceptance_report.json",
                        help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶")
    
    args = parser.parse_args()
    
    validator = AcceptanceCriteriaValidator()
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡ŒéªŒè¯
    if args.mode in ["files", "all"]:
        validator.validate_files_existence()
        if args.lora_path:
            validator.validate_model_weights(args.lora_path)
    
    if args.mode in ["cer", "all"]:
        if args.base_results and args.lora_results:
            validator.validate_cer_reduction(args.base_results, args.lora_results)
        else:
            logger.warning("âš ï¸  è·³è¿‡CERéªŒè¯ï¼šç¼ºå°‘è¯„ä¼°ç»“æœæ–‡ä»¶")
    
    if args.mode in ["table", "all"]:
        if args.table_test_data and args.lora_path:
            validator.validate_task_accuracy(
                "table",
                args.table_test_data,
                args.model_name,
                args.lora_path,
                args.device
            )
        else:
            logger.warning("âš ï¸  è·³è¿‡è¡¨æ ¼è¯†åˆ«éªŒè¯ï¼šç¼ºå°‘æµ‹è¯•æ•°æ®æˆ–æ¨¡å‹")
    
    if args.mode in ["formula", "all"]:
        if args.formula_test_data and args.lora_path:
            validator.validate_task_accuracy(
                "formula",
                args.formula_test_data,
                args.model_name,
                args.lora_path,
                args.device
            )
        else:
            logger.warning("âš ï¸  è·³è¿‡å…¬å¼è¯†åˆ«éªŒè¯ï¼šç¼ºå°‘æµ‹è¯•æ•°æ®æˆ–æ¨¡å‹")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = validator.generate_report(args.output)
    
    # è¿”å›çŠ¶æ€ç 
    sys.exit(0 if report["overall_status"] == "PASSED" else 1)


if __name__ == "__main__":
    main()
