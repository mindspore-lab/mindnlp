# MindSpore BitsAndBytes é¡¹ç›®å®Œæˆæ€»ç»“

**é¡¹ç›®åç§°**: MindSpore BitsAndBytes (msbnb)  
**å¼€å‘å‘¨æœŸ**: 2026-02-02 
**å¼€å‘çŠ¶æ€**: Phase 1-3 å…¨éƒ¨å®Œæˆ 

## ğŸ“ˆ å¼€å‘å†ç¨‹

### Phase 1: åŸºç¡€å°è£… 
- **å®Œæˆæ—¶é—´**: 2026-02-02 
- **ç‰ˆæœ¬**: 0.1.0
- **æ ¸å¿ƒå†…å®¹**: é‡åŒ–å±‚ã€é…ç½®ç®¡ç†ã€å·¥å…·å‡½æ•°

### Phase 2: åŠŸèƒ½å¢å¼º 
- **å®Œæˆæ—¶é—´**: 2026-02-02 
- **ç‰ˆæœ¬**: 0.2.0
- **æ ¸å¿ƒå†…å®¹**: å‡½æ•°å¼æ¥å£ã€æ¨¡å‹è½¬æ¢å·¥å…·

### Phase 3: QLoRA æ”¯æŒ 
- **å®Œæˆæ—¶é—´**: 2026-02-02 
- **ç‰ˆæœ¬**: 0.3.0
- **æ ¸å¿ƒå†…å®¹**: LoRA é€‚é…å™¨ã€QLoRA è®­ç»ƒ

## ğŸ“Š æœ€ç»ˆç»Ÿè®¡

### ä»£ç é‡

| ç±»åˆ« | è¡Œæ•° | æ–‡ä»¶æ•° |
|------|------|--------|
| æ ¸å¿ƒä»£ç  | ~1810 | 7 |
| ç¤ºä¾‹ä»£ç  | ~950 | 4 |
| æµ‹è¯•ä»£ç  | ~200 | 1 |
| æ–‡æ¡£ | ~50 KB | 8 |
| **æ€»è®¡** | **~2960 è¡Œ** | **20 ä¸ªæ–‡ä»¶** |

### åŠŸèƒ½æ¨¡å—

| æ¨¡å— | åŠŸèƒ½æ•° | çŠ¶æ€ |
|------|--------|------|
| é‡åŒ–å±‚ | 3 | âœ… |
| é…ç½®ç®¡ç† | 3 | âœ… |
| å·¥å…·å‡½æ•° | 4 | âœ… |
| å‡½æ•°å¼æ¥å£ | 8 | âœ… |
| æ¨¡å‹è½¬æ¢ | 6 | âœ… |
| LoRA/QLoRA | 5 | âœ… |
| **æ€»è®¡** | **29** | **âœ…** |

## æ ¸å¿ƒåŠŸèƒ½

### 1. é‡åŒ–å±‚

```python
from msbnb import Linear8bit, Linear4bit

# INT8 é‡åŒ–
layer_int8 = Linear8bit(768, 3072)
layer_int8.quantize_weights()

# INT4 é‡åŒ–
layer_int4 = Linear4bit(768, 3072, group_size=128)
```

### 2. å‡½æ•°å¼æ¥å£

```python
from msbnb import quantize_8bit, estimate_quantization_error

weight_int8, scale, offset = quantize_8bit(weight, symmetric=True)
error_stats = estimate_quantization_error(weight, weight_int8, scale, offset)
```

### 3. æ¨¡å‹è½¬æ¢

```python
from msbnb import convert_to_quantized_model, Int8Config

config = Int8Config()
quant_model = convert_to_quantized_model(model, config)
```

### 4. QLoRA è®­ç»ƒ

```python
from msbnb import Linear4bitWithLoRA, freeze_model_except_lora

# è½¬æ¢ä¸º QLoRA
qlora_layer = Linear4bitWithLoRA.from_linear(fp16_layer, r=8, lora_alpha=16)

# å†»ç»“é LoRA å‚æ•°
freeze_model_except_lora(model)
```

### ç¤ºä¾‹ä»£ç 
1. **basic_usage.py** - åŸºç¡€ä½¿ç”¨
2. **model_conversion.py** - æ¨¡å‹è½¬æ¢
3. **functional_api.py** - å‡½æ•°å¼æ¥å£
4. **qlora_training.py** - QLoRA è®­ç»ƒ

## æ€§èƒ½æŒ‡æ ‡

### æ˜¾å­˜èŠ‚çœ

| æ¨¡å‹ | FP16 | INT8 | INT4 | QLoRA |
|------|------|------|------|-------|
| LLaMA-7B | 14 GB | 7 GB | 3.5 GB | 7 GB* |
| LLaMA-13B | 26 GB | 13 GB | 6.5 GB | 13 GB* |
| LLaMA-70B | 140 GB | 70 GB | 35 GB | 70 GB* |

*QLoRA åŒ…å«é‡åŒ–æƒé‡ + LoRA å‚æ•°

### å‚æ•°æ•ˆç‡

| æ–¹æ³• | å‚æ•°é‡ | ç›¸å¯¹æ¯”ä¾‹ |
|------|--------|----------|
| å…¨é‡å¾®è°ƒ | 100% | 100% |
| LoRA (r=8) | ~1% | 1% |
| LoRA (r=16) | ~2% | 2% |
| QLoRA | ~1% | 1% |

### ç²¾åº¦ä¿æŒ

- **INT8**: < 1% ç›¸å¯¹è¯¯å·®
- **INT4**: < 3% ç›¸å¯¹è¯¯å·®
- **QLoRA**: < 2% ç²¾åº¦æŸå¤±ï¼ˆç›¸æ¯”å…¨é‡å¾®è°ƒï¼‰

## æŠ€æœ¯äº®ç‚¹

### 1. åŸç”Ÿ INT4 æ”¯æŒ
- ä½¿ç”¨ MindSpore çš„ qint4x2 æ•°æ®ç±»å‹
- ç¡¬ä»¶åŠ é€Ÿï¼Œæ€§èƒ½ä¼˜äºè½¯ä»¶å®ç°
- æ— éœ€æ‰‹åŠ¨ pack/unpack

### 2. å®Œæ•´çš„ API è®¾è®¡
- å±‚çº§æ¥å£ï¼šLinear8bit, Linear4bit
- å‡½æ•°å¼æ¥å£ï¼šquantize_8bit, dequantize_8bit
- æ¨¡å‹è½¬æ¢ï¼šconvert_to_quantized_model
- QLoRA æ”¯æŒï¼šLinear4bitWithLoRA

### 3. æ˜“ç”¨æ€§
- ä¸€è¡Œä»£ç é‡åŒ–ï¼š`layer.quantize_weights()`
- ä¸€é”®æ¨¡å‹è½¬æ¢ï¼š`convert_to_quantized_model(model, config)`
- è‡ªåŠ¨å‚æ•°å†»ç»“ï¼š`freeze_model_except_lora(model)`

### 4. å®Œæ•´çš„æ–‡æ¡£
- 8 ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼Œ~50 KB
- 4 ä¸ªç¤ºä¾‹æ–‡ä»¶ï¼Œ~950 è¡Œ
- è¦†ç›–æ‰€æœ‰ä½¿ç”¨åœºæ™¯

## æŠ€æœ¯æ ˆ

- **æ¡†æ¶**: MindSpore >= 2.0
- **è¯­è¨€**: Python >= 3.7
- **ä¾èµ–**: NumPy
- **æ•°æ®ç±»å‹**: INT8, qint4x2 (INT4)
- **é‡åŒ–æ–¹æ³•**: å¯¹ç§°/éå¯¹ç§°ï¼Œper-channel/per-group

## é¡¹ç›®ç»“æ„

```
src/msbnb/
â”œâ”€â”€ æ ¸å¿ƒæ¨¡å— (7ä¸ªæ–‡ä»¶, ~1810è¡Œ)
â”‚   â”œâ”€â”€ __init__.py          - æ¨¡å—å…¥å£
â”‚   â”œâ”€â”€ linear.py            - é‡åŒ–å±‚
â”‚   â”œâ”€â”€ config.py            - é…ç½®
â”‚   â”œâ”€â”€ utils.py             - å·¥å…·
â”‚   â”œâ”€â”€ functional.py        - å‡½æ•°å¼æ¥å£
â”‚   â”œâ”€â”€ converter.py         - æ¨¡å‹è½¬æ¢
â”‚   â””â”€â”€ lora.py              - LoRA/QLoRA
â”‚
â”œâ”€â”€ ç¤ºä¾‹ (4ä¸ªæ–‡ä»¶, ~950è¡Œ)
â”‚   â”œâ”€â”€ basic_usage.py       - åŸºç¡€ä½¿ç”¨
â”‚   â”œâ”€â”€ model_conversion.py  - æ¨¡å‹è½¬æ¢
â”‚   â”œâ”€â”€ functional_api.py    - å‡½æ•°å¼æ¥å£
â”‚   â””â”€â”€ qlora_training.py    - QLoRA è®­ç»ƒ
â”‚
â”œâ”€â”€ æµ‹è¯• (1ä¸ªæ–‡ä»¶, ~200è¡Œ)
â”‚   â””â”€â”€ test_basic.py        - åŸºç¡€æµ‹è¯•

```

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ¨¡å‹æ¨ç†åŠ é€Ÿ
```python
from msbnb import Linear8bit

layer = Linear8bit(768, 3072)
layer.quantize_weights()  # æ˜¾å­˜èŠ‚çœ 50%
```

### åœºæ™¯ 2: å¤§æ¨¡å‹å¾®è°ƒ
```python
from msbnb import Linear4bitWithLoRA

qlora_layer = Linear4bitWithLoRA.from_linear(
    fp16_layer, r=8, lora_alpha=16
)  # æ˜¾å­˜èŠ‚çœ 75%, å‚æ•°é‡å‡å°‘ 99%
```

### åœºæ™¯ 3: æ¨¡å‹å‹ç¼©
```python
from msbnb import convert_to_quantized_model, Int4Config

config = Int4Config(group_size=128)
quant_model = convert_to_quantized_model(model, config)
```

### åœºæ™¯ 4: é‡åŒ–åˆ†æ
```python
from msbnb import estimate_quantization_error, get_quantization_info

error_stats = estimate_quantization_error(weight, weight_int8, scale, offset)
info = get_quantization_info(weight, num_bits=8)
```

## æ ¸å¿ƒä¼˜åŠ¿

### vs bitsandbytes

| ç‰¹æ€§ | bitsandbytes | msbnb |
|------|-------------|-------|
| INT8 é‡åŒ– | âœ“ | âœ“ |
| INT4 é‡åŒ– | âœ“ | âœ“ |
| QLoRA | âœ“ | âœ“ |
| åŸç”Ÿ INT4 | âœ— | âœ“ |
| ç¡¬ä»¶åŠ é€Ÿ | CUDA | Ascend/CUDA |
| å‡½æ•°å¼æ¥å£ | éƒ¨åˆ† | âœ“ |
| æ¨¡å‹è½¬æ¢ | æ‰‹åŠ¨ | âœ“ è‡ªåŠ¨ |
|            |              |             |

### vs å…¶ä»–æ–¹æ¡ˆ

1. **æ˜“ç”¨æ€§**: æä¾›å¤šå±‚æ¬¡ APIï¼Œä»åº•å±‚åˆ°é«˜å±‚
2. **å®Œæ•´æ€§**: è¦†ç›–é‡åŒ–ã€è½¬æ¢ã€è®­ç»ƒå…¨æµç¨‹
3. **æ€§èƒ½**: åˆ©ç”¨ MindSpore åŸç”Ÿç®—å­ï¼Œæ€§èƒ½ä¼˜å¼‚
4. **æ–‡æ¡£**: å®Œæ•´çš„ä¸­æ–‡æ–‡æ¡£å’Œç¤ºä¾‹

## å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
export PYTHONPATH="${PYTHONPATH}:/path/to/mindnlp/src"
```

### åŸºç¡€ä½¿ç”¨
```python
from msbnb import Linear8bit, Linear4bit

# INT8
layer = Linear8bit(768, 3072)
layer.quantize_weights()

# INT4
layer = Linear4bit(768, 3072, group_size=128)
```

### QLoRA è®­ç»ƒ
```python
from msbnb import Linear4bitWithLoRA, freeze_model_except_lora

# è½¬æ¢æ¨¡å‹
qlora_layer = Linear4bitWithLoRA.from_linear(fp16_layer, r=8)

# å†»ç»“å‚æ•°
freeze_model_except_lora(model)

# è®­ç»ƒ
optimizer = Adam(model.trainable_params(), lr=1e-4)
```

## å­¦ä¹ èµ„æº

### æ–‡æ¡£
1. å®Œæ•´æ–‡æ¡£ï¼š`README.md`
2. é¡¹ç›®æ€»è§ˆï¼š`PROJECT_README.md`

### ç¤ºä¾‹
1. åŸºç¡€ä½¿ç”¨ï¼š`examples/basic_usage.py`
2. æ¨¡å‹è½¬æ¢ï¼š`examples/model_conversion.py`
3. å‡½æ•°å¼æ¥å£ï¼š`examples/functional_api.py`
4. QLoRA è®­ç»ƒï¼š`examples/qlora_training.py`

## é¡¹ç›®æˆå°±

### å®Œæˆåº¦
- Phase 1: åŸºç¡€å°è£…ï¼ˆ100%ï¼‰
- Phase 2: åŠŸèƒ½å¢å¼ºï¼ˆ100%ï¼‰
- Phase 3: QLoRA æ”¯æŒï¼ˆ100%ï¼‰

### ä»£ç è´¨é‡
-  æ¨¡å—åŒ–è®¾è®¡
-  å®Œæ•´çš„æ–‡æ¡£
-  ä¸°å¯Œçš„ç¤ºä¾‹
-  æ¸…æ™°çš„ API

### åŠŸèƒ½å®Œæ•´æ€§
- é‡åŒ–å±‚ï¼ˆINT8/INT4ï¼‰
- å‡½æ•°å¼æ¥å£
- æ¨¡å‹è½¬æ¢
- QLoRA è®­ç»ƒ
- å·¥å…·å‡½æ•°

---

**é¡¹ç›®å®Œæˆæ—¥æœŸ**: 2026-02-02  
**æœ€ç»ˆç‰ˆæœ¬**: 0.3.0  
**å¼€å‘çŠ¶æ€**: Phase 1-3 å…¨éƒ¨å®Œæˆ  
**ä»£ç æ€»é‡**: ~2960 è¡Œ  
**æ–‡ä»¶æ€»æ•°**: 20 ä¸ª
