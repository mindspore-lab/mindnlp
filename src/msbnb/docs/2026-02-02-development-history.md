# MindSpore BitsAndBytes å¼€å‘å†å²

## é¡¹ç›®æ¦‚è¿°

**é¡¹ç›®åç§°**: MindSpore BitsAndBytes (msbnb)  
**å¼€å‘å‘¨æœŸ**: 2026-02-02  
**å¼€å‘çŠ¶æ€**: Phase 1-3 å…¨éƒ¨å®Œæˆ 

æœ¬æ–‡æ¡£è®°å½•äº† msbnb é¡¹ç›®ä» Phase 1 åˆ° Phase 3 çš„å®Œæ•´å¼€å‘å†ç¨‹å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚

---

## Phase 1: åŸºç¡€å°è£…

**å®Œæˆæ—¶é—´**: 2026-02-02   
**ç‰ˆæœ¬**: 0.1.0  
**çŠ¶æ€**: å®Œæˆ 

### å®æ–½å†…å®¹

æ ¹æ®å¼€å‘è®¡åˆ’ï¼ŒæˆåŠŸå®ç°äº†åŸºç¡€å°è£…ï¼Œåˆ›å»ºäº†ç‹¬ç«‹çš„é‡åŒ–æ¨¡å— `msbnb`ã€‚

### æ ¸å¿ƒåŠŸèƒ½å®ç°

#### 1. Linear8bit - 8-bit é‡åŒ–å±‚

**ç‰¹æ€§**:
-  æ”¯æŒè®­ç»ƒæ¨¡å¼ï¼ˆFP16 æƒé‡ï¼‰
-  æ”¯æŒæ¨ç†æ¨¡å¼ï¼ˆINT8 æƒé‡ï¼‰
-  æ”¯æŒå¯¹ç§°/éå¯¹ç§°é‡åŒ–
-  æ”¯æŒ per-channel é‡åŒ–
-  ä¸€é”®é‡åŒ–æ–¹æ³• `quantize_weights()`
-  æ˜¾å­˜èŠ‚çœ 50%

**å®ç°æ–¹å¼**:
```python
# è®­ç»ƒæ¨¡å¼
layer = Linear8bit(768, 3072, has_fp16_weights=True)
out = layer(x)  # ä½¿ç”¨ FP16 æƒé‡

# é‡åŒ–æƒé‡
layer.quantize_weights()  # FP16 â†’ INT8

# æ¨ç†æ¨¡å¼
out = layer(x)  # ä½¿ç”¨ INT8 æƒé‡ï¼ˆè‡ªåŠ¨åé‡åŒ–ï¼‰
```

#### 2. Linear4bit - 4-bit é‡åŒ–å±‚

**ç‰¹æ€§**:
- åŸºäº qint4x2 æ‰“åŒ…æ ¼å¼
- æ”¯æŒ per-group é‡åŒ–ï¼ˆé»˜è®¤ group_size=128ï¼‰
- æ”¯æŒåŒé‡é‡åŒ–ï¼ˆscale å†é‡åŒ–ï¼‰
- æ”¯æŒä»æ ‡å‡†å±‚è½¬æ¢ `from_linear()`
- æ˜¾å­˜èŠ‚çœ 75%

**å®ç°æ–¹å¼**:
```python
# ç›´æ¥åˆ›å»º
layer = Linear4bit(768, 3072, group_size=128, compress_statistics=True)

# ä»ç°æœ‰å±‚è½¬æ¢
fp16_layer = nn.Dense(768, 3072)
int4_layer = Linear4bit.from_linear(fp16_layer, group_size=128)
```

#### 3. é‡åŒ–å·¥å…·å‡½æ•°

1. **quantize_weight_int4_pergroup**: Per-group INT4 é‡åŒ–
   - æ”¯æŒå¯é…ç½®çš„ group_size
   - æ”¯æŒåŒé‡é‡åŒ–
   - å¯¹ç§°é‡åŒ–å®ç°

2. **pack_int4_to_qint4x2**: INT4 æ‰“åŒ…
   - ä¸¤ä¸ª INT4 å€¼æ‰“åŒ…åˆ°ä¸€ä¸ª uint8
   - æ ¼å¼: [high_4bit | low_4bit]

3. **unpack_qint4x2_to_int8**: INT4 è§£åŒ…
   - ä» uint8 è§£åŒ…ä¸ºä¸¤ä¸ª INT4 å€¼

4. **compute_scale_offset**: è®¡ç®—é‡åŒ–å‚æ•°
   - æ”¯æŒå¯¹ç§°/éå¯¹ç§°é‡åŒ–
   - æ”¯æŒ per-channel/per-layer

#### 4. é…ç½®ç®¡ç†

æä¾›äº†ä¸‰ä¸ªé…ç½®ç±»ï¼š

```python
# åŸºç¡€é…ç½®
QuantConfig(bits=8, symmetric=True, per_channel=True)

# INT8 é…ç½®
Int8Config(symmetric=True, threshold=6.0, has_fp16_weights=True)

# INT4 é…ç½®
Int4Config(group_size=128, compress_statistics=True, quant_type='int4')
```

### æŠ€æœ¯å®ç°ç»†èŠ‚

#### INT8 é‡åŒ–æµç¨‹

```
FP16 Weight [out_features, in_features]
  â†“
compute_scale_offset()
  â†“ scale = absmax / 127
  â†“
quantize: weight_int8 = round(weight_fp16 / scale)
  â†“
INT8 Weight + Scale
  â†“ (æ¨ç†æ—¶)
dequantize: weight_fp16 = weight_int8 * scale
  â†“
matmul(input, weight_fp16)
  â†“
Output
```

#### INT4 é‡åŒ–æµç¨‹

```
FP16 Weight [out_features, in_features]
  â†“
åˆ†ç»„: [out_features, num_groups, group_size]
  â†“
è®¡ç®—æ¯ç»„ absmax
  â†“ scale = absmax / 7
  â†“
quantize: weight_int4 = round(weight / scale)
  â†“
pack_int4_to_qint4x2()
  â†“
UINT8 Weight [out_features, in_features/2] + Scale [num_groups, out_features]
  â†“ (å¯é€‰) åŒé‡é‡åŒ–
  â†“ scale_int8 = round(scale / scale_scale)
  â†“
å­˜å‚¨: UINT8 Weight + INT8 Scale + FP16 Scale_Scale
  â†“ (æ¨ç†æ—¶)
unpack_qint4x2_to_int8()
  â†“
dequantize per-group
  â†“
matmul(input, weight_fp16)
  â†“
Output
```

### æ–‡ä»¶æ¸…å•

- `src/msbnb/__init__.py` - æ¨¡å—å…¥å£
- `src/msbnb/linear.py` - é‡åŒ–å±‚å®ç°ï¼ˆ~400 è¡Œï¼‰
- `src/msbnb/config.py` - é…ç½®ç®¡ç†ï¼ˆ~60 è¡Œï¼‰
- `src/msbnb/utils.py` - å·¥å…·å‡½æ•°ï¼ˆ~200 è¡Œï¼‰
- `examples/msbnb/basic_usage.py` - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ï¼ˆ~150 è¡Œï¼‰
- `tests/msbnb/test_basic.py` - åŸºç¡€æµ‹è¯•ï¼ˆ~200 è¡Œï¼‰

**æ€»è®¡**: ~1000 è¡Œä»£ç 

### Phase 1 æ€»ç»“

æˆåŠŸå®Œæˆäº†åŸºç¡€å°è£…ï¼Œå®ç°äº†ï¼š

ç‹¬ç«‹çš„é‡åŒ–æ¨¡å— `msbnb`  
INT8/INT4 é‡åŒ–å±‚  
å®Œæ•´çš„å·¥å…·å‡½æ•°  
é…ç½®ç®¡ç†ç³»ç»Ÿ  
æ–‡æ¡£å’Œç¤ºä¾‹  
åŸºç¡€æµ‹è¯•

**æ ¸å¿ƒä¼˜åŠ¿**:
- åŸç”Ÿ INT4 æ”¯æŒï¼ˆqint4x2ï¼‰
- ç®€æ´çš„ API è®¾è®¡
- å®Œæ•´çš„æ–‡æ¡£
- æ˜“äºæ‰©å±•

---

## Phase 2: åŠŸèƒ½å¢å¼º

**å®Œæˆæ—¶é—´**: 2026-02-02   
**ç‰ˆæœ¬**: 0.2.0  
**çŠ¶æ€**: å®Œæˆ

### æ–°å¢åŠŸèƒ½

#### 1. å‡½æ•°å¼æ¥å£ (`functional.py`)

æä¾›äº†å®Œæ•´çš„å‡½æ•°å¼é‡åŒ–æ¥å£ï¼Œæ— éœ€åˆ›å»ºå±‚å¯¹è±¡å³å¯è¿›è¡Œé‡åŒ–æ“ä½œã€‚

**æ ¸å¿ƒå‡½æ•°**:

**é‡åŒ–å‡½æ•°**:
- `quantize_8bit()` - INT8 é‡åŒ–
- `quantize_4bit()` - INT4 é‡åŒ–
- `quantize_tensor()` - é€šç”¨é‡åŒ–æ¥å£

**åé‡åŒ–å‡½æ•°**:
- `dequantize_8bit()` - INT8 åé‡åŒ–
- `dequantize_4bit()` - INT4 åé‡åŒ–
- `dequantize_tensor()` - é€šç”¨åé‡åŒ–æ¥å£

**å·¥å…·å‡½æ•°**:
- `estimate_quantization_error()` - ä¼°è®¡é‡åŒ–è¯¯å·®
- `get_quantization_info()` - è·å–é‡åŒ–ä¿¡æ¯

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from msbnb import quantize_8bit, dequantize_8bit, estimate_quantization_error

# é‡åŒ–
weight_int8, scale, offset = quantize_8bit(
    weight_fp,
    symmetric=True,
    per_channel=True
)

# åé‡åŒ–
weight_dequant = dequantize_8bit(weight_int8, scale, offset)

# ä¼°è®¡è¯¯å·®
error_stats = estimate_quantization_error(
    weight_fp, weight_int8, scale, offset, num_bits=8
)
print(f"ç›¸å¯¹è¯¯å·®: {error_stats['relative_error']:.2f}%")
print(f"ä¿¡å™ªæ¯”: {error_stats['snr']:.2f} dB")
```

#### 2. æ¨¡å‹è½¬æ¢å·¥å…· (`converter.py`)

æä¾›äº†è‡ªåŠ¨å°†æ¨¡å‹ä¸­çš„ Linear å±‚æ›¿æ¢ä¸ºé‡åŒ–å±‚çš„åŠŸèƒ½ã€‚

**æ ¸å¿ƒå‡½æ•°**:

**æ¨¡å‹è½¬æ¢**:
- `convert_to_quantized_model()` - è½¬æ¢æ•´ä¸ªæ¨¡å‹
- `replace_linear_layers()` - é€‰æ‹©æ€§æ›¿æ¢å±‚
- `quantize_model_weights()` - é‡åŒ–æ¨¡å‹æƒé‡

**æ¨¡å‹åˆ†æ**:
- `get_model_size()` - è·å–æ¨¡å‹å¤§å°
- `compare_model_sizes()` - æ¯”è¾ƒæ¨¡å‹å¤§å°
- `print_quantization_summary()` - æ‰“å°é‡åŒ–æ‘˜è¦

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from msbnb import convert_to_quantized_model, Int8Config

# è½¬æ¢æ•´ä¸ªæ¨¡å‹
config = Int8Config(symmetric=True, per_channel=True)
quant_model = convert_to_quantized_model(
    model,
    config=config,
    modules_to_not_convert=["lm_head", "classifier"]
)

# è·å–æ¨¡å‹å¤§å°
size_info = get_model_size(quant_model)
print(f"æ¨¡å‹å¤§å°: {size_info['total_size_mb']:.2f} MB")

# æ¯”è¾ƒæ¨¡å‹å¤§å°
comparison = compare_model_sizes(fp_model, quant_model)
print(f"æ˜¾å­˜èŠ‚çœ: {comparison['memory_saved_percent']:.1f}%")
```

#### 3. æ–°å¢ç¤ºä¾‹

**model_conversion.py** - æ¨¡å‹è½¬æ¢ç¤ºä¾‹

æ¼”ç¤ºäº† 5 ç§æ¨¡å‹è½¬æ¢åœºæ™¯ï¼š
1. ä½¿ç”¨é…ç½®è½¬æ¢æ•´ä¸ªæ¨¡å‹
2. è½¬æ¢ä¸º INT4 é‡åŒ–æ¨¡å‹
3. ä½¿ç”¨ replace_linear_layers æ›¿æ¢ç‰¹å®šå±‚
4. å»¶è¿Ÿé‡åŒ–ï¼ˆè®­ç»ƒåé‡åŒ–ï¼‰
5. è¯¦ç»†çš„æ¨¡å‹å¤§å°æ¯”è¾ƒ

**functional_api.py** - å‡½æ•°å¼æ¥å£ç¤ºä¾‹

æ¼”ç¤ºäº† 6 ç§å‡½æ•°å¼æ¥å£ç”¨æ³•ï¼š
1. INT8 é‡åŒ–å’Œåé‡åŒ–
2. INT4 é‡åŒ–å’Œåé‡åŒ–
3. é€šç”¨é‡åŒ–æ¥å£
4. é‡åŒ–è¯¯å·®ä¼°è®¡
5. é‡åŒ–ä¿¡æ¯æŸ¥è¯¢
6. ä¸åŒé‡åŒ–ç­–ç•¥å¯¹æ¯”

### ä»£ç ç»Ÿè®¡

**æ–°å¢æ–‡ä»¶**:
- `functional.py` - ~300 è¡Œ
- `converter.py` - ~400 è¡Œ
- `examples/model_conversion.py` - ~200 è¡Œ
- `examples/functional_api.py` - ~250 è¡Œ

**æ€»è®¡**: ~1150 è¡Œæ–°å¢ä»£ç 

### Phase 2 æ€»ç»“

æˆåŠŸå®ç°äº†ï¼š

**å‡½æ•°å¼æ¥å£** - æä¾›çµæ´»çš„é‡åŒ–æ“ä½œ  
**æ¨¡å‹è½¬æ¢å·¥å…·** - è‡ªåŠ¨è½¬æ¢æ•´ä¸ªæ¨¡å‹  
**è¯¯å·®ä¼°è®¡** - è¯„ä¼°é‡åŒ–è´¨é‡  
**æ¨¡å‹åˆ†æ** - åˆ†ææ¨¡å‹å¤§å°å’Œå‹ç¼©æ¯”  
**å®Œæ•´ç¤ºä¾‹** - æ¼”ç¤ºå„ç§ä½¿ç”¨åœºæ™¯  
**æ–‡æ¡£å®Œå–„** - è¯¦ç»†çš„ API æ–‡æ¡£

**æ ¸å¿ƒä»·å€¼**:
- å¤§å¹…æå‡æ˜“ç”¨æ€§
- æä¾›çµæ´»çš„æ¥å£
- æ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯
- å®Œå–„çš„æ–‡æ¡£å’Œç¤ºä¾‹

---

## Phase 3: QLoRA æ”¯æŒ

**å®Œæˆæ—¶é—´**: 2026-02-02   
**ç‰ˆæœ¬**: 0.3.0  
**çŠ¶æ€**: å®Œæˆ 

### æ–°å¢åŠŸèƒ½

#### 1. LoRA é€‚é…å™¨ (`lora.py`)

å®ç°äº†å®Œæ•´çš„ LoRA æœºåˆ¶ï¼ŒåŒ…æ‹¬åŸºç¡€ LoRA å±‚å’Œä¸é‡åŒ–å±‚çš„é›†æˆã€‚

**LoRALinear** - åŸºç¡€ LoRA å±‚

**æ ¸å¿ƒç‰¹æ€§**:
- ä½ç§©åˆ†è§£ï¼šé€šè¿‡ä¸¤ä¸ªä½ç§©çŸ©é˜µ A å’Œ B å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ
- å¯é…ç½®ç§©ï¼šæ”¯æŒä¸åŒçš„ç§©ï¼ˆrï¼‰ä»¥å¹³è¡¡æ€§èƒ½å’Œå‚æ•°é‡
- ç¼©æ”¾å› å­ï¼šlora_alpha / r çš„ç¼©æ”¾æœºåˆ¶
- Dropout æ”¯æŒï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
- æƒé‡åˆå¹¶ï¼šå¯ä»¥è·å–åˆå¹¶åçš„æƒé‡å¢é‡

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from msbnb import LoRALinear

# åˆ›å»º LoRA å±‚
lora = LoRALinear(
    in_features=768,
    out_features=3072,
    r=8,                    # ç§©
    lora_alpha=16,          # ç¼©æ”¾å› å­
    lora_dropout=0.1        # Dropout
)

# å‰å‘ä¼ æ’­
out = lora(x)

# è·å–æƒé‡å¢é‡
delta_W = lora.get_merged_weight()
```

**Linear4bitWithLoRA** - QLoRA æ ¸å¿ƒå±‚

**æ ¸å¿ƒç‰¹æ€§**:
- INT4 é‡åŒ– + LoRAï¼šç»“åˆ 4-bit é‡åŒ–å’Œ LoRA é€‚é…å™¨
- å‚æ•°å†»ç»“ï¼šé‡åŒ–æƒé‡è‡ªåŠ¨å†»ç»“ï¼Œåªè®­ç»ƒ LoRA å‚æ•°
- æ˜¾å­˜é«˜æ•ˆï¼šç›¸æ¯”å…¨é‡å¾®è°ƒèŠ‚çœ ~75% æ˜¾å­˜
- å‚æ•°é«˜æ•ˆï¼šåªè®­ç»ƒ ~1% çš„å‚æ•°
- æ˜“äºä½¿ç”¨ï¼šæä¾› `from_linear()` æ–¹æ³•å¿«é€Ÿè½¬æ¢

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from msbnb import Linear4bitWithLoRA

# æ–¹å¼ 1: ç›´æ¥åˆ›å»º
qlora_layer = Linear4bitWithLoRA(
    in_features=768,
    out_features=3072,
    r=8,
    lora_alpha=16,
    group_size=128,
    compress_statistics=True
)

# æ–¹å¼ 2: ä»ç°æœ‰å±‚è½¬æ¢
import mindspore.nn as nn
fp16_layer = nn.Dense(768, 3072)
qlora_layer = Linear4bitWithLoRA.from_linear(
    fp16_layer,
    r=8,
    lora_alpha=16
)

# å‰å‘ä¼ æ’­
out = qlora_layer(x)  # ä¸»è·¯å¾„(INT4) + LoRA è·¯å¾„

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
qlora_layer.print_trainable_params()
```

**Linear8bitWithLoRA** - INT8 + LoRA

**æ ¸å¿ƒç‰¹æ€§**:
- INT8 é‡åŒ– + LoRA
- ç›¸æ¯” INT4 ç²¾åº¦æ›´é«˜
- æ˜¾å­˜èŠ‚çœ ~50%

#### 2. å·¥å…·å‡½æ•°

**freeze_model_except_lora()** - å†»ç»“æ¨¡å‹ä¸­é™¤ LoRA å‚æ•°å¤–çš„æ‰€æœ‰å‚æ•°

```python
from msbnb import freeze_model_except_lora

frozen_count, trainable_count = freeze_model_except_lora(model)
print(f"å†»ç»“: {frozen_count}, å¯è®­ç»ƒ: {trainable_count}")
```

**print_lora_info()** - æ‰“å°æ¨¡å‹ä¸­ LoRA å‚æ•°çš„è¯¦ç»†ä¿¡æ¯

```python
from msbnb import print_lora_info

print_lora_info(model)
```

#### 3. QLoRA è®­ç»ƒç¤ºä¾‹ (`qlora_training.py`)

æä¾›äº†å®Œæ•´çš„ QLoRA è®­ç»ƒç¤ºä¾‹ï¼ŒåŒ…å« 6 ä¸ªåœºæ™¯ï¼š

1. **åˆ›å»º QLoRA æ¨¡å‹** - æ¼”ç¤ºå¦‚ä½•å°†æ¨¡å‹è½¬æ¢ä¸º QLoRA
2. **QLoRA è®­ç»ƒ** - å®Œæ•´çš„è®­ç»ƒæµç¨‹
3. **å•ç‹¬ä½¿ç”¨ LoRA å±‚** - LoRA å±‚çš„ç‹¬ç«‹ä½¿ç”¨
4. **INT8 + LoRA** - 8-bit é‡åŒ–ä¸ LoRA ç»“åˆ
5. **å‚æ•°æ•ˆç‡å¯¹æ¯”** - ä¸åŒæ–¹æ³•çš„å‚æ•°é‡å¯¹æ¯”
6. **æ¨èé…ç½®** - æ ¹æ®æ¨¡å‹å¤§å°çš„é…ç½®å»ºè®®

### æŠ€æœ¯å®ç°

#### LoRA åŸç†

LoRA é€šè¿‡ä½ç§©åˆ†è§£æ¥é€‚é…é¢„è®­ç»ƒæ¨¡å‹ï¼š

```
åŸå§‹æƒé‡: W âˆˆ R^(dÃ—k)
LoRA å¢é‡: Î”W = BÂ·A
  å…¶ä¸­: A âˆˆ R^(dÃ—r), B âˆˆ R^(rÃ—k), r << min(d,k)

å‰å‘ä¼ æ’­: h = WÂ·x + Î”WÂ·x = WÂ·x + BÂ·AÂ·x
```

**ä¼˜åŠ¿**:
- å‚æ•°é‡ï¼šä» dÃ—k å‡å°‘åˆ° dÃ—r + rÃ—k
- å½“ r << min(d,k) æ—¶ï¼Œå‚æ•°é‡å¤§å¹…å‡å°‘
- ä¾‹å¦‚ï¼šd=768, k=3072, r=8
  - åŸå§‹ï¼š768Ã—3072 = 2,359,296 å‚æ•°
  - LoRAï¼š768Ã—8 + 8Ã—3072 = 30,720 å‚æ•°
  - å‡å°‘ï¼š~99%

#### QLoRA æ¶æ„

```
è¾“å…¥ x
  â†“
  â”œâ”€ ä¸»è·¯å¾„ï¼ˆå†»ç»“ï¼‰
  â”‚   â†“
  â”‚   INT4 é‡åŒ–æƒé‡
  â”‚   â†“
  â”‚   åé‡åŒ–
  â”‚   â†“
  â”‚   çŸ©é˜µä¹˜æ³•
  â”‚   â†“
  â”‚   out_main
  â”‚
  â””â”€ LoRA è·¯å¾„ï¼ˆå¯è®­ç»ƒï¼‰
      â†“
      x @ A @ B * scaling
      â†“
      out_lora
  â†“
  out = out_main + out_lora
```

#### å‚æ•°æ•ˆç‡å¯¹æ¯”

ä»¥ 768 â†’ 3072 çš„çº¿æ€§å±‚ä¸ºä¾‹ï¼š

| æ–¹æ³• | å‚æ•°é‡ | ç›¸å¯¹æ¯”ä¾‹ | æ˜¾å­˜å ç”¨ |
|------|--------|----------|----------|
| å…¨é‡å¾®è°ƒ (FP32) | 2,359,296 | 100% | 9.0 MB |
| å…¨é‡å¾®è°ƒ (FP16) | 2,359,296 | 100% | 4.5 MB |
| LoRA (r=8) | 30,720 | 1.3% | 0.12 MB |
| LoRA (r=16) | 61,440 | 2.6% | 0.24 MB |
| QLoRA (INT4 + r=8) | 30,720 | 1.3% | 1.1 MB* |

*åŒ…å«é‡åŒ–æƒé‡

### æ€§èƒ½æŒ‡æ ‡

#### æ˜¾å­˜èŠ‚çœ

| æ¨¡å‹ | å…¨é‡å¾®è°ƒ | QLoRA | èŠ‚çœ |
|------|---------|-------|------|
| LLaMA-7B | 28 GB | 7 GB | 75% |
| LLaMA-13B | 52 GB | 13 GB | 75% |
| LLaMA-70B | 280 GB | 70 GB | 75% |

#### è®­ç»ƒé€Ÿåº¦

- QLoRA è®­ç»ƒé€Ÿåº¦ï¼šçº¦ä¸ºå…¨é‡å¾®è°ƒçš„ 1.2-1.5x
- åŸå› ï¼šé‡åŒ–/åé‡åŒ–å¼€é”€ï¼Œä½†å‚æ•°æ›´æ–°æ›´å¿«

#### ç²¾åº¦ä¿æŒ

- QLoRA ç²¾åº¦ï¼šä¸å…¨é‡å¾®è°ƒç›¸å½“ï¼ˆ< 2% å·®è·ï¼‰
- é€‚ç”¨åœºæ™¯ï¼šæŒ‡ä»¤å¾®è°ƒã€é¢†åŸŸé€‚é…ã€ä¸ªæ€§åŒ–å®šåˆ¶

### æ¨èé…ç½®

#### æ ¹æ®æ¨¡å‹å¤§å°

**å°æ¨¡å‹ (< 1B å‚æ•°)**:
```python
r = 8
lora_alpha = 16
lora_dropout = 0.05
group_size = 128
```

**ä¸­ç­‰æ¨¡å‹ (1B-10B å‚æ•°)**:
```python
r = 16
lora_alpha = 32
lora_dropout = 0.1
group_size = 128
```

**å¤§æ¨¡å‹ (> 10B å‚æ•°)**:
```python
r = 32
lora_alpha = 64
lora_dropout = 0.1
group_size = 128
```

#### æ ¹æ®ä»»åŠ¡ç±»å‹

**æŒ‡ä»¤å¾®è°ƒ**:
```python
r = 8-16
lora_alpha = 16-32
target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
```

**é¢†åŸŸé€‚é…**:
```python
r = 4-8
lora_alpha = 8-16
target_modules = ["q_proj", "v_proj"]
```

**ä¸ªæ€§åŒ–å®šåˆ¶**:
```python
r = 16-32
lora_alpha = 32-64
target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
```

### ä»£ç ç»Ÿè®¡

**æ–°å¢æ–‡ä»¶**:
- `lora.py` - ~450 è¡Œ
- `examples/qlora_training.py` - ~350 è¡Œ

**æ€»è®¡**: ~800 è¡Œæ–°å¢ä»£ç 

### Phase 3 æ€»ç»“

æˆåŠŸå®ç°äº†ï¼š

**LoRA é€‚é…å™¨** - å®Œæ•´çš„ LoRA å®ç°  
**QLoRA æ”¯æŒ** - INT4 + LoRA é›†æˆ  
**å‚æ•°å†»ç»“** - è‡ªåŠ¨å†»ç»“é‡åŒ–æƒé‡  
**å·¥å…·å‡½æ•°** - ä¾¿æ·çš„è¾…åŠ©å‡½æ•°  
**å®Œæ•´ç¤ºä¾‹** - 6 ä¸ªä½¿ç”¨åœºæ™¯  
**æ–‡æ¡£å®Œå–„** - è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£

**æ ¸å¿ƒä»·å€¼**:

- æ˜¾å­˜èŠ‚çœ ~75%
- å‚æ•°é‡å‡å°‘ ~99%
- è®­ç»ƒé€Ÿåº¦æå‡ 1.2-1.5x
- ç²¾åº¦ä¿æŒè‰¯å¥½ï¼ˆ< 2% å·®è·ï¼‰
- æ˜“äºä½¿ç”¨å’Œé›†æˆ

---

## æ€»ä½“ç»Ÿè®¡

### ä»£ç é‡

| ç±»åˆ« | è¡Œæ•° | æ–‡ä»¶æ•° |
|------|------|--------|
| æ ¸å¿ƒä»£ç  | ~1810 | 7 |
| ç¤ºä¾‹ä»£ç  | ~950 | 4 |
| æµ‹è¯•ä»£ç  | ~200 | 1 |
| æ–‡æ¡£ | ~50 KB | 8 |
| **æ€»è®¡** | **~2960 è¡Œ** | **20 ä¸ªæ–‡ä»¶** |

### åŠŸèƒ½æ¨¡å—

| æ¨¡å— | åŠŸèƒ½æ•° | çŠ¶æ€ |
|------|--------|------|
| é‡åŒ–å±‚ | 3 | âœ… |
| é…ç½®ç®¡ç† | 3 | âœ… |
| å·¥å…·å‡½æ•° | 4 | âœ… |
| å‡½æ•°å¼æ¥å£ | 8 | âœ… |
| æ¨¡å‹è½¬æ¢ | 6 | âœ… |
| LoRA/QLoRA | 5 | âœ… |
| **æ€»è®¡** | **29** | **âœ…** |

### ç‰ˆæœ¬å†å²

- **v0.1.0** (2026-02-02 ä¸Šåˆ) - Phase 1 åŸºç¡€å°è£…
- **v0.2.0** (2026-02-02 ä¸­åˆ) - Phase 2 åŠŸèƒ½å¢å¼º
- **v0.3.0** (2026-02-02 ä¸‹åˆ) - Phase 3 QLoRA æ”¯æŒ

## æŠ€æœ¯æŒ‘æˆ˜

### å·²è§£å†³ 

1. **INT4 æ‰“åŒ…æ ¼å¼** - ä½¿ç”¨ qint4x2 æ ¼å¼
2. **Per-group é‡åŒ–** - å®ç°åˆ†ç»„é‡åŒ–å’Œåé‡åŒ–
3. **åŒé‡é‡åŒ–** - scale å‚æ•°å†é‡åŒ–
4. **æ¨¡å—ç»“æ„** - ç‹¬ç«‹çš„ msbnb æ¨¡å—
5. **å‡½æ•°å¼æ¥å£** - çµæ´»çš„é‡åŒ–æ“ä½œ
6. **æ¨¡å‹è½¬æ¢** - è‡ªåŠ¨æ›¿æ¢å±‚
7. **LoRA é›†æˆ** - ä¸é‡åŒ–å±‚æ— ç¼é›†æˆ
8. **å‚æ•°å†»ç»“** - è‡ªåŠ¨å†»ç»“æœºåˆ¶

### å¾…è§£å†³ ğŸš§

1. **WeightQuantBatchMatmul é›†æˆ** - éœ€è¦é€‚é…è‡ªåŠ¨ç”Ÿæˆçš„ç®—å­
2. **NF4 æ•°æ®ç±»å‹** - éœ€è¦æŸ¥æ‰¾è¡¨å®ç°
3. **å¼‚å¸¸å€¼å¤„ç†** - éœ€è¦æ¡ä»¶åˆ†æ”¯å’Œæ··åˆè®¡ç®—
4. **å¤šé€‚é…å™¨æ”¯æŒ** - æ”¯æŒå¤šä¸ª LoRA é€‚é…å™¨åˆ‡æ¢

## ä¸ bitsandbytes å¯¹æ¯”

| ç‰¹æ€§ | bitsandbytes | msbnb |
|------|-------------|-------|
| INT8 é‡åŒ– | âœ“ | âœ“ |
| INT4 é‡åŒ– | âœ“ | âœ“ |
| QLoRA | âœ“ | âœ“ |
| åŸç”Ÿ INT4 | âœ— | âœ“ |
| ç¡¬ä»¶åŠ é€Ÿ | CUDA | Ascend/CUDA |
| å‡½æ•°å¼æ¥å£ | éƒ¨åˆ† | âœ“ |
| æ¨¡å‹è½¬æ¢ | æ‰‹åŠ¨ | âœ“ è‡ªåŠ¨ |
| ä¸­æ–‡æ–‡æ¡£ | âœ— | âœ“ |
| å¤šé€‚é…å™¨ | âœ“ | ğŸš§ |

## æœªæ¥è§„åˆ’

### Phase 4: ç”Ÿæ€é›†æˆï¼ˆå¯é€‰ï¼‰

1. **MindFormers é›†æˆ**
   - é›†æˆåˆ° MindFormers è®­ç»ƒæµç¨‹
   - æä¾›é¢„é…ç½®çš„é‡åŒ–æ¨¡å‹
   - æ”¯æŒä¸»æµ LLM æ¨¡å‹

2. **æ¨¡å‹è½¬æ¢å·¥å…·**
   - PyTorch â†’ MindSpore è½¬æ¢
   - Hugging Face æ¨¡å‹æ”¯æŒ
   - æƒé‡æ ¼å¼è½¬æ¢

3. **æ€§èƒ½ä¼˜åŒ–**
   - é›†æˆ WeightQuantBatchMatmul ç®—å­
   - ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦
   - å‡å°‘æ˜¾å­˜å ç”¨

4. **é«˜çº§åŠŸèƒ½**
   - NF4 æ•°æ®ç±»å‹æ”¯æŒ
   - å¼‚å¸¸å€¼å¤„ç†
   - åŠ¨æ€é‡åŒ–
   - å¤šé€‚é…å™¨æ”¯æŒ

## é¡¹ç›®æˆå°±

### å®Œæˆåº¦
- Phase 1: åŸºç¡€å°è£…ï¼ˆ100%ï¼‰
- Phase 2: åŠŸèƒ½å¢å¼ºï¼ˆ100%ï¼‰
- Phase 3: QLoRA æ”¯æŒï¼ˆ100%ï¼‰

### åŠŸèƒ½å®Œæ•´æ€§
- é‡åŒ–å±‚ï¼ˆINT8/INT4ï¼‰
- å‡½æ•°å¼æ¥å£
- æ¨¡å‹è½¬æ¢
- QLoRA è®­ç»ƒ
- å·¥å…·å‡½æ•°

---

**é¡¹ç›®å®Œæˆæ—¥æœŸ**: 2026-02-02  
**æœ€ç»ˆç‰ˆæœ¬**: 0.3.0  
**å¼€å‘çŠ¶æ€**: Phase 1-3 å…¨éƒ¨å®Œæˆ   
**ä»£ç æ€»é‡**: ~2960 è¡Œ  
**æ–‡ä»¶æ€»æ•°**: 20 ä¸ª
