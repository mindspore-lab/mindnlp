# MindSpore BitsAndBytes (msbnb)

åŸºäº MindSpore åŸç”Ÿé‡åŒ–ç®—å­å®ç°çš„ bitsandbytes é£æ ¼é‡åŒ–åº“ã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ… **INT8 é‡åŒ–**: æ”¯æŒ 8-bit æƒé‡é‡åŒ–ï¼Œæ˜¾å­˜èŠ‚çœ 50%
- âœ… **INT4 é‡åŒ–**: æ”¯æŒ 4-bit æƒé‡é‡åŒ–ï¼Œæ˜¾å­˜èŠ‚çœ 75%
- âœ… **Per-channel/Per-group é‡åŒ–**: æ›´ç²¾ç»†çš„é‡åŒ–ç²’åº¦
- âœ… **åŒé‡é‡åŒ–**: å¯¹ scale å‚æ•°å†æ¬¡é‡åŒ–ï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
- âœ… **å‡½æ•°å¼æ¥å£**: çµæ´»çš„é‡åŒ–æ“ä½œ âœ¨ Phase 2
- âœ… **æ¨¡å‹è½¬æ¢å·¥å…·**: ä¸€é”®è½¬æ¢ç°æœ‰æ¨¡å‹ âœ¨ Phase 2
- ğŸš§ **QLoRA æ”¯æŒ**: æ”¯æŒå¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒï¼ˆPhase 3ï¼‰

## å®‰è£…

```bash
# ä»æºç å®‰è£…
cd src
pip install -e .
```

## å¿«é€Ÿå¼€å§‹

### INT8 é‡åŒ–

```python
import numpy as np
import mindspore as ms
from mindspore import Tensor
from msbnb import Linear8bit

# åˆ›å»º INT8 é‡åŒ–å±‚
layer = Linear8bit(768, 3072, has_fp16_weights=True)

# è®­ç»ƒæ¨¡å¼ï¼ˆæƒé‡ä¿æŒ FP16ï¼‰
x = Tensor(np.random.randn(32, 768), dtype=ms.float16)
out = layer(x)

# é‡åŒ–æƒé‡
layer.quantize_weights()

# æ¨ç†æ¨¡å¼ï¼ˆä½¿ç”¨ INT8 æƒé‡ï¼‰
out = layer(x)
```

### INT4 é‡åŒ–

```python
from msbnb import Linear4bit

# åˆ›å»º INT4 é‡åŒ–å±‚
layer = Linear4bit(768, 3072, group_size=128, compress_statistics=True)

x = Tensor(np.random.randn(32, 768), dtype=ms.float16)
out = layer(x)

# ä»ç°æœ‰å±‚è½¬æ¢
import mindspore.nn as nn
fp16_layer = nn.Dense(768, 3072)
int4_layer = Linear4bit.from_linear(fp16_layer, group_size=128)
```

### æ¨¡å‹è½¬æ¢ âœ¨ æ–°å¢

```python
from msbnb import convert_to_quantized_model, Int8Config

# è½¬æ¢æ•´ä¸ªæ¨¡å‹
config = Int8Config(symmetric=True, per_channel=True)
quant_model = convert_to_quantized_model(
    model,
    config=config,
    modules_to_not_convert=["lm_head", "classifier"]
)

# è·å–æ¨¡å‹å¤§å°
from msbnb import get_model_size, compare_model_sizes

size_info = get_model_size(quant_model)
comparison = compare_model_sizes(fp_model, quant_model)
print(f"æ˜¾å­˜èŠ‚çœ: {comparison['memory_saved_percent']:.1f}%")
```

### å‡½æ•°å¼æ¥å£ âœ¨ æ–°å¢

```python
from msbnb import quantize_8bit, dequantize_8bit, estimate_quantization_error

# é‡åŒ–
weight_int8, scale, offset = quantize_8bit(
    weight_fp,
    symmetric=True,
    per_channel=True
)

# åé‡åŒ–
weight_dequant = dequantize_8bit(weight_int8, scale, offset)

# ä¼°è®¡è¯¯å·®
error_stats = estimate_quantization_error(
    weight_fp, weight_int8, scale, offset, num_bits=8
)
print(f"ç›¸å¯¹è¯¯å·®: {error_stats['relative_error']:.2f}%")
print(f"ä¿¡å™ªæ¯”: {error_stats['snr']:.2f} dB")
```

### é…ç½®ç®¡ç†

```python
from msbnb import Int8Config, Int4Config

# INT8 é…ç½®
config = Int8Config(
    symmetric=True,
    per_channel=True,
    threshold=6.0
)

# INT4 é…ç½®
config = Int4Config(
    group_size=128,
    compress_statistics=True
)
```

## æ¶æ„è®¾è®¡

```
msbnb/
â”œâ”€â”€ __init__.py          # æ¨¡å—å…¥å£
â”œâ”€â”€ linear.py            # é‡åŒ–çº¿æ€§å±‚
â”‚   â”œâ”€â”€ Linear8bit       # 8-bit é‡åŒ–å±‚
â”‚   â”œâ”€â”€ Linear4bit       # 4-bit é‡åŒ–å±‚
â”‚   â””â”€â”€ LinearQuant      # åŸºç±»
â”œâ”€â”€ config.py            # é‡åŒ–é…ç½®
â”‚   â”œâ”€â”€ QuantConfig      # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ Int8Config       # INT8 é…ç½®
â”‚   â””â”€â”€ Int4Config       # INT4 é…ç½®
â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ quantize_weight_int4_pergroup
â”‚   â”œâ”€â”€ pack_int4_to_qint4x2
â”‚   â”œâ”€â”€ unpack_qint4x2_to_int8
â”‚   â””â”€â”€ compute_scale_offset
â””â”€â”€ README.md            # æ–‡æ¡£
```

## æŠ€æœ¯ç»†èŠ‚

### INT8 é‡åŒ–

- **é‡åŒ–æ–¹æ³•**: å¯¹ç§°/éå¯¹ç§°é‡åŒ–
- **é‡åŒ–ç²’åº¦**: Per-channel / Per-layer
- **æ•°æ®ç±»å‹**: INT8
- **æ˜¾å­˜èŠ‚çœ**: 50%
- **ç²¾åº¦æŸå¤±**: < 1%

### INT4 é‡åŒ–

- **é‡åŒ–æ–¹æ³•**: å¯¹ç§°é‡åŒ–
- **é‡åŒ–ç²’åº¦**: Per-group (é»˜è®¤ 128)
- **æ•°æ®ç±»å‹**: qint4x2 (æ‰“åŒ…æ ¼å¼)
- **åŒé‡é‡åŒ–**: å¯é€‰
- **æ˜¾å­˜èŠ‚çœ**: 75%
- **ç²¾åº¦æŸå¤±**: < 3%

### ä¸ bitsandbytes å¯¹æ¯”

| ç‰¹æ€§ | bitsandbytes | msbnb |
|-----|-------------|-------|
| INT8 é‡åŒ– | âœ“ | âœ“ |
| INT4 é‡åŒ– | âœ“ | âœ“ |
| NF4 æ•°æ®ç±»å‹ | âœ“ | âœ— (ä½¿ç”¨æ ‡å‡† INT4) |
| å¼‚å¸¸å€¼å¤„ç† | âœ“ | ğŸš§ |
| QLoRA | âœ“ | ğŸš§ |
| ç¡¬ä»¶åŠ é€Ÿ | CUDA | Ascend/CUDA |
| åŸç”Ÿ INT4 | âœ— | âœ“ (qint4x2) |

## æ€§èƒ½æŒ‡æ ‡

### æ˜¾å­˜å ç”¨

| æ¨¡å‹ | FP16 | INT8 | INT4 |
|------|------|------|------|
| LLaMA-7B | 14 GB | 7 GB | 3.5 GB |
| LLaMA-13B | 26 GB | 13 GB | 6.5 GB |
| Qwen-7B | 14 GB | 7 GB | 3.5 GB |

### æ¨ç†é€Ÿåº¦

- INT8: 1.5-2x ååé‡æå‡
- INT4: 2-3x ååé‡æå‡

## å¼€å‘è·¯çº¿

### Phase 1: åŸºç¡€å°è£… âœ…
- [x] Linear8bit å®ç°
- [x] Linear4bit å®ç°
- [x] é‡åŒ–å·¥å…·å‡½æ•°
- [x] é…ç½®ç®¡ç†

### Phase 2: åŠŸèƒ½å¢å¼º ğŸš§
- [ ] æ¨¡å‹è½¬æ¢å·¥å…·
- [ ] å‡½æ•°å¼æ¥å£
- [ ] æ··åˆç²¾åº¦æ”¯æŒ
- [ ] æ€§èƒ½ä¼˜åŒ–

### Phase 3: QLoRA æ”¯æŒ ğŸš§
- [ ] LoRA é€‚é…å™¨
- [ ] å‚æ•°å†»ç»“æœºåˆ¶
- [ ] è®­ç»ƒç¤ºä¾‹

### Phase 4: ç”Ÿæ€é›†æˆ ğŸ“‹
- [ ] MindFormers é›†æˆ
- [ ] æ¨¡å‹è½¬æ¢å·¥å…·
- [ ] å®Œæ•´æ–‡æ¡£

## ç¤ºä¾‹

æ›´å¤šç¤ºä¾‹è¯·å‚è€ƒ `examples/msbnb/` ç›®å½•ã€‚

## å‚è€ƒæ–‡çŒ®

1. [LLM.int8()](https://arxiv.org/abs/2208.07339) - 8-bit Matrix Multiplication for Transformers
2. [QLoRA](https://arxiv.org/abs/2305.14314) - Efficient Finetuning of Quantized LLMs
3. [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) - å®˜æ–¹å®ç°


