"""
æœåŠ¡å™¨ç«¯ KV Cache åŠŸèƒ½æµ‹è¯•
æµ‹è¯• NPU è®¾å¤‡ä¸Šçš„ KV Cache å’Œä¼˜åŒ–åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

print("=" * 80)
print("KV Cache and Flash Attention - Server Test (NPU)")
print("=" * 80)

# æµ‹è¯•1: å¯¼å…¥æ¨¡å—ï¼ˆé¿å…å¯¼å…¥æ•´ä¸ªmindnlpåŒ…ï¼‰
print("\n[Test 1] Importing cache_manager directly...")
try:
    # ç›´æ¥å¯¼å…¥ï¼Œç»•è¿‡ mindnlp.__init__.py
    import importlib.util
    
    cache_manager_path = project_root / "src/mindnlp/ocr/utils/cache_manager.py"
    spec = importlib.util.spec_from_file_location("cache_manager", cache_manager_path)
    cache_manager = importlib.util.module_from_spec(spec)
    
    # å…ˆè®¾ç½®æ¨¡å—åˆ° sys.modules é¿å… dataclass é—®é¢˜
    sys.modules['cache_manager'] = cache_manager
    spec.loader.exec_module(cache_manager)
    
    CacheConfig = cache_manager.CacheConfig
    KVCacheManager = cache_manager.KVCacheManager
    detect_flash_attention_support = cache_manager.detect_flash_attention_support
    get_optimal_cache_config = cache_manager.get_optimal_cache_config
    
    print("âœ… Successfully imported cache_manager components")
except Exception as e:
    print(f"âŒ Failed to import cache_manager: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯•2: åˆ›å»ºé…ç½®
print("\n[Test 2] Creating CacheConfig for NPU...")
try:
    config = CacheConfig(
        enable_kv_cache=True,
        max_cache_size_mb=1024.0,
        enable_lru=True,
        cache_ttl_seconds=300.0,
        enable_flash_attention=False,  # NPU ä¸æ”¯æŒ
    )
    print(f"âœ… CacheConfig created:")
    print(f"   - KV Cache: {config.enable_kv_cache}")
    print(f"   - Max Size: {config.max_cache_size_mb} MB")
    print(f"   - LRU: {config.enable_lru}")
    print(f"   - Flash Attention: {config.enable_flash_attention}")
except Exception as e:
    print(f"âŒ Failed to create config: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•3: KVCacheManager åŸºæœ¬æ“ä½œ
print("\n[Test 3] Testing KVCacheManager...")
try:
    manager = KVCacheManager(config)
    print("âœ… KVCacheManager created")
    
    # æµ‹è¯• put/get
    import torch
    test_tensor = torch.randn(10, 10)
    manager.put("test_key", test_tensor)
    print("âœ… Cache put successful")
    
    retrieved = manager.get("test_key")
    if retrieved is not None and torch.equal(retrieved, test_tensor):
        print("âœ… Cache get successful")
    else:
        print("âŒ Cache get failed")
    
    # æµ‹è¯•ç»Ÿè®¡
    stats = manager.get_stats()
    print(f"âœ… Cache stats: {stats}")
    
    # æµ‹è¯•æ¸…ç†
    manager.clear()
    print("âœ… Cache cleared")
    
except Exception as e:
    print(f"âŒ KVCacheManager test failed: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•4: Flash Attention æ£€æµ‹ï¼ˆNPUåº”è¯¥ä¸æ”¯æŒï¼‰
print("\n[Test 4] Flash Attention support detection...")
try:
    supported, reason = detect_flash_attention_support()
    if supported:
        print(f"âœ… Flash Attention supported: {reason}")
    else:
        print(f"âœ… Flash Attention not supported (expected for NPU): {reason}")
except Exception as e:
    print(f"âŒ Flash Attention detection failed: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•5: NPUä¼˜åŒ–é…ç½®
print("\n[Test 5] Getting optimal config for NPU...")
try:
    npu_config = get_optimal_cache_config("npu:0", model_size_gb=7.0)
    print(f"âœ… NPU optimal config:")
    print(f"   - KV Cache: {npu_config.enable_kv_cache}")
    print(f"   - Flash Attention: {npu_config.enable_flash_attention}")
    print(f"   - Max Cache Size: {npu_config.max_cache_size_mb} MB")
except Exception as e:
    print(f"âŒ Optimal config failed: {e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•6: æµ‹è¯•å¸¦NPZæ¨¡å‹çš„å®é™…æ¨ç†ï¼ˆå¦‚æœæ¨¡å‹å­˜åœ¨ï¼‰
print("\n[Test 6] Testing with actual model (if available)...")
model_path = "/data1/model_weights/qwen2vl_lora_merged.npz"
if os.path.exists(model_path):
    print(f"âœ… Model found: {model_path}")
    print("   Attempting to load model with KV Cache...")
    
    try:
        from mindnlp.ocr.models.qwen2vl import Qwen2VLModel
        from PIL import Image
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_img = Image.new('RGB', (800, 600), color='white')
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨ä¼˜åŒ–é…ç½®ï¼‰
        model = Qwen2VLModel(
            model_name=model_path,
            device="npu:0",
            cache_config=npu_config
        )
        print("âœ… Model loaded successfully")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        print(f"âœ… Model info:")
        print(f"   - Device: {model_info.get('device')}")
        print(f"   - KV Cache Enabled: {model_info.get('kv_cache_enabled')}")
        print(f"   - Flash Attention: {model_info.get('flash_attention_enabled')}")
        print(f"   - Attention Implementation: {model_info.get('attn_implementation', 'N/A')}")
        
        # è¿è¡Œä¸€æ¬¡æ¨ç†æµ‹è¯•
        print("\n   Running inference test...")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_img},
                    {"type": "text", "text": "Extract text."}
                ]
            }
        ]
        
        import time
        start_time = time.time()
        result = model.infer(messages, max_new_tokens=128)
        inference_time = time.time() - start_time
        
        print(f"âœ… Inference completed in {inference_time:.2f}s")
        print(f"   Result: {result[:100] if result else 'Empty'}...")
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = model.get_cache_stats()
        print(f"âœ… Cache stats after inference: {cache_stats}")
        
    except Exception as e:
        print(f"âš ï¸  Model test failed (expected if dependencies missing): {e}")
        import traceback
        traceback.print_exc()
else:
    print(f"âš ï¸  Model not found at {model_path}, skipping model test")

print("\n" + "=" * 80)
print("Server Test Summary")
print("=" * 80)
print("âœ… All basic tests completed!")
print("\nğŸ“ Next steps:")
print("1. Run full benchmark: python scripts/ocr/benchmark_kv_cache.py")
print("2. Run comparison test: python scripts/ocr/benchmark_comparison.py")
print("\nTo run benchmarks on NPU:")
print("cd /data1/mindnlp")
print("python scripts/ocr/benchmark_kv_cache.py \\")
print("    --model_path /data1/model_weights/qwen2vl_lora_merged.npz \\")
print("    --device npu:0 \\")
print("    --output /data1/benchmark_results/kv_cache_npu.json")
