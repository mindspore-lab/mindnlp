{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:21.128535Z",
     "iopub.status.busy": "2023-04-19T07:56:21.128258Z",
     "iopub.status.idle": "2023-04-19T07:56:22.828889Z",
     "shell.execute_reply": "2023-04-19T07:56:22.828308Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindnlp.models.gpt2 import gpt2, config_gpt2\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from os.path import join\n",
    "from mindspore import Tensor, Parameter, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.dataset import text\n",
    "from mindnlp.transforms.tokenizers.bert_tokenizer import BertTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面进行自定义数据集来加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.831820Z",
     "iopub.status.busy": "2023-04-19T07:56:22.831465Z",
     "iopub.status.idle": "2023-04-19T07:56:22.835718Z",
     "shell.execute_reply": "2023-04-19T07:56:22.835293Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        # self.batch_size = batch_size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.data_list[index].strip()\n",
    "        input_ids = [int(token_id) for token_id in input_ids.split()]\n",
    "        return np.array(input_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里是数据集的下载地址，包括train和evaluate的数据，分别是一个txt文件，将这两个文件下载后放置于GPT2-Summary-mindspore/data目录下，即可开始运行该train代码\n",
    "train：https://workdrive.zoho.com.cn/file/gs6x30edfd9e0cd904025820c7722e8a95dc2\n",
    "evaluate：https://workdrive.zoho.com.cn/file/gs6x39f2f13afbaf0403b894709126c1e660e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.837886Z",
     "iopub.status.busy": "2023-04-19T07:56:22.837634Z",
     "iopub.status.idle": "2023-04-19T07:56:22.848643Z",
     "shell.execute_reply": "2023-04-19T07:56:22.848221Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD = '[PAD]'\n",
    "pad_id = 0\n",
    "logger = None\n",
    "\n",
    "\"\"\"\n",
    "设置训练参数\n",
    "\"\"\"\n",
    "# 设置命令行参数\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', default='6', type=str, required=False, help='设置使用哪些显卡')\n",
    "parser.add_argument('--no_cuda', default=False, help='不使用GPU进行训练')\n",
    "parser.add_argument('--model_config', default='GPT2-Summary-mindspore/config/model_config_dialogue_small.json', type=str, required=False,\n",
    "                    help='选择模型参数')\n",
    "parser.add_argument('--vocab_path', default='GPT2-Summary-mindspore/vocabulary/vocab_small.txt', type=str, required=False, help='选择词库')\n",
    "parser.add_argument('--train_raw_path', default='GPT2-Summary-mindspore/data/train_with_summ.txt', type=str, required=False, help='原始训练语料')\n",
    "parser.add_argument('--train_tokenized_path', default='GPT2-Summary-mindspore/data/train_tokenized.txt', type=str,\n",
    "                    required=False,\n",
    "                    help='将原始训练语料tokenize之后的数据的存放位置')\n",
    "parser.add_argument('--log_path', default='GPT2-Summary-mindspore/data/training.log', type=str, required=False, help='训练日志存放位置')\n",
    "parser.add_argument('--raw', default=True, help='是否对原始训练语料做tokenize。若尚未对原始训练语料进行tokenize，则指定该参数')\n",
    "parser.add_argument('--epochs', default=6, type=int, required=False, help='训练的轮次')\n",
    "parser.add_argument('--batch_size', default=5, type=int, required=False, help='训练batch size')\n",
    "parser.add_argument('--lr', default=1.5e-4, type=float, required=False, help='学习率')\n",
    "parser.add_argument('--warmup_steps', default=2000, type=int, required=False, help='warm up步数')\n",
    "parser.add_argument('--log_step', default=1, type=int, required=False, help='多少步汇报一次loss')\n",
    "parser.add_argument('--gradient_accumulation', default=2, type=int, required=False, help='梯度积累')\n",
    "parser.add_argument('--max_grad_norm', default=1.0, type=float, required=False)\n",
    "parser.add_argument('--dialogue_model_output_path', default='GPT2-Summary-mindspore/summary_model/', type=str, required=False,\n",
    "                    help='对话模型输出路径')\n",
    "parser.add_argument('--pretrained_model', default='', type=str, required=False, help='预训练的GPT2模型的路径')\n",
    "parser.add_argument('--writer_dir', default='GPT2-Summary-mindspore/tensorboard_summary/', type=str, required=False, help='Tensorboard路径')\n",
    "parser.add_argument('--seed', type=int, default=None, help='设置种子用于生成随机数，以使得训练的结果是确定的')\n",
    "parser.add_argument('--num_workers', type=int, default=1, help=\"dataloader加载数据时使用的线程数量\")\n",
    "parser.add_argument('--train_mmi', default=False, help=\"若指定该参数，则训练DialoGPT的MMI模型\")\n",
    "parser.add_argument('--train_mmi_tokenized_path', default='GPT2-Summary-mindspore/data/train_mmi_tokenized.txt', type=str,\n",
    "                    required=False,\n",
    "                    help='将原始训练语料的每段对话翻转，然后进行tokenize之后的数据的存放位置，用于训练MMI模型')\n",
    "parser.add_argument('--mmi_model_output_path', default='GPT2-Summary-mindspore/mmi_model', type=str, required=False, help='MMI模型保存路径')\n",
    "# parser.add_argument('--max_len', type=int, default=60, help='每个utterance的最大长度,超过指定长度则进行截断')\n",
    "# parser.add_argument('--max_history_len', type=int, default=4, help=\"dialogue history的最大长度\")\n",
    "# return parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于暴露接口更加底层，所以是直接求导返回梯度的。因为没有直接挂载到Tensor属性的操作，因此需要单独维护一份和训练参数相同大小的参数进行累加的计算。\n",
    "\n",
    "如下代码实现了一个单独的Accumulator，其中self.inner_grads就是单独存储累加梯度的参数，直接clone一份训练参数即可。此外，还需要单独维护一个计数器，用来保证间隔accumulate_step 进行一次参数更新。\n",
    "\n",
    "在__call__函数里实现的步骤和Pytorch的实现无异，都是持续累加，达到累加步数后先更新参数，后清零已有的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.850789Z",
     "iopub.status.busy": "2023-04-19T07:56:22.850445Z",
     "iopub.status.idle": "2023-04-19T07:56:22.855460Z",
     "shell.execute_reply": "2023-04-19T07:56:22.855043Z"
    }
   },
   "outputs": [],
   "source": [
    "@mindspore.jit_class\n",
    "class Accumulator():\n",
    "    \"\"\"_summary_ 梯度更新类\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, accumulate_step, clip_norm=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.clip_norm = clip_norm\n",
    "        self.inner_grads = optimizer.parameters.clone(prefix=\"accumulate_\", init='zeros')\n",
    "        self.zeros = optimizer.parameters.clone(prefix=\"zeros_\", init='zeros')\n",
    "        self.counter = Parameter(Tensor(1, mindspore.int32), 'counter_')\n",
    "        assert accumulate_step > 0\n",
    "        self.accumulate_step = accumulate_step\n",
    "        self.map = ops.HyperMap()\n",
    "\n",
    "    def __call__(self, grads):\n",
    "        # 将单步获得的梯度累加至Accumulator的inner_grads\n",
    "        self.map(ops.partial(ops.assign_add), self.inner_grads, grads)\n",
    "        if self.counter % self.accumulate_step == 0:\n",
    "            # 如果达到累积步数，进行参数优化更新\n",
    "            self.optimizer(self.inner_grads)\n",
    "            # 完成参数优化更新后，清零inner_grads\n",
    "            self.map(ops.partial(ops.assign), self.inner_grads, self.zeros)\n",
    "        # 计算步数加一\n",
    "        ops.assign_add(self.counter, Tensor(1, mindspore.int32))\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.857507Z",
     "iopub.status.busy": "2023-04-19T07:56:22.857174Z",
     "iopub.status.idle": "2023-04-19T07:56:22.860051Z",
     "shell.execute_reply": "2023-04-19T07:56:22.859634Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_random_seed(args):\n",
    "    \"\"\"\n",
    "    设置训练的随机种子\n",
    "    \"\"\"\n",
    "    mindspore.set_seed(args.seed)\n",
    "    mindspore.set_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "# 当得到比较好的结果时我们通常希望这个结果是可以复现\n",
    "if args.seed:\n",
    "    set_random_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.862041Z",
     "iopub.status.busy": "2023-04-19T07:56:22.861784Z",
     "iopub.status.idle": "2023-04-19T07:56:22.865690Z",
     "shell.execute_reply": "2023-04-19T07:56:22.865269Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_logger(args):\n",
    "    \"\"\"\n",
    "    将日志输出到日志文件和控制台\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # 创建一个handler，用于写入日志文件\n",
    "    file_handler = logging.FileHandler(\n",
    "        filename=args.log_path)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # 创建一个handler，用于将日志输出到控制台\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.DEBUG)\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = create_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.867676Z",
     "iopub.status.busy": "2023-04-19T07:56:22.867420Z",
     "iopub.status.idle": "2023-04-19T07:56:22.871045Z",
     "shell.execute_reply": "2023-04-19T07:56:22.870627Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(args, vocab_size):\n",
    "    \"\"\"\n",
    "    :param args:\n",
    "    :param vocab_size:字典大小\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if args.pretrained_model:  # 如果指定了预训练的GPT2模型\n",
    "        model = gpt2.GPT2LMHeadModel.load(args.pretrained_model)\n",
    "        # model = gpt2.GPT2LMHeadModel.load(args.pretrained_model)\n",
    "    else:  # 若没有指定预训练模型，则初始化模型\n",
    "        # model_config = config_gpt2.GPT2Config.from_json_file(args.model_config)\n",
    "        model_config = config_gpt2.GPT2Config.from_json(args.model_config)\n",
    "        model = gpt2.GPT2LMHeadModel(config=model_config)\n",
    "    # 根据tokenizer的vocabulary调整GPT2模型的voca的大小\n",
    "    model.resize_token_embeddings(vocab_size)\n",
    "    logger.info('model config:\\n{}'.format(model.config.to_json_string()))\n",
    "    return model, model.config.to_dict().get(\"n_ctx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.873017Z",
     "iopub.status.busy": "2023-04-19T07:56:22.872748Z",
     "iopub.status.idle": "2023-04-19T07:56:33.259400Z",
     "shell.execute_reply": "2023-04-19T07:56:33.258692Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(args.vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_list = f.read().strip().split(\"\\n\")\n",
    "# 初始化tokenizer\n",
    "vocab = text.Vocab.from_list(vocab_list)\n",
    "# mindspore中Windows平台尚不支持 BertTokenizer 。\n",
    "tokenizer = BertTokenizer(vocab=vocab, lower_case=True, return_token=False)\n",
    "\n",
    "cls_token_id = vocab.tokens_to_ids('[CLS]')\n",
    "sep_token_id = vocab.tokens_to_ids('[SEP]')\n",
    "unk_token_id = vocab.tokens_to_ids('[UNK]')\n",
    "pad_id = vocab.tokens_to_ids(PAD)\n",
    "\n",
    "# tokenizer的字典大小\n",
    "vocab_size = len(vocab_list)\n",
    "# 加载GPT2模型\n",
    "model, n_ctx = create_model(args, vocab_size)\n",
    "\n",
    "# 由于单独维护了一个Accumulator，\n",
    "# 这里将optimizer作为入参统一放进了Accumulator进行计算\n",
    "accumulate_step = args.gradient_accumulation\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=args.lr)\n",
    "accumulator = Accumulator(optimizer, accumulate_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.262428Z",
     "iopub.status.busy": "2023-04-19T07:56:33.262134Z",
     "iopub.status.idle": "2023-04-19T07:56:33.268067Z",
     "shell.execute_reply": "2023-04-19T07:56:33.267578Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    计算非pad_id的平均loss和准确率\n",
    "    :param outputs:\n",
    "    :param labels:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logits = outputs[0]  # 每个token用来预测下一个token的prediction_score,维度:[batch_size,token_len,voca_size]\n",
    "    # 用前n-1个token，预测出第n个token\n",
    "    # 用第i个token的prediction_score用来预测第i+1个token。\n",
    "    # 假定有input有n个token，则shift_logits表示model中第[0,n-2]个token的prediction_score，shift_labels表示第[1，n-1]的label\n",
    "    shift_logits = logits[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    shift_labels = shift_labels.astype(mindspore.int32)\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=pad_id, reduction='sum')  # 忽略pad_id的loss,并对所有的非pad_id的loss进行求和\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.shape[-1]),\n",
    "                    shift_labels.view(-1))\n",
    "    \n",
    "    preds = shift_logits.argmax(axis=-1) # preds表示对应的prediction_score预测出的token在voca中的id。维度为[batch_size,token_len]\n",
    "\n",
    "    # 对非pad_id的token的loss进行求平均，且计算出预测的准确率\n",
    "    # not_ignore = shift_labels != pad_id\n",
    "    not_ignore = shift_labels.ne(pad_id)  # 进行非运算，返回一个tensor，若targets_view的第i个位置为pad_id，则置为0，否则为1\n",
    "    num_targets = not_ignore.astype(mindspore.int64).sum()  # 计算target中的非pad_id的数量\n",
    "\n",
    "    correct = (shift_labels == preds) & not_ignore  # 计算model预测正确的token的个数，排除pad的tokne\n",
    "    correct = correct.float().sum()\n",
    "\n",
    "    accuracy = correct / num_targets\n",
    "    loss = loss / num_targets\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.270280Z",
     "iopub.status.busy": "2023-04-19T07:56:33.269923Z",
     "iopub.status.idle": "2023-04-19T07:56:33.274862Z",
     "shell.execute_reply": "2023-04-19T07:56:33.274397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define forward function\n",
    "def forward_fn(data, label):\n",
    "    \"\"\"_summary_ 前向推理步骤\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        label (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    logits = model(data)\n",
    "    loss, accuracy = calculate_loss_and_accuracy(logits, label)\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "# @mindspore.jit\n",
    "def train_step(data, label):\n",
    "    \"\"\"_summary_ 训练步骤\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        label (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = ops.depend(loss, accumulator(grads))\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下两个函数用于对原始语料进行处理的。\n",
    "第一个preprocess_raw_data函数是根据获取到的文本内容，将其转换为相应的token id。\n",
    "第二个preprocess_mmi_raw_data函数是训练MMI模型，转换的形式与第一个相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.277011Z",
     "iopub.status.busy": "2023-04-19T07:56:33.276824Z",
     "iopub.status.idle": "2023-04-19T07:56:33.282842Z",
     "shell.execute_reply": "2023-04-19T07:56:33.282375Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_raw_data(args, n_ctx):\n",
    "    \"\"\"\n",
    "    对原始语料进行处理，将原始语料转换为用于train的token id，对于每个dialogue，将其处于成如下形式\"[CLS]utterance1[SEP]utterance2[SEP]utterance3[SEP]\"\n",
    "    :param args:\n",
    "    :param tokenizer:\n",
    "    :param n_ctx:GPT2模型的上下文窗口大小,对于超过n_ctx(n_ctx包括了特殊字符)的dialogue进行截断\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logger.info(\"tokenizing raw data,raw data path:{}, token output path:{}\".format(args.train_raw_path,\n",
    "                                                                                    args.train_tokenized_path))\n",
    "\n",
    "    with open(args.train_tokenized_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        with open(args.train_raw_path, 'r',encoding=\"utf-8\") as file:\n",
    "            for line in tqdm(file.readlines()):\n",
    "                try:\n",
    "                    file_line = json.loads(line)\n",
    "                except:\n",
    "                    print (\"line\",line)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    dialogue_ids = [cls_token_id]\n",
    "                    dialogue_ids.extend([vocab.tokens_to_ids(word) \n",
    "                                         if vocab.tokens_to_ids(word) != -1 \n",
    "                                         else unk_token_id \n",
    "                                         for word in file_line['article']])\n",
    "                    dialogue_ids.append(sep_token_id)  # 每个utterance之后添加[SEP]，表示utterance结束\n",
    "                    dialogue_ids.extend([vocab.tokens_to_ids(word) \n",
    "                                         if vocab.tokens_to_ids(word) != -1 \n",
    "                                         else unk_token_id \n",
    "                                         for word in file_line['summarization']])\n",
    "                    dialogue_ids.append(sep_token_id)  # 每个utterance之后添加[SEP]，表示utterance结束\n",
    "                    # 对超过n_ctx的长度进行截断,否则GPT2模型会报错\n",
    "                    dialogue_ids = dialogue_ids[:n_ctx]\n",
    "                    for dialogue_id in dialogue_ids:\n",
    "                        f.write(str(dialogue_id) + ' ')\n",
    "                    f.write(\"\\n\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.284934Z",
     "iopub.status.busy": "2023-04-19T07:56:33.284750Z",
     "iopub.status.idle": "2023-04-19T07:56:33.290982Z",
     "shell.execute_reply": "2023-04-19T07:56:33.290524Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_mmi_raw_data(args, n_ctx):\n",
    "    \"\"\"\n",
    "    对原始语料进行处理，将原始语料的每段对话进行翻转，然后转换为用于train MMI模型的token id，对于每个dialogue，将其处于成如下形式\"[CLS]utterance N[SEP]utterance N-1[SEP]utterance N-2[SEP]\"\n",
    "    :param args:\n",
    "    :param tokenizer:\n",
    "    :param n_ctx:GPT2模型的上下文窗口大小,对于超过n_ctx(n_ctx包括了特殊字符)的dialogue进行截断\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logger.info(\"tokenizing MMI raw data,raw data path:{}, token output path:{}\".format(args.train_raw_path,\n",
    "                                                                                        args.train_mmi_tokenized_path))\n",
    "    with open(args.train_raw_path, 'rb') as f:\n",
    "        data = f.read().decode(\"utf-8\")\n",
    "    if \"\\r\\n\" in data:\n",
    "        train_data = data.split(\"\\r\\n\\r\\n\")\n",
    "    else:\n",
    "        train_data = data.split(\"\\n\\n\")\n",
    "    logger.info(\"there are {} dialogue in raw dataset\".format(len(train_data)))\n",
    "    with open(args.train_mmi_tokenized_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for dialogue_index, dialogue in enumerate(tqdm(train_data)):\n",
    "            if \"\\r\\n\" in data:\n",
    "                utterances = dialogue.split(\"\\r\\n\")\n",
    "            else:\n",
    "                utterances = dialogue.split(\"\\n\")\n",
    "            dialogue_ids = [cls_token_id]  # 每个dialogue以[CLS]开头\n",
    "            for utterance in reversed(utterances):  # 将一段对话进行翻转\n",
    "                dialogue_ids.extend([vocab.tokens_to_ids(word) for word in utterance])\n",
    "                dialogue_ids.append(sep_token_id)  # 每个utterance之后添加[SEP]，表示utterance结束\n",
    "            # 对超过n_ctx的长度进行截断,否则GPT2模型会报错\n",
    "            dialogue_ids = dialogue_ids[:n_ctx]\n",
    "            for dialogue_id in dialogue_ids:\n",
    "                f.write(str(dialogue_id) + ' ')\n",
    "            # 最后一条记录不添加换行符\n",
    "            if dialogue_index < len(train_data) - 1:\n",
    "                f.write(\"\\n\")\n",
    "    logger.info(\"finish preprocessing raw data,the result is stored in {}\".format(args.train_tokenized_path))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理原始语料并加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.293082Z",
     "iopub.status.busy": "2023-04-19T07:56:33.292805Z",
     "iopub.status.idle": "2023-04-19T08:26:49.525134Z",
     "shell.execute_reply": "2023-04-19T08:26:49.524346Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建对话模型的输出目录\n",
    "if not os.path.exists(args.dialogue_model_output_path):\n",
    "    os.mkdir(args.dialogue_model_output_path)\n",
    "# 创建MMI模型的输出目录\n",
    "if not os.path.exists(args.mmi_model_output_path):\n",
    "    os.mkdir(args.mmi_model_output_path)\n",
    "\n",
    "# 对原始数据进行预处理,将原始语料转换成对应的token_id\n",
    "if args.raw and args.train_mmi:  # 如果当前是要训练MMI模型\n",
    "    preprocess_mmi_raw_data(args, n_ctx)\n",
    "elif args.raw and not args.train_mmi:  # 如果当前是要训练对话生成模型\n",
    "    print (\"_______________________________________\")\n",
    "    preprocess_raw_data(args, n_ctx)\n",
    "    \n",
    "# 是否使用多块GPU进行并行运算\n",
    "multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.528223Z",
     "iopub.status.busy": "2023-04-19T08:26:49.527795Z",
     "iopub.status.idle": "2023-04-19T08:26:49.535798Z",
     "shell.execute_reply": "2023-04-19T08:26:49.535309Z"
    }
   },
   "outputs": [],
   "source": [
    "# 记录模型参数数量\n",
    "num_parameters = 0\n",
    "parameters = model.trainable_params()\n",
    "for parameter in parameters:\n",
    "    num_parameters += parameter.numel()\n",
    "logger.info('number of model parameters: {}'.format(num_parameters))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型参数，并对wte进行处理成能识别的训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.538389Z",
     "iopub.status.busy": "2023-04-19T08:26:49.537855Z",
     "iopub.status.idle": "2023-04-19T08:26:49.543223Z",
     "shell.execute_reply": "2023-04-19T08:26:49.542757Z"
    }
   },
   "outputs": [],
   "source": [
    "def ckpt_to_mindspore(mth_file, size:str=None):\n",
    "    \"\"\"_summary_ \n",
    "    Resolve the parameter wte.embedding of the generative model,\n",
    "    the lack of training transformer prefix in table allows for loading\n",
    "\n",
    "    Args:\n",
    "        mth_file (_type_): _description_\n",
    "        size (str, optional): _description_. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: _description_\n",
    "        RuntimeError: _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mindspore\n",
    "    except:\n",
    "        raise ImportError(f\"'import mindspore' failed, please install mindspore by \"\n",
    "                          f\"`pip mindspore torch` or instructions from 'https://www.mindspore.cn/install'\")\n",
    "\n",
    "    size = \"mindspore\" if not size else size # rename ckpt\n",
    "\n",
    "    from mindspore import Tensor\n",
    "    from mindspore.train.serialization import save_checkpoint\n",
    "\n",
    "    logging.info('Starting checkpoint conversion.')\n",
    "    ms_ckpt = []\n",
    "    state_dict = mindspore.load_checkpoint(mth_file)\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if 'wte.embedding_table' in k:\n",
    "            k = k.replace('wte.embedding_table', 'transformer.wte.embedding_table')\n",
    "        ms_ckpt.append({'name': k, 'data': Tensor(v.numpy())})\n",
    "\n",
    "    try:\n",
    "        save_checkpoint(ms_ckpt, mth_file)\n",
    "    except:\n",
    "        raise RuntimeError(f'Save checkpoint to {mth_file} failed, please checkout the path.')\n",
    "\n",
    "    return mth_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.545590Z",
     "iopub.status.busy": "2023-04-19T08:26:49.545378Z",
     "iopub.status.idle": "2023-04-19T08:26:49.554940Z",
     "shell.execute_reply": "2023-04-19T08:26:49.554449Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_list, multi_gpu, args):\n",
    "    \"\"\"_summary_ 训练逻辑，进行数据加载之后的推理和保存模型\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        train_list (_type_): _description_\n",
    "        multi_gpu (_type_): _description_\n",
    "        args (_type_): _description_\n",
    "\n",
    "    Raises:\n",
    "        exception: _description_\n",
    "    \"\"\"\n",
    "    train_dataset = MyDataset(train_list)\n",
    "    train_dataloader = ds.GeneratorDataset(train_dataset, column_names=\"input_ids\" ,num_parallel_workers=args.num_workers, shuffle=True)\n",
    "    train_dataloader = train_dataloader.padded_batch(batch_size=args.batch_size, drop_remainder=True, pad_info={})\n",
    "    train_dataloader = train_dataloader.repeat(1)\n",
    "    train_dataloader = train_dataloader.shuffle(10)\n",
    "    \n",
    "    # 计算所有epoch进行参数优化的总步数total_steps\n",
    "    total_steps = int(train_dataset.__len__() * args.epochs / args.batch_size / args.gradient_accumulation)\n",
    "    logger.info('total training steps = {}'.format(total_steps))\n",
    "\n",
    "    logger.info('starting training')\n",
    "    # 记录 out of memory的次数\n",
    "    oom_time = 0\n",
    "    # 开始训练\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_start_time = datetime.now()\n",
    "        size = len(train_dataloader)\n",
    "        # batch_idx为int，input_ids为一个tensor\n",
    "        for batch_idx, input_ids in enumerate(train_dataloader.create_tuple_iterator()):\n",
    "            # 注意：GPT2模型的construct()函数，是对于给定的context，生成一个token，而不是生成一串token\n",
    "            # GPT2Model的输入为n个token_id时，输出也是n个hidden_state，使用第n个hidden_state预测第n+1个token\n",
    "            # 解决在运行过程中，由于显存不足产生的cuda out of memory的问题\n",
    "            try:\n",
    "                input_id = input_ids[0].astype(mindspore.int64)\n",
    "                loss = train_step(input_id, input_id)\n",
    "                \n",
    "                if batch_idx % args.log_step == 0:\n",
    "                    loss, current = loss.asnumpy(), batch_idx\n",
    "                    logger.info(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "            except RuntimeError as exception:\n",
    "                if \"out of memory\" in str(exception):\n",
    "                    oom_time += 1\n",
    "                    logger.info(\"WARNING: ran out of memory,times: {}\".format(oom_time))    \n",
    "                else:\n",
    "                    logger.info(str(exception))\n",
    "                    raise exception\n",
    "        logger.info('saving model for epoch {}'.format(epoch + 1))\n",
    "        if args.train_mmi:  # 当前训练MMI模型\n",
    "            model_path = join(args.mmi_model_output_path, 'model_epoch{}'.format(epoch + 1))\n",
    "        else:  # 当前训练对话模型\n",
    "            model_path = join(args.dialogue_model_output_path, 'model_epoch{}'.format(epoch + 1))\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_path = f\"{model_path}/mindspore_model.ckpt\"\n",
    "        save_checkpoint(model_to_save, model_path)\n",
    "        ckpt_to_mindspore(model_path)\n",
    "        logger.info('epoch {} finished'.format(epoch + 1))\n",
    "        epoch_finish_time = datetime.now()\n",
    "        logger.info('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n",
    "    logger.info('training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.557325Z",
     "iopub.status.busy": "2023-04-19T08:26:49.556863Z",
     "iopub.status.idle": "2023-04-19T08:26:50.029863Z",
     "shell.execute_reply": "2023-04-19T08:26:50.029250Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "logger.info(\"loading traing data\")\n",
    "if args.train_mmi:  # 如果是训练MMI模型\n",
    "    with open(args.train_mmi_tokenized_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "else:  # 如果是训练对话生成模型\n",
    "    with open(args.train_tokenized_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "data_list = [line.rstrip() for line in data.split(\"\\n\") if line.rstrip()]\n",
    "train_list = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:50.032628Z",
     "iopub.status.busy": "2023-04-19T08:26:50.032311Z",
     "iopub.status.idle": "2023-04-19T22:22:05.182820Z",
     "shell.execute_reply": "2023-04-19T22:22:05.182184Z"
    }
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "train(model, train_list, multi_gpu, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cjh1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
