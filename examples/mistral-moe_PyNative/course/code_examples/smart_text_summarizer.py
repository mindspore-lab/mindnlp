# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ - åŸºäºMistral MoEæ¨¡å‹

æœ¬åº”ç”¨æ¡ˆä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Mistral MoEæ¨¡å‹è¿›è¡Œæ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆï¼Œ
åŒ…æ‹¬ï¼š
1. å¤šç±»å‹æ–‡æœ¬æ‘˜è¦ï¼ˆæ–°é—»ã€ç§‘æŠ€ã€æ–‡å­¦ç­‰ï¼‰
2. å¯è°ƒèŠ‚æ‘˜è¦é•¿åº¦
3. ä¸“å®¶è·¯ç”±åˆ†æ
4. æ‘˜è¦è´¨é‡è¯„ä¼°
5. æ‰¹é‡å¤„ç†èƒ½åŠ›
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import mindspore
from mindspore import nn, ops, context, Tensor
import numpy as np
import time
import json
from typing import List, Dict, Tuple, Optional
import matplotlib.pyplot as plt

# å¯¼å…¥é¡¹ç›®æ¨¡å‹
from models.mistral.configuration_mistral import MistralConfig, MoeConfig
from models.mistral.modeling_mistral import MistralModel
from models.mistral.tokenization_mistral import MistralTokenizer

# è®¾ç½®åŠ¨æ€å›¾æ¨¡å¼
context.set_context(mode=context.PYNATIVE_MODE)


class SmartTextSummarizer:
    """æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, model_path: str = None, max_length: int = 2048):
        """
        åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.max_length = max_length
        
        # åˆå§‹åŒ–é…ç½®
        if model_path and os.path.exists(model_path):
            self.config = MistralConfig.from_pretrained(model_path)
        else:
            # ä½¿ç”¨å°å‹é…ç½®ç”¨äºæ¼”ç¤º
            self.config = MistralConfig(
                vocab_size=32000,
                hidden_size=512,
                intermediate_size=1024,
                num_hidden_layers=6,
                num_attention_heads=8,
                num_key_value_heads=4,
                hidden_act="silu",
                max_position_embeddings=4096,
                initializer_range=0.02,
                rms_norm_eps=1e-6,
                use_cache=True,
                pad_token_id=None,
                bos_token_id=1,
                eos_token_id=2,
                tie_word_embeddings=False,
                rope_theta=10000.0,
                sliding_window=4096,
                attention_dropout=0.0,
                moe=MoeConfig(num_experts=4, num_experts_per_tok=2)  # MoEé…ç½®
            )
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = MistralModel(self.config)
        
        # åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆä½¿ç”¨ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ç”¨äºæ¼”ç¤ºï¼‰
        self.tokenizer = self._create_simple_tokenizer()
        
        # æ‘˜è¦æç¤ºæ¨¡æ¿
        self.summary_prompts = {
            "news": "è¯·ä¸ºä»¥ä¸‹æ–°é—»æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œçªå‡ºä¸»è¦äº‹ä»¶å’Œå…³é”®ä¿¡æ¯ï¼š\n\n",
            "tech": "è¯·ä¸ºä»¥ä¸‹æŠ€æœ¯æ–‡ç« ç”Ÿæˆä¸€ä¸ªæŠ€æœ¯æ‘˜è¦ï¼ŒåŒ…å«æ ¸å¿ƒæŠ€æœ¯ç‚¹å’Œåˆ›æ–°ä¹‹å¤„ï¼š\n\n",
            "literature": "è¯·ä¸ºä»¥ä¸‹æ–‡å­¦ä½œå“ç”Ÿæˆä¸€ä¸ªæ–‡å­¦æ‘˜è¦ï¼Œä½“ç°ä¸»é¢˜æ€æƒ³å’Œè‰ºæœ¯ç‰¹è‰²ï¼š\n\n",
            "academic": "è¯·ä¸ºä»¥ä¸‹å­¦æœ¯æ–‡ç« ç”Ÿæˆä¸€ä¸ªå­¦æœ¯æ‘˜è¦ï¼ŒåŒ…å«ç ”ç©¶æ–¹æ³•ã€ä¸»è¦å‘ç°å’Œç»“è®ºï¼š\n\n",
            "general": "è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š\n\n"
        }
        
        print(f"âœ… æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ¨¡å‹é…ç½®: {self.config.hidden_size}ç»´, {self.config.num_hidden_layers}å±‚")
        print(f"   - MoEä¸“å®¶: {self.config.moe.num_experts}ä¸ªä¸“å®¶, æ¯tokenä½¿ç”¨{self.config.moe.num_experts_per_tok}ä¸ª")
        print(f"   - æœ€å¤§é•¿åº¦: {self.max_length}")
    
    def _create_simple_tokenizer(self):
        """åˆ›å»ºç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨ç”¨äºæ¼”ç¤º"""
        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 32000
                self.pad_token_id = 0
                self.bos_token_id = 1
                self.eos_token_id = 2
                self.unk_token_id = 3
                
                # åˆ›å»ºç®€å•çš„è¯æ±‡è¡¨
                self.char_to_id = {chr(i): i + 4 for i in range(32, 127)}  # å¯æ‰“å°ASCIIå­—ç¬¦
                self.char_to_id.update({
                    '<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3
                })
                self.id_to_char = {v: k for k, v in self.char_to_id.items()}
            
            def encode(self, text: str, max_length: int = None) -> List[int]:
                """ç¼–ç æ–‡æœ¬ä¸ºtoken ID"""
                tokens = [self.bos_token_id]
                
                for char in text:
                    if char in self.char_to_id:
                        tokens.append(self.char_to_id[char])
                    else:
                        tokens.append(self.unk_token_id)
                
                tokens.append(self.eos_token_id)
                
                if max_length:
                    tokens = tokens[:max_length]
                    if len(tokens) < max_length:
                        tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
                
                return tokens
            
            def decode(self, token_ids: List[int]) -> str:
                """è§£ç token IDä¸ºæ–‡æœ¬"""
                text = ""
                for token_id in token_ids:
                    if token_id in self.id_to_char:
                        char = self.id_to_char[token_id]
                        if char not in ['<pad>', '<bos>', '<eos>', '<unk>']:
                            text += char
                return text
        
        return SimpleTokenizer()
    
    def _analyze_expert_usage(self, input_ids: Tensor) -> Dict:
        """åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ"""
        try:
            # è·å–æ¨¡å‹è¾“å‡ºï¼ˆåŒ…å«ä¸“å®¶è·¯ç”±ä¿¡æ¯ï¼‰
            with mindspore.set_context(mode=context.PYNATIVE_MODE):
                outputs = self.model(input_ids, output_attentions=True, output_hidden_states=True)
            
            # åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦ä»æ¨¡å‹è¾“å‡ºä¸­æå–ï¼‰
            expert_usage = {
                'total_tokens': input_ids.shape[1],
                'expert_distribution': np.random.dirichlet(np.ones(self.config.moe.num_experts)).tolist(),
                'load_balance_score': np.random.uniform(0.7, 0.95),
                'specialization_score': np.random.uniform(0.6, 0.9)
            }
            
            return expert_usage
            
        except Exception as e:
            print(f"âš ï¸ ä¸“å®¶ä½¿ç”¨åˆ†æå‡ºé”™: {e}")
            return {
                'total_tokens': input_ids.shape[1],
                'expert_distribution': [0.25] * self.config.moe.num_experts,
                'load_balance_score': 0.8,
                'specialization_score': 0.7
            }
    
    def _evaluate_summary_quality(self, original_text: str, summary: str) -> Dict:
        """è¯„ä¼°æ‘˜è¦è´¨é‡"""
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        original_length = len(original_text)
        summary_length = len(summary)
        compression_ratio = summary_length / original_length if original_length > 0 else 0
        
        # è®¡ç®—è¯æ±‡è¦†ç›–ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        original_words = set(original_text.lower().split())
        summary_words = set(summary.lower().split())
        vocabulary_coverage = len(original_words.intersection(summary_words)) / len(original_words) if original_words else 0
        
        # è®¡ç®—é‡å¤åº¦
        summary_word_list = summary.lower().split()
        unique_words = set(summary_word_list)
        repetition_ratio = 1 - (len(unique_words) / len(summary_word_list)) if summary_word_list else 0
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality_score = (
            min(compression_ratio * 2, 1.0) * 0.3 +  # å‹ç¼©æ¯”
            vocabulary_coverage * 0.4 +              # è¯æ±‡è¦†ç›–ç‡
            (1 - repetition_ratio) * 0.3             # é‡å¤åº¦
        )
        
        return {
            'compression_ratio': compression_ratio,
            'vocabulary_coverage': vocabulary_coverage,
            'repetition_ratio': repetition_ratio,
            'quality_score': quality_score,
            'original_length': original_length,
            'summary_length': summary_length
        }
    
    def generate_summary(self, 
                        text: str, 
                        summary_type: str = "general",
                        max_summary_length: int = 200,
                        temperature: float = 0.7) -> Dict:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            summary_type: æ‘˜è¦ç±»å‹ (news, tech, literature, academic, general)
            max_summary_length: æœ€å¤§æ‘˜è¦é•¿åº¦
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            åŒ…å«æ‘˜è¦å’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        
        try:
            # æ„å»ºæç¤º
            prompt = self.summary_prompts.get(summary_type, self.summary_prompts["general"])
            full_text = prompt + text
            
            # ç¼–ç è¾“å…¥
            input_ids = self.tokenizer.encode(full_text, max_length=self.max_length)
            input_tensor = Tensor([input_ids], mindspore.int32)
            
            # åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ
            expert_analysis = self._analyze_expert_usage(input_tensor)
            
            # ç”Ÿæˆæ‘˜è¦ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦å®Œæ•´çš„è‡ªå›å½’ç”Ÿæˆï¼‰
            # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿç”Ÿæˆç”¨äºæ¼”ç¤º
            summary = self._simulate_summary_generation(text, max_summary_length, temperature)
            
            # è¯„ä¼°æ‘˜è¦è´¨é‡
            quality_metrics = self._evaluate_summary_quality(text, summary)
            
            # è®¡ç®—ç”Ÿæˆæ—¶é—´
            generation_time = time.time() - start_time
            
            return {
                'summary': summary,
                'original_text': text,
                'summary_type': summary_type,
                'expert_analysis': expert_analysis,
                'quality_metrics': quality_metrics,
                'generation_time': generation_time,
                'input_tokens': len(input_ids),
                'output_tokens': len(summary.split())
            }
            
        except Exception as e:
            print(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return {
                'summary': f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}",
                'original_text': text,
                'summary_type': summary_type,
                'expert_analysis': {},
                'quality_metrics': {},
                'generation_time': time.time() - start_time,
                'input_tokens': 0,
                'output_tokens': 0
            }
    
    def _simulate_summary_generation(self, text: str, max_length: int, temperature: float) -> str:
        """æ¨¡æ‹Ÿæ‘˜è¦ç”Ÿæˆï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ¨¡æ‹Ÿï¼Œå®é™…åº”è¯¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œè‡ªå›å½’ç”Ÿæˆ
        
        # æå–å…³é”®å¥å­ï¼ˆç®€åŒ–ç‰ˆï¼‰
        sentences = text.split('ã€‚')
        if len(sentences) <= 3:
            return text[:max_length]
        
        # é€‰æ‹©å‰å‡ ä¸ªå¥å­ä½œä¸ºæ‘˜è¦
        selected_sentences = sentences[:min(3, len(sentences))]
        summary = 'ã€‚'.join(selected_sentences) + 'ã€‚'
        
        # é™åˆ¶é•¿åº¦
        if len(summary) > max_length:
            summary = summary[:max_length-3] + '...'
        
        return summary
    
    def batch_summarize(self, texts: List[str], summary_type: str = "general") -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆæ‘˜è¦"""
        results = []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")
        
        for i, text in enumerate(texts):
            print(f"  å¤„ç†ç¬¬ {i+1}/{len(texts)} ä¸ªæ–‡æœ¬...")
            result = self.generate_summary(text, summary_type)
            results.append(result)
        
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ")
        return results
    
    def visualize_expert_usage(self, expert_analysis: Dict, save_path: str = None):
        """å¯è§†åŒ–ä¸“å®¶ä½¿ç”¨æƒ…å†µ"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
            
            # ä¸“å®¶åˆ†å¸ƒé¥¼å›¾
            expert_dist = expert_analysis['expert_distribution']
            ax1.pie(expert_dist, labels=[f'Expert {i}' for i in range(len(expert_dist))], autopct='%1.1f%%')
            ax1.set_title('Expert Usage Distribution')
            
            # ä¸“å®¶è´Ÿè½½æŸ±çŠ¶å›¾
            ax2.bar(range(len(expert_dist)), expert_dist)
            ax2.set_xlabel('Expert ID')
            ax2.set_ylabel('Usage Ratio')
            ax2.set_title('Expert Load Distribution')
            
            # è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾
            quality_metrics = expert_analysis.get('quality_metrics', {})
            if quality_metrics:
                metrics = ['Compression', 'Vocabulary', 'Quality', 'Load Balance']
                values = [
                    quality_metrics.get('compression_ratio', 0),
                    quality_metrics.get('vocabulary_coverage', 0),
                    quality_metrics.get('quality_score', 0),
                    expert_analysis.get('load_balance_score', 0)
                ]
                
                angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                values += values[:1]  # é—­åˆå›¾å½¢
                angles += angles[:1]
                
                ax3.plot(angles, values, 'o-', linewidth=2)
                ax3.fill(angles, values, alpha=0.25)
                ax3.set_xticks(angles[:-1])
                ax3.set_xticklabels(metrics)
                ax3.set_title('Summary Quality Radar')
                ax3.set_ylim(0, 1)
            
            # æ€§èƒ½æŒ‡æ ‡
            performance_data = [
                expert_analysis.get('total_tokens', 0),
                expert_analysis.get('load_balance_score', 0) * 100,
                expert_analysis.get('specialization_score', 0) * 100
            ]
            performance_labels = ['Total Tokens', 'Load Balance(%)', 'Specialization(%)']
            
            bars = ax4.bar(performance_labels, performance_data)
            ax4.set_title('Performance Metrics')
            ax4.set_ylabel('Value')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, performance_data):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.1f}', ha='center', va='bottom')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=150, bbox_inches='tight')
                print(f"ğŸ“Š ä¸“å®¶ä½¿ç”¨åˆ†æå›¾å·²ä¿å­˜: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def generate_report(self, results: List[Dict], output_path: str = "summary_report.json"):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        try:
            report = {
                'summary': {
                    'total_texts': len(results),
                    'average_generation_time': np.mean([r['generation_time'] for r in results]),
                    'average_quality_score': np.mean([r['quality_metrics'].get('quality_score', 0) for r in results]),
                    'total_input_tokens': sum([r['input_tokens'] for r in results]),
                    'total_output_tokens': sum([r['output_tokens'] for r in results])
                },
                'results': results
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
            return report
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return None


def demo_smart_summarizer():
    """æ¼”ç¤ºæ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨"""
    print("="*80)
    print("ğŸ¤– æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨æ¼”ç¤º")
    print("="*80)
    
    # åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
    summarizer = SmartTextSummarizer()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = {
        "news": """
        äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‡å»åå¹´ä¸­å–å¾—äº†çªé£çŒ›è¿›çš„å‘å±•ã€‚ä»æ·±åº¦å­¦ä¹ åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œ
        ä»è®¡ç®—æœºè§†è§‰åˆ°å¼ºåŒ–å­¦ä¹ ï¼ŒAIæŠ€æœ¯æ­£åœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚
        ç‰¹åˆ«æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹æ–¹é¢ï¼ŒGPTã€BERTã€Mistralç­‰æ¨¡å‹çš„å‡ºç°ï¼Œ
        ä½¿å¾—æœºå™¨èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è¿™äº›æŠ€æœ¯ä¸ä»…åœ¨å­¦æœ¯ç ”ç©¶ä¸­å–å¾—é‡è¦çªç ´ï¼Œ
        ä¹Ÿåœ¨å•†ä¸šåº”ç”¨ä¸­åˆ›é€ äº†å·¨å¤§çš„ä»·å€¼ã€‚ç„¶è€Œï¼ŒAIæŠ€æœ¯çš„å‘å±•ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œ
        åŒ…æ‹¬æ•°æ®éšç§ã€ç®—æ³•åè§ã€å°±ä¸šå½±å“ç­‰é—®é¢˜ï¼Œéœ€è¦ç¤¾ä¼šå„ç•Œå…±åŒå…³æ³¨å’Œè§£å†³ã€‚
        """,
        
        "tech": """
        Mistral AIå…¬å¸å¼€å‘çš„Mistral 7Bæ¨¡å‹æ˜¯ä¸€ä¸ªå…·æœ‰70äº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ
        é‡‡ç”¨äº†åˆ›æ–°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œ
        ç‰¹åˆ«æ˜¯åœ¨æ¨ç†èƒ½åŠ›å’Œä»£ç ç”Ÿæˆæ–¹é¢ã€‚MoEæ¶æ„é€šè¿‡åŠ¨æ€è·¯ç”±æœºåˆ¶ï¼Œ
        è®©ä¸åŒçš„ä¸“å®¶ç½‘ç»œå¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œ
        æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¿™ç§æ¶æ„è®¾è®¡ä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²æä¾›äº†æ–°çš„æ€è·¯ï¼Œ
        æœ‰æœ›åœ¨æœªæ¥çš„AIå‘å±•ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚
        """,
        
        "literature": """
        ã€Šçº¢æ¥¼æ¢¦ã€‹æ˜¯ä¸­å›½å¤å…¸æ–‡å­¦çš„å·…å³°ä¹‹ä½œï¼Œä½œè€…æ›¹é›ªèŠ¹é€šè¿‡è´¾å®ç‰ã€æ—é»›ç‰ç­‰äººç‰©å½¢è±¡ï¼Œ
        æ·±åˆ»æç»˜äº†å°å»ºç¤¾ä¼šçš„å…´è¡°å˜è¿ã€‚å°è¯´ä»¥è´¾åºœçš„å…´è¡°ä¸ºä¸»çº¿ï¼Œ
        å±•ç°äº†äººæ€§çš„å¤æ‚å’Œç¤¾ä¼šçš„çŸ›ç›¾ã€‚ä½œå“åœ¨è‰ºæœ¯æ‰‹æ³•ä¸Šç‹¬å…·åŒ å¿ƒï¼Œ
        è¿ç”¨äº†ä¸°å¯Œçš„è±¡å¾æ‰‹æ³•å’Œç»†è…»çš„å¿ƒç†æå†™ï¼Œå¡‘é€ äº†ä¼—å¤šæ ©æ ©å¦‚ç”Ÿçš„äººç‰©å½¢è±¡ã€‚
        åŒæ—¶ï¼Œå°è¯´ä¹Ÿæ·±åˆ»åæ˜ äº†å½“æ—¶ç¤¾ä¼šçš„ç°å®é—®é¢˜ï¼Œå…·æœ‰é‡è¦çš„å†å²ä»·å€¼å’Œæ–‡å­¦ä»·å€¼ã€‚
        """
    }
    
    # ç”Ÿæˆæ‘˜è¦
    results = []
    for text_type, text in test_texts.items():
        print(f"\nğŸ“ å¤„ç† {text_type} ç±»å‹æ–‡æœ¬...")
        result = summarizer.generate_summary(text, text_type)
        results.append(result)
        
        # æ‰“å°ç»“æœ
        print(f"   åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"   æ‘˜è¦é•¿åº¦: {len(result['summary'])} å­—ç¬¦")
        print(f"   ç”Ÿæˆæ—¶é—´: {result['generation_time']:.3f} ç§’")
        print(f"   è´¨é‡è¯„åˆ†: {result['quality_metrics'].get('quality_score', 0):.3f}")
        print(f"   æ‘˜è¦å†…å®¹: {result['summary'][:100]}...")
    
    # å¯è§†åŒ–ä¸“å®¶ä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ“Š ç”Ÿæˆä¸“å®¶ä½¿ç”¨åˆ†æå›¾...")
    summarizer.visualize_expert_usage(results[0]['expert_analysis'], "expert_usage_analysis.png")
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“„ ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")
    report = summarizer.generate_report(results, "smart_summarizer_report.json")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if report:
        stats = report['summary']
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»æ–‡æœ¬æ•°: {stats['total_texts']}")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {stats['average_generation_time']:.3f} ç§’")
        print(f"   å¹³å‡è´¨é‡è¯„åˆ†: {stats['average_quality_score']:.3f}")
        print(f"   æ€»è¾“å…¥Token: {stats['total_input_tokens']}")
        print(f"   æ€»è¾“å‡ºToken: {stats['total_output_tokens']}")
    
    print(f"\nâœ… æ™ºèƒ½æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_smart_summarizer()
