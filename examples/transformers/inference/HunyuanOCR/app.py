import os
import numpy as np
from PIL import Image
import mindtorch
import mindhf
from transformers import AutoProcessor
from qwen_vl_utils import process_vision_info  # è¯·ç¡®ä¿è¯¥æ¨¡å—åœ¨ä½ çš„ç¯å¢ƒå¯ç”¨
from transformers import AutoModel
import gradio as gr
from argparse import ArgumentParser
import copy
import requests
from io import BytesIO
import tempfile
import hashlib
import gc

# å…³é”®ä¼˜åŒ–ï¼šè®¾ç½®ç¯å¢ƒå˜é‡åŠ é€Ÿ transformers
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…tokenizerè­¦å‘Š
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"


def _get_args():
    parser = ArgumentParser()

    parser.add_argument(
        "-c",
        "--checkpoint-path",
        type=str,
        default="lvyufeng/HunyuanOCR",
        help="Checkpoint name or path, default to %(default)r",
    )
    parser.add_argument(
        "--cpu-only", action="store_true", help="Run demo with CPU only"
    )

    parser.add_argument(
        "--flash-attn2",
        action="store_true",
        default=False,
        help="Enable flash_attention_2 when loading the model.",
    )
    parser.add_argument(
        "--share",
        action="store_true",
        default=False,
        help="Create a publicly shareable link for the interface.",
    )
    parser.add_argument(
        "--inbrowser",
        action="store_true",
        default=False,
        help="Automatically launch the interface in a new tab on the default browser.",
    )

    args = parser.parse_args()
    return args


def _load_model_processor(args):
    print(f"[INFO] åŠ è½½æ¨¡å‹ï¼ˆeager æ¨¡å¼ï¼‰")

    model = AutoModel.from_pretrained(
        args.checkpoint_path,
        attn_implementation="eager",
        torch_dtype=mindtorch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )

    # å…³é”®ï¼šç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå¯ç”¨ä¼šå¯¼è‡´ææ…¢ï¼‰
    if hasattr(model, "gradient_checkpointing_disable"):
        model.gradient_checkpointing_disable()
        print(f"[INFO] æ¢¯åº¦æ£€æŸ¥ç‚¹å·²ç¦ç”¨")

    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    print(f"[INFO] æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")

    processor = AutoProcessor.from_pretrained(
        args.checkpoint_path, use_fast=False, trust_remote_code=True
    )

    print(f"[INFO] æ¨¡å‹åŠ è½½å®Œæˆï¼Œå½“å‰è®¾å¤‡: {next(model.parameters()).device}")
    return model, processor


def _parse_text(text):
    """è§£ææ–‡æœ¬ï¼Œå¤„ç†ç‰¹æ®Šæ ¼å¼"""
    # if text is None:
    #     return text
    text = text.replace("<trans>", "").replace("</trans>", "")
    return text


def _remove_image_special(text):
    """ç§»é™¤å›¾åƒç‰¹æ®Šæ ‡è®°"""
    # if text is None:
    #     return text
    # # ç§»é™¤å¯èƒ½çš„å›¾åƒç‰¹æ®Šæ ‡è®°
    # import re
    # text = re.sub(r'<image>|</image>|<img>|</img>', '', text)
    # return text
    return text


def _gc():
    """åƒåœ¾å›æ”¶"""
    gc.collect()
    if mindtorch.cuda.is_available():
        mindtorch.cuda.empty_cache()


def clean_repeated_substrings(text):
    """Clean repeated substrings in text"""
    n = len(text)
    if n < 2000:
        return text
    for length in range(2, n // 10 + 1):
        candidate = text[-length:]
        count = 0
        i = n - length

        while i >= 0 and text[i : i + length] == candidate:
            count += 1
            i -= length

        if count >= 10:
            return text[: n - length * (count - 1)]

    return text


def _launch_demo(args, model, processor):
    # å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ªæ˜¯å¦æ˜¯é¦–æ¬¡è°ƒç”¨
    first_call = [True]

    # å…³é”®ä¿®å¤ï¼šç§»é™¤ model å’Œ processor å‚æ•°ï¼Œä½¿ç”¨é—­åŒ…è®¿é—®
    # å¢åŠ  duration åˆ° 120 ç§’ï¼Œé¿å…é«˜å³°æœŸè¶…æ—¶
    def call_local_model(messages):
        import time
        import sys

        start_time = time.time()

        if first_call[0]:
            print(f"[INFO] ========== è¿™æ˜¯é¦–æ¬¡æ¨ç†è°ƒç”¨ ==========")
            first_call[0] = False
        else:
            print(f"[INFO] ========== è¿™æ˜¯ç¬¬ N æ¬¡æ¨ç†è°ƒç”¨ ==========")

        print(f"[DEBUG] ========== å¼€å§‹æ¨ç† ==========")
        print(f"[DEBUG] Python version: {sys.version}")
        print(f"[DEBUG] PyTorch version: {mindtorch.__version__}")
        print(f"[DEBUG] CUDA available: {mindtorch.cuda.is_available()}")
        if mindtorch.cuda.is_available():
            print(f"[DEBUG] CUDA device count: {mindtorch.cuda.device_count()}")
            print(f"[DEBUG] Current CUDA device: {mindtorch.cuda.current_device()}")
            print(f"[DEBUG] Device name: {mindtorch.cuda.get_device_name(0)}")
            print(
                f"[DEBUG] GPU Memory allocated: {mindtorch.cuda.memory_allocated(0) / 1024**3:.2f} GB"
            )
            print(
                f"[DEBUG] GPU Memory reserved: {mindtorch.cuda.memory_reserved(0) / 1024**3:.2f} GB"
            )

        # å…³é”®ï¼šæ£€æŸ¥å¹¶ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
        model_device = next(model.parameters()).device
        print(f"[DEBUG] Model device: {model_device}")
        print(f"[DEBUG] Model dtype: {next(model.parameters()).dtype}")

        if str(model_device) == "cpu":
            print(f"[ERROR] æ¨¡å‹åœ¨ CPU ä¸Šï¼å°è¯•ç§»åŠ¨åˆ° GPU...")
            if mindtorch.cuda.is_available():
                move_start = time.time()
                model.cuda()
                move_time = time.time() - move_start
                print(
                    f"[DEBUG] Model device after cuda(): {next(model.parameters()).device}"
                )
                print(f"[DEBUG] æ¨¡å‹ç§»åŠ¨åˆ° GPU è€—æ—¶: {move_time:.2f}s")
            else:
                print(f"[CRITICAL] CUDA ä¸å¯ç”¨ï¼å°†åœ¨ CPU ä¸Šè¿è¡Œï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼")
                print(f"[CRITICAL] è¿™å¯èƒ½æ˜¯å› ä¸º ZeroGPU èµ„æºç´§å¼ æˆ–è¶…æ—¶")
        else:
            print(f"[INFO] æ¨¡å‹å·²åœ¨ GPU ä¸Š: {model_device}")

        messages = [messages]

        # ä½¿ç”¨ processor æ„é€ è¾“å…¥æ ¼å¼
        texts = [
            processor.apply_chat_template(
                msg, tokenize=False, add_generation_prompt=True
            )
            for msg in messages
        ]
        print(f"[DEBUG] æ¨¡æ¿æ„å»ºå®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")

        image_inputs, video_inputs = process_vision_info(messages)
        print(f"[DEBUG] å›¾åƒå¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")

        # æ£€æŸ¥å›¾åƒè¾“å…¥å¤§å°
        if image_inputs:
            for idx, img in enumerate(image_inputs):
                if hasattr(img, "size"):
                    print(f"[DEBUG] Image {idx} size: {img.size}")
                elif isinstance(img, np.ndarray):
                    print(f"[DEBUG] Image {idx} shape: {img.shape}")

        print(f"[DEBUG] å¼€å§‹ processor ç¼–ç è¾“å…¥...")
        processor_start = time.time()

        print(f"[DEBUG] å¼€å§‹ processor ç¼–ç è¾“å…¥...")
        processor_start = time.time()
        inputs = processor(
            text=texts,
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        print(f"[DEBUG] Processor ç¼–ç å®Œæˆï¼Œè€—æ—¶: {time.time() - processor_start:.2f}s")

        # ç¡®ä¿è¾“å…¥åœ¨ GPU ä¸Š
        to_device_start = time.time()
        inputs = inputs.to("cuda" if mindtorch.cuda.is_available() else "cpu")
        print(f"[DEBUG] è¾“å…¥ç§»åˆ°è®¾å¤‡è€—æ—¶: {time.time() - to_device_start:.2f}s")
        print(f"[DEBUG] è¾“å…¥å‡†å¤‡å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - start_time:.2f}s")
        print(f"[DEBUG] Input IDs shape: {inputs.input_ids.shape}")
        print(f"[DEBUG] Input device: {inputs.input_ids.device}")
        print(f"[DEBUG] Input sequence length: {inputs.input_ids.shape[1]}")

        # ç”Ÿæˆ
        gen_start = time.time()
        print(f"[DEBUG] ========== å¼€å§‹ç”Ÿæˆ tokens ==========")

        # å…³é”®ä¼˜åŒ–ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´ max_new_tokens
        # OCR ä»»åŠ¡é€šå¸¸ä¸éœ€è¦ 8192 tokensï¼Œè¿™ä¼šå¯¼è‡´ä¸å¿…è¦çš„ç­‰å¾…
        max_new_tokens = 2048  # ä» 8192 é™åˆ° 2048ï¼Œå¤§å¹…æé€Ÿ
        print(f"[DEBUG] max_new_tokens: {max_new_tokens}")

        # æ·»åŠ è¿›åº¦å›è°ƒ
        token_count = [0]
        last_time = [gen_start]

        def progress_callback(input_ids, scores, **kwargs):
            token_count[0] += 1
            current_time = time.time()
            if token_count[0] % 10 == 0 or (current_time - last_time[0]) > 2.0:
                elapsed = current_time - gen_start
                tokens_per_sec = token_count[0] / elapsed if elapsed > 0 else 0
                print(
                    f"[DEBUG] å·²ç”Ÿæˆ {token_count[0]} tokens, é€Ÿåº¦: {tokens_per_sec:.2f} tokens/s, è€—æ—¶: {elapsed:.2f}s"
                )
                last_time[0] = current_time
            return False

        with mindtorch.no_grad():
            print(
                f"[DEBUG] è¿›å…¥ mindtorch.no_grad() ä¸Šä¸‹æ–‡ï¼Œè€—æ—¶: {time.time() - start_time:.2f}s"
            )

            # å…ˆåšä¸€æ¬¡ç®€å•çš„å‰å‘ä¼ æ’­æµ‹è¯•
            print(f"[DEBUG] æµ‹è¯•å‰å‘ä¼ æ’­...")
            forward_test_start = time.time()
            try:
                with mindtorch.cuda.amp.autocast(dtype=mindtorch.bfloat16):
                    test_outputs = model(**inputs, use_cache=False)
                print(
                    f"[DEBUG] å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {time.time() - forward_test_start:.2f}s"
                )
            except Exception as e:
                print(f"[WARNING] å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")

            print(
                f"[DEBUG] å¼€å§‹è°ƒç”¨ model.generate()... (å½“å‰è€—æ—¶: {time.time() - start_time:.2f}s)"
            )
            generate_call_start = time.time()

            try:
                # å…³é”®ï¼šæ·»åŠ æ›´æ¿€è¿›çš„ç”Ÿæˆå‚æ•°ï¼Œå¼ºåˆ¶æ—©åœ
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    temperature=0,
                )
                print(
                    f"[DEBUG] model.generate() è¿”å›ï¼Œè€—æ—¶: {time.time() - generate_call_start:.2f}s"
                )
            except Exception as e:
                print(f"[ERROR] ç”Ÿæˆå¤±è´¥: {e}")
                import traceback

                traceback.print_exc()
                raise

        print(f"[DEBUG] é€€å‡º mindtorch.no_grad() ä¸Šä¸‹æ–‡")

        gen_time = time.time() - gen_start
        print(f"[DEBUG] ========== ç”Ÿæˆå®Œæˆ ==========")
        print(f"[DEBUG] ç”Ÿæˆè€—æ—¶: {gen_time:.2f}s")
        print(f"[DEBUG] Output shape: {generated_ids.shape}")

        # è§£ç è¾“å‡º
        if "input_ids" in inputs:
            input_ids = inputs.input_ids
        else:
            input_ids = inputs.inputs

        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(input_ids, generated_ids)
        ]

        actual_tokens = len(generated_ids_trimmed[0])
        print(f"[DEBUG] å®é™…ç”Ÿæˆ token æ•°: {actual_tokens}")
        print(
            f"[DEBUG] æ¯ token è€—æ—¶: {gen_time/actual_tokens if actual_tokens > 0 else 0:.3f}s"
        )

        output_texts = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )

        total_time = time.time() - start_time
        print(f"[DEBUG] ========== å…¨éƒ¨å®Œæˆ ==========")
        print(f"[DEBUG] æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"[DEBUG] è¾“å‡ºé•¿åº¦: {len(output_texts[0])} å­—ç¬¦")
        print(f"[DEBUG] è¾“å‡ºé¢„è§ˆ: {output_texts[0][:100]}...")
        output_texts[0] = clean_repeated_substrings(output_texts[0])
        return output_texts

    def create_predict_fn():

        def predict(_chatbot, task_history):
            nonlocal model, processor
            chat_query = _chatbot[-1][0]
            query = task_history[-1][0]
            if len(chat_query) == 0:
                _chatbot.pop()
                task_history.pop()
                return _chatbot
            print("User: ", query)
            history_cp = copy.deepcopy(task_history)
            full_response = ""
            messages = []
            content = []
            for q, a in history_cp:
                if isinstance(q, (tuple, list)):
                    # åˆ¤æ–­æ˜¯URLè¿˜æ˜¯æœ¬åœ°è·¯å¾„
                    img_path = q[0]
                    if img_path.startswith(("http://", "https://")):
                        content.append({"type": "image", "image": img_path})
                    else:
                        content.append(
                            {"type": "image", "image": f"{os.path.abspath(img_path)}"}
                        )
                else:
                    content.append({"type": "text", "text": q})
                    messages.append({"role": "user", "content": content})
                    messages.append(
                        {"role": "assistant", "content": [{"type": "text", "text": a}]}
                    )
                    content = []
            messages.pop()

            # è°ƒç”¨æ¨¡å‹è·å–å“åº”ï¼ˆå·²ä¿®æ”¹ï¼šä¸å†ä¼ é€’ model å’Œ processorï¼‰
            response_list = call_local_model(messages)
            response = response_list[0] if response_list else ""

            _chatbot[-1] = (
                _parse_text(chat_query),
                _remove_image_special(_parse_text(response)),
            )
            full_response = _parse_text(response)

            task_history[-1] = (query, full_response)
            print("HunyuanOCR: " + _parse_text(full_response))
            yield _chatbot

        return predict

    def create_regenerate_fn():

        def regenerate(_chatbot, task_history):
            nonlocal model, processor
            if not task_history:
                return _chatbot
            item = task_history[-1]
            if item[1] is None:
                return _chatbot
            task_history[-1] = (item[0], None)
            chatbot_item = _chatbot.pop(-1)
            if chatbot_item[0] is None:
                _chatbot[-1] = (_chatbot[-1][0], None)
            else:
                _chatbot.append((chatbot_item[0], None))
            # ä½¿ç”¨å¤–å±‚çš„predictå‡½æ•°
            _chatbot_gen = predict(_chatbot, task_history)
            for _chatbot in _chatbot_gen:
                yield _chatbot

        return regenerate

    predict = create_predict_fn()
    regenerate = create_regenerate_fn()

    def add_text(history, task_history, text):
        task_text = text
        history = history if history is not None else []
        task_history = task_history if task_history is not None else []
        history = history + [(_parse_text(text), None)]
        task_history = task_history + [(task_text, None)]
        return history, task_history, ""

    def add_file(history, task_history, file):
        history = history if history is not None else []
        task_history = task_history if task_history is not None else []
        history = history + [((file.name,), None)]
        task_history = task_history + [((file.name,), None)]
        return history, task_history

    def download_url_image(url):
        """ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            # ä½¿ç”¨ URL çš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶åï¼Œé¿å…é‡å¤ä¸‹è½½
            url_hash = hashlib.md5(url.encode()).hexdigest()
            temp_dir = tempfile.gettempdir()
            temp_path = os.path.join(temp_dir, f"hyocr_demo_{url_hash}.png")

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if os.path.exists(temp_path):
                return temp_path

            # ä¸‹è½½å›¾ç‰‡
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            with open(temp_path, "wb") as f:
                f.write(response.content)
            return temp_path
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {e}")
            return url  # å¤±è´¥æ—¶è¿”å›åŸ URL

    def reset_user_input():
        return gr.update(value="")

    def reset_state(_chatbot, task_history):
        task_history.clear()
        _chatbot.clear()
        _gc()
        return []

    # ç¤ºä¾‹å›¾ç‰‡è·¯å¾„é…ç½® - è¯·æ›¿æ¢ä¸ºå®é™…å›¾ç‰‡è·¯å¾„
    EXAMPLE_IMAGES = {
        "spotting": "examples/spotting.jpg",
        "parsing": "examples/parsing.jpg",
        "ie": "examples/ie.jpg",
        "vqa": "examples/vqa.jpg",
        "translation": "examples/translation.jpg",
    }

    with gr.Blocks(
        css="""
        body {
            background: #f5f7fa;
        }
        .gradio-container {
            max-width: 100% !important;
            padding: 0 40px !important;
        }
        .header-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 30px 0;
            margin: -20px -40px 30px -40px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .header-content {
            max-width: 1600px;
            margin: 0 auto;
            padding: 0 40px;
            display: flex;
            align-items: center;
            gap: 20px;
        }
        .header-logo {
            height: 60px;
        }
        .header-text h1 {
            color: white;
            font-size: 32px;
            font-weight: bold;
            margin: 0 0 5px 0;
        }
        .header-text p {
            color: rgba(255,255,255,0.9);
            margin: 0;
            font-size: 14px;
        }
        .main-container {
            max-width: 1800px;
            margin: 0 auto;
        }
        .chatbot {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08) !important;
            border-radius: 12px !important;
            border: 1px solid #e5e7eb !important;
            background: white !important;
        }
        .input-panel {
            background: white;
            padding: 20px;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border: 1px solid #e5e7eb;
        }
        .input-box textarea {
            border: 2px solid #e5e7eb !important;
            border-radius: 8px !important;
            font-size: 14px !important;
        }
        .input-box textarea:focus {
            border-color: #667eea !important;
        }
        .btn-primary {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
            border: none !important;
            color: white !important;
            font-weight: 500 !important;
            padding: 10px 24px !important;
            font-size: 14px !important;
        }
        .btn-primary:hover {
            transform: translateY(-1px) !important;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4) !important;
        }
        .btn-secondary {
            background: white !important;
            border: 2px solid #667eea !important;
            color: #667eea !important;
            padding: 8px 20px !important;
            font-size: 14px !important;
        }
        .btn-secondary:hover {
            background: #f0f4ff !important;
        }
        .example-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
            margin-top: 30px;
        }
        .example-card {
            background: white;
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border: 1px solid #e5e7eb;
            transition: all 0.3s ease;
        }
        .example-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.15);
            border-color: #667eea;
        }
        .example-image-wrapper {
            width: 100%;
            height: 180px;
            overflow: hidden;
            background: #f5f7fa;
        }
        .example-image-wrapper img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        .example-btn {
            width: 100% !important;
            white-space: pre-wrap !important;
            text-align: left !important;
            padding: 16px !important;
            background: white !important;
            border: none !important;
            border-top: 1px solid #e5e7eb !important;
            color: #1f2937 !important;
            font-size: 14px !important;
            line-height: 1.6 !important;
            transition: all 0.3s ease !important;
            font-weight: 500 !important;
        }
        .example-btn:hover {
            background: #f9fafb !important;
            color: #667eea !important;
        }
        .feature-section {
            background: white;
            padding: 24px;
            border-radius: 12px;
            margin-top: 30px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border: 1px solid #e5e7eb;
        }
        .section-title {
            font-size: 18px;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 20px;
            padding-bottom: 12px;
            border-bottom: 2px solid #e5e7eb;
        }
    """
    ) as demo:
        # é¡¶éƒ¨å¯¼èˆªæ 
        gr.HTML(
            """
        <div class="header-section">
            <div class="header-content">
                <div class="header-text">
                    <h1>HunyuanOCR + MindNLP</h1>
                    <p>Powered by Tencent Hunyuan Team and MindSpore Team</p>
                </div>
            </div>
        </div>
        """
        )

        with gr.Column(elem_classes=["main-container"]):
            # å¯¹è¯åŒºåŸŸ - å…¨å®½
            chatbot = gr.Chatbot(
                label="ğŸ’¬ å¯¹è¯çª—å£",
                height=600,
                bubble_full_width=False,
                layout="bubble",
                show_copy_button=True,
                elem_classes=["chatbot"],
            )

            # è¾“å…¥æ§åˆ¶é¢æ¿ - å…¨å®½
            with gr.Group(elem_classes=["input-panel"]):
                query = gr.Textbox(
                    lines=2,
                    label="ğŸ’­ è¾“å…¥æ‚¨çš„é—®é¢˜",
                    placeholder="è¯·å…ˆä¸Šä¼ å›¾ç‰‡ï¼Œç„¶åè¾“å…¥é—®é¢˜ã€‚ä¾‹å¦‚ï¼šæ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚",
                    elem_classes=["input-box"],
                    show_label=False,
                )

                with gr.Row():
                    addfile_btn = gr.UploadButton(
                        "ğŸ“ ä¸Šä¼ å›¾ç‰‡",
                        file_types=["image"],
                        elem_classes=["btn-secondary"],
                    )
                    submit_btn = gr.Button(
                        "ğŸš€ å‘é€æ¶ˆæ¯",
                        variant="primary",
                        elem_classes=["btn-primary"],
                        scale=3,
                    )
                    regen_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ", elem_classes=["btn-secondary"])
                    empty_bin = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", elem_classes=["btn-secondary"])

            # ç¤ºä¾‹åŒºåŸŸ - 5åˆ—ç½‘æ ¼å¸ƒå±€
            gr.HTML(
                '<div class="section-title">ğŸ“š å¿«é€Ÿä½“éªŒç¤ºä¾‹ - ç‚¹å‡»ä¸‹æ–¹å¡ç‰‡å¿«é€ŸåŠ è½½</div>'
            )

            with gr.Row():
                # ç¤ºä¾‹1ï¼šspotting
                with gr.Column(scale=1):
                    with gr.Group(elem_classes=["example-card"]):
                        gr.Image("examples/spotting.jpg", height=180, elem_id="example-image-1")
                        example_1_btn = gr.Button(
                            "ğŸ” æ–‡å­—æ£€æµ‹å’Œè¯†åˆ«", elem_classes=["example-btn"]
                        )

                # ç¤ºä¾‹2ï¼šparsing
                with gr.Column(scale=1):
                    with gr.Group(elem_classes=["example-card"]):
                        gr.Image("examples/parsing.jpg", height=180, elem_id="example-image-2")
                        example_2_btn = gr.Button(
                            "ğŸ“‹ æ–‡æ¡£è§£æ", elem_classes=["example-btn"]
                        )

                # ç¤ºä¾‹3ï¼šie
                with gr.Column(scale=1):
                    with gr.Group(elem_classes=["example-card"]):
                        gr.Image("examples/ie.jpg", height=180, elem_id="example-image-3")
                        example_3_btn = gr.Button(
                            "ğŸ¯ ä¿¡æ¯æŠ½å–", elem_classes=["example-btn"]
                        )

                # ç¤ºä¾‹4ï¼šVQA
                with gr.Column(scale=1):
                    with gr.Group(elem_classes=["example-card"]):
                        gr.Image("examples/vqa.jpg", height=180, elem_id="example-image-4")
                        example_4_btn = gr.Button(
                            "ğŸ’¬ è§†è§‰é—®ç­”", elem_classes=["example-btn"]
                        )

                # ç¤ºä¾‹5ï¼štranslation
                with gr.Column(scale=1):
                    with gr.Group(elem_classes=["example-card"]):
                        gr.Image("examples/translation.jpg", height=180, elem_id="example-image-5")
                        example_5_btn = gr.Button(
                            "ğŸŒ å›¾ç‰‡ç¿»è¯‘", elem_classes=["example-btn"]
                        )

        task_history = gr.State([])

        # ç¤ºä¾‹1ï¼šæ–‡æ¡£è¯†åˆ«
        def load_example_1(history, task_hist):
            prompt = "æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚"
            image_path = EXAMPLE_IMAGES["spotting"]
            # # ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°
            # image_path = download_url_image(image_url)
            # æ¸…ç©ºå¯¹è¯å†å²
            history = []
            task_hist = []
            history = history + [((image_path,), None)]
            task_hist = task_hist + [((image_path,), None)]
            return history, task_hist, prompt

        # ç¤ºä¾‹2ï¼šåœºæ™¯æ–‡å­—
        def load_example_2(history, task_hist):
            prompt = "æå–æ–‡æ¡£å›¾ç‰‡ä¸­æ­£æ–‡çš„æ‰€æœ‰ä¿¡æ¯ç”¨markdown æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä¸­é¡µçœ‰ã€é¡µè„šéƒ¨åˆ†å¿½ç•¥ï¼Œè¡¨æ ¼ç”¨html æ ¼å¼è¡¨è¾¾ï¼Œæ–‡æ¡£ä¸­å…¬å¼ç”¨latex æ ¼å¼è¡¨ç¤ºï¼ŒæŒ‰ç…§é˜…è¯»é¡ºåºç»„ç»‡è¿›è¡Œè§£æã€‚"
            image_path = EXAMPLE_IMAGES["parsing"]
            # # ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°
            # image_path = download_url_image(image_url)
            # æ¸…ç©ºå¯¹è¯å†å²
            history = []
            task_hist = []
            history = history + [((image_path,), None)]
            task_hist = task_hist + [((image_path,), None)]
            return history, task_hist, prompt

        # ç¤ºä¾‹3ï¼šè¡¨æ ¼æå–
        def load_example_3(history, task_hist):
            prompt = "æå–å›¾ç‰‡ä¸­çš„ï¼š['å•ä»·', 'ä¸Šè½¦æ—¶é—´','å‘ç¥¨å·ç ', 'çœå‰ç¼€', 'æ€»é‡‘é¢', 'å‘ç¥¨ä»£ç ', 'ä¸‹è½¦æ—¶é—´', 'é‡Œç¨‹æ•°'] çš„å­—æ®µå†…å®¹ï¼Œå¹¶ä¸”æŒ‰ç…§JSONæ ¼å¼è¿”å›ã€‚"
            image_path = EXAMPLE_IMAGES["ie"]
            # # ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°
            # image_path = download_url_image(image_url)
            # æ¸…ç©ºå¯¹è¯å†å²
            history = []
            task_hist = []
            history = history + [((image_path,), None)]
            task_hist = task_hist + [((image_path,), None)]
            return history, task_hist, prompt

        # ç¤ºä¾‹4ï¼šæ‰‹å†™ä½“
        def load_example_4(history, task_hist):
            prompt = "What is the highest life expectancy at birth of male?"
            image_path = EXAMPLE_IMAGES["vqa"]
            # # ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°
            # image_path = download_url_image(image_url)
            # æ¸…ç©ºå¯¹è¯å†å²
            history = []
            task_hist = []
            history = history + [((image_path,), None)]
            task_hist = task_hist + [((image_path,), None)]
            return history, task_hist, prompt

        # ç¤ºä¾‹5ï¼šç¿»è¯‘
        def load_example_5(history, task_hist):
            prompt = "å°†å›¾ä¸­æ–‡å­—ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚"
            image_path = EXAMPLE_IMAGES["translation"]
            # ä¸‹è½½ URL å›¾ç‰‡åˆ°æœ¬åœ°
            # image_path = download_url_image(image_url)
            # æ¸…ç©ºå¯¹è¯å†å²
            history = []
            task_hist = []
            history = history + [((image_path,), None)]
            task_hist = task_hist + [((image_path,), None)]
            return history, task_hist, prompt

        # ç»‘å®šäº‹ä»¶
        example_1_btn.click(
            load_example_1, [chatbot, task_history], [chatbot, task_history, query]
        )
        example_2_btn.click(
            load_example_2, [chatbot, task_history], [chatbot, task_history, query]
        )
        example_3_btn.click(
            load_example_3, [chatbot, task_history], [chatbot, task_history, query]
        )
        example_4_btn.click(
            load_example_4, [chatbot, task_history], [chatbot, task_history, query]
        )
        example_5_btn.click(
            load_example_5, [chatbot, task_history], [chatbot, task_history, query]
        )

        submit_btn.click(
            add_text, [chatbot, task_history, query], [chatbot, task_history]
        ).then(predict, [chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(reset_user_input, [], [query])
        empty_bin.click(
            reset_state, [chatbot, task_history], [chatbot], show_progress=True
        )
        regen_btn.click(
            regenerate, [chatbot, task_history], [chatbot], show_progress=True
        )
        addfile_btn.upload(
            add_file,
            [chatbot, task_history, addfile_btn],
            [chatbot, task_history],
            show_progress=True,
        )

        # åŠŸèƒ½è¯´æ˜åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=1):
                gr.HTML(
                    """
                <div class="feature-section">
                    <div class="section-title">âœ¨ æ ¸å¿ƒåŠŸèƒ½</div>
                    <ul style="line-height: 2; color: #4b5563; font-size: 14px; margin: 0; padding-left: 20px;">
                        <li><strong>ğŸ¯ é«˜ç²¾åº¦æ–‡å­—æ£€æµ‹è¯†åˆ«</strong> - æ”¯æŒå¤šåœºæ™¯æ–‡å­—æ£€æµ‹ä¸è¯†åˆ«</li>
                        <li><strong>ğŸ“ æ™ºèƒ½æ–‡æ¡£è§£æ</strong> - è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç»“æ„ï¼Œæ”¯æŒå¤šç²’åº¦æ–‡æ¡£è§£æ</li>
                        <li><strong>ğŸ“‹ ä¿¡æ¯æå–</strong> - æ”¯æŒ30+é«˜é¢‘å¡è¯ç¥¨æ®è¯†åˆ«å’Œç»“æ„åŒ–è¾“å‡º</li>
                        <li><strong>âœï¸ è§†è§‰é—®ç­”</strong> - æ”¯æŒä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¼€æ”¾å¼é—®ç­”</li>
                        <li><strong>ğŸŒ è·¨è¯­è¨€ç¿»è¯‘</strong> - æ”¯æŒä¸­è‹±äº’è¯‘åŠ14+è¯­ç§è¯‘ä¸ºä¸­è‹±æ–‡</li>
                    </ul>
                </div>
                """
                )

            with gr.Column(scale=1):
                gr.HTML(
                    """
                <div class="feature-section">
                    <div class="section-title">ğŸ’¡ ä½¿ç”¨å»ºè®®</div>
                    <ul style="line-height: 2; color: #4b5563; font-size: 14px; margin: 0; padding-left: 20px;">
                        <li><strong>æ¨ç†æ¡†æ¶</strong> - æ­£å¼ç”Ÿäº§æ¨èä½¿ç”¨VLLMï¼Œä»¥è·å–æ›´å¥½çš„æ¨ç†æ€§èƒ½å’Œç²¾åº¦</li>
                        <li><strong>æ‹æ‘„è§’åº¦</strong> - ç¡®ä¿å›¾ç‰‡æ¸…æ™°ï¼Œå…‰çº¿å……è¶³ï¼Œåˆ†è¾¨ç‡é€‚ä¸­ï¼Œé¿å…ä¸¥é‡å€¾æ–œã€é®æŒ¡æˆ–åå…‰ï¼Œæ­£é¢æ‹æ‘„æ•ˆæœæœ€ä½³</li>
                        <li><strong>æ–‡ä»¶å¤§å°</strong> - å»ºè®®å•å¼ å›¾ç‰‡ä¸è¶…è¿‡ 10MBï¼Œæ”¯æŒ JPG/PNG æ ¼å¼</li>
                        <li><strong>ä½¿ç”¨åœºæ™¯</strong> - é€‚ç”¨äºæ–‡å­—æ£€æµ‹è¯†åˆ«ã€æ–‡æ¡£æ•°å­—åŒ–ã€ç¥¨æ®è¯†åˆ«ã€ä¿¡æ¯æå–ã€æ–‡å­—å›¾ç‰‡ç¿»è¯‘ç­‰</li>
                        <li><strong>åˆè§„ä½¿ç”¨</strong> - ä»…ä¾›å­¦ä¹ ç ”ç©¶ï¼Œè¯·éµå®ˆæ³•å¾‹æ³•è§„ï¼Œå°Šé‡éšç§æƒ</li>
                    </ul>
                </div>
                """
                )

        # åº•éƒ¨ç‰ˆæƒä¿¡æ¯
        gr.HTML(
            """
        <div style="text-align: center; color: #9ca3af; font-size: 13px; margin-top: 40px; padding: 20px; border-top: 1px solid #e5e7eb;">
            <p style="margin: 0;">Â© 2025 Tencent Hunyuan Team. All rights reserved.</p>
            <p style="margin: 5px 0 0 0;">æœ¬ç³»ç»ŸåŸºäº HunyuanOCR æ„å»º | ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨</p>
        </div>
        """
        )
    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        # server_port=args.server_port,
        # server_name=args.server_name,
    )


def main():
    args = _get_args()
    model, processor = _load_model_processor(args)
    _launch_demo(args, model, processor)


if __name__ == "__main__":
    main()
