(MindSpore) [ma-user work]$python mindNLPBigbirdpagesus.py 
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 1.259 seconds.
Prefix dict has been built successfully.
加载模型和分词器
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. 
  warnings.warn(
  0%|                                                                                                     | 0.00/2.15G [00:10<?, ?B/s]
Failed to download: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.
Retrying... (attempt 0/5)
100%|████████████████████████████████████████████████████████████████████████████████████████████| 2.15G/2.15G [12:36<00:00, 3.05MB/s]
BigBirdPegasusForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 232/232 [00:00<00:00, 901kB/s]
模型和分词器加载完成
input question: Nice to meet you too. What are you interested in?
Attention type 'block_sparse' is not possible if sequence_length: 13 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...
output answer: we present a new method for the detection of rare events, based on the use of time - frequency combs.<n> we show how this technique can be used to detect rare events in a broad range of time - frequency domains.<n> we also show how this technique can be used to study the evolution of the spectrum of rare events. <n> rare events ; amplitude ; phase ; amplitude ; frequency ; time - frequency combs + _ pacs : _<n> 11.30.er, 12.20.fv, 12.20.ds, 12.60.jv, 12.60.jv @xmath0 department of physics and astronomy, iowa state university, ames, ia 50011 + @xmath1 department of physics and astronomy, university of iowa, ames, ia 50011 + @xmath2 department of physics and astronomy, university of iowa, ames, ia 50011 + _ key words : _ rare events ; amplitude ; phase ; frequency ; spectrum ; time - frequency combs + _ pacs : _<n> 11.30.er 
加载数据集
README.md: 3.11kB [00:00, 1.24MB/s]                                                                                                   
Synthetic-Persona-Chat_train.csv: 100%|██████████████████████████████████████████████████████████| 15.9M/15.9M [00:10<00:00, 1.55MB/s]
Synthetic-Persona-Chat_valid.csv: 1.78MB [00:00, 3.65MB/s]
Synthetic-Persona-Chat_test.csv: 1.72MB [00:00, 6.70MB/s]
Generating train split: 8938 examples [00:00, 29991.79 examples/s]
Generating validation split: 1000 examples [00:00, 29394.73 examples/s]
Generating test split: 968 examples [00:00, 31306.81 examples/s]
dataset finished
dataset: DatasetDict({
    train: Dataset({
        features: ['user 1 personas', 'user 2 personas', 'Best Generated Conversation'],
        num_rows: 8938
    })
    validation: Dataset({
        features: ['user 1 personas', 'user 2 personas', 'Best Generated Conversation'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['user 1 personas', 'user 2 personas', 'Best Generated Conversation'],
        num_rows: 968
    })
})
dataset['train'][0]: {'user 1 personas': 'I am 32.\nI do not want a job.\nI play video games all day.\nI still live at home with my parents.', 'user 2 personas': 'My favorite drink is iced coffee.\nI have a black belt in karate.\nI m in a jazz band and play the saxophone.\nI vacation along lake michigan every summer.', 'Best Generated Conversation': "User 1: Hi! I'm [user 1's name].\nUser 2: Hi [user 1's name], I'm [user 2's name].\nUser 1: What do you do for fun?\nUser 2: I like to play video games, go to the beach, and read.\nUser 1: I like to play video games too! I'm not much of a reader, though.\nUser 2: What video games do you like to play?\nUser 1: I like to play a lot of different games, but I'm really into competitive online games right now.\nUser 2: I'm not really into competitive games, I like to play more relaxing games.\nUser 1: That's cool. What kind of relaxing games do you like to play?\nUser 2: I like to play puzzle games, simulation games, and story-based games.\nUser 1: I've never been much of a puzzle game person, but I do like simulation games and story-based games.\nUser 2: Nice! What's your favorite simulation game?\nUser 1: I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.\nUser 2: I've heard good things about that game. I might have to check it out.\nUser 1: You should! It's a lot of fun.\nUser 2: Well, I'm glad we met. Maybe we can play some games together sometime.\nUser 1: That would be fun!\nUser 2: Great! I'll send you my Steam name.\nUser 1: Ok, sounds good."}
dataset_train: Dataset({
    features: ['user 1 personas', 'user 2 personas', 'Best Generated Conversation'],
    num_rows: 8938
})
dataset_train['Best Generated Conversation'][0]:
 User 1: Hi! I'm [user 1's name].
User 2: Hi [user 1's name], I'm [user 2's name].
User 1: What do you do for fun?
User 2: I like to play video games, go to the beach, and read.
User 1: I like to play video games too! I'm not much of a reader, though.
User 2: What video games do you like to play?
User 1: I like to play a lot of different games, but I'm really into competitive online games right now.
User 2: I'm not really into competitive games, I like to play more relaxing games.
User 1: That's cool. What kind of relaxing games do you like to play?
User 2: I like to play puzzle games, simulation games, and story-based games.
User 1: I've never been much of a puzzle game person, but I do like simulation games and story-based games.
User 2: Nice! What's your favorite simulation game?
User 1: I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun.
User 2: I've heard good things about that game. I might have to check it out.
User 1: You should! It's a lot of fun.
User 2: Well, I'm glad we met. Maybe we can play some games together sometime.
User 1: That would be fun!
User 2: Great! I'll send you my Steam name.
User 1: Ok, sounds good.
dataset_train['user 1 personas'][0]: I am 32.
I do not want a job.
I play video games all day.
I still live at home with my parents.
dataset_train['user 2 personas'][0]: My favorite drink is iced coffee.
I have a black belt in karate.
I m in a jazz band and play the saxophone.
I vacation along lake michigan every summer.
dataset_train.column_names: ['user 1 personas', 'user 2 personas', 'Best Generated Conversation']
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 8938/8938 [00:02<00:00, 4322.77 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4363.55 examples/s]
Saving the dataset (1/1 shards): 100%|█████████████████████████████████████████████| 235897/235897 [00:00<00:00, 618411.48 examples/s]
Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████| 26872/26872 [00:00<00:00, 642174.54 examples/s]
tokenizer数据集
Map:   0%|                                                                                          | 0/235897 [00:00<?, ? examples/s]/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:4025: UserWarning: `as_target_tokenizer` is deprecated. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|████████████████████████████████████████████████████████████████████████████| 235897/235897 [05:00<00:00, 785.79 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████| 26872/26872 [00:34<00:00, 776.18 examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████| 235897/235897 [01:23<00:00, 2816.62 examples/s]
Filter: 100%|██████████████████████████████████████████████████████████████████████████| 26872/26872 [00:09<00:00, 2822.90 examples/s]
Saving the dataset (2/2 shards): 100%|██████████████████████████████████████████████| 235897/235897 [00:05<00:00, 41400.40 examples/s]
Saving the dataset (1/1 shards): 100%|████████████████████████████████████████████████| 26872/26872 [00:00<00:00, 40392.40 examples/s]
dataset_train_tokenized: Dataset({
    features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 2358
})
dataset_valid_tokenized: Dataset({
    features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],
    num_rows: 268
})
开始训练
  0%|                                                                      | 0/5900 [00:00<?, ?it/s]  0%|                                                           | 1/5900 [00:44<72:58:00, 44.53s/it]  8%|████▉                                                     | 500/5900 [08:48<1:31:37,  1.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1826, 'learning_rate': 4.577586206896552e-05, 'epoch': 1.0}                              
 10%|█████▊                                                    | 590/5900 [11:23<1:21:43,  1.08it/s]{'eval_loss': 0.23969502747058868, 'eval_runtime': 8.5857, 'eval_samples_per_second': 7.804, 'eval_steps_per_second': 1.98, 'epoch': 1.0}                                                               
 17%|█████████▋                                               | 1000/5900 [17:39<1:09:31,  1.17it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1614, 'learning_rate': 4.0689655172413795e-05, 'epoch': 2.0}                             
{'eval_loss': 0.24506837129592896, 'eval_runtime': 8.1272, 'eval_samples_per_second': 8.244, 'eval_steps_per_second': 2.092, 'epoch': 2.0}                                                              
 25%|██████████████▍                                          | 1500/5900 [27:10<1:10:00,  1.05it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1435, 'learning_rate': 3.560344827586207e-05, 'epoch': 3.0}                              
{'eval_loss': 0.25298023223876953, 'eval_runtime': 9.0821, 'eval_samples_per_second': 7.377, 'eval_steps_per_second': 1.872, 'epoch': 3.0}          
 34%|████████████████████                                       | 2000/5900 [38:21<54:25,  1.19it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1398, 'learning_rate': 3.0517241379310348e-05, 'epoch': 4.0}                             
{'eval_loss': 0.25483402609825134, 'eval_runtime': 8.0022, 'eval_samples_per_second': 8.373, 'eval_steps_per_second': 2.124, 'epoch': 4.0}                                                              
 42%|█████████████████████████                                  | 2500/5900 [49:13<59:14,  1.05s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1344, 'learning_rate': 2.543103448275862e-05, 'epoch': 5.0}                              
{'eval_loss': 0.2594688832759857, 'eval_runtime': 8.7124, 'eval_samples_per_second': 7.69, 'eval_steps_per_second': 1.951, 'epoch': 5.0}                                                                
 51%|██████████████████████████████                             | 3000/5900 [58:45<46:59,  1.03it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
 59%|█████████████████████████████████▊                       | 3500/5900 [1:08:06<39:27,  1.01it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1263, 'learning_rate': 2.0344827586206897e-05, 'epoch': 6.0}                             
{'eval_loss': 0.26629871129989624, 'eval_runtime': 8.345, 'eval_samples_per_second': 8.029, 'eval_steps_per_second': 2.037, 'epoch': 6.0}                                                               
 68%|██████████████████████████████████████▋                  | 4000/5900 [1:17:49<32:05,  1.01s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.12, 'learning_rate': 1.5258620689655174e-05, 'epoch': 7.0}                               
{'eval_loss': 0.26900094747543335, 'eval_runtime': 8.5508, 'eval_samples_per_second': 7.836, 'eval_steps_per_second': 1.988, 'epoch': 7.0}                                                              
 76%|███████████████████████████████████████████▍             | 4500/5900 [1:27:16<23:49,  1.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1147, 'learning_rate': 1.0172413793103449e-05, 'epoch': 8.0}                             
{'eval_loss': 0.27548155188560486, 'eval_runtime': 9.2144, 'eval_samples_per_second': 7.271, 'eval_steps_per_second': 1.845, 'epoch': 8.0}                                                              
 85%|████████████████████████████████████████████████▎        | 5000/5900 [1:36:52<14:59,  1.00it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1105, 'learning_rate': 5.086206896551724e-06, 'epoch': 9.0}                              
{'eval_loss': 0.2791420519351959, 'eval_runtime': 8.8894, 'eval_samples_per_second': 7.537, 'eval_steps_per_second': 1.912, 'epoch': 9.0}                                                               
 93%|█████████████████████████████████████████████████████▏   | 5500/5900 [1:46:34<06:40,  1.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
{'loss': 0.1082, 'learning_rate': 0.0, 'epoch': 10.0}                                               
{'eval_loss': 0.2831324338912964, 'eval_runtime': 9.2625, 'eval_samples_per_second': 7.234, 'eval_steps_per_second': 1.835, 'epoch': 10.0}                                                              
{'train_runtime': 6861.2649, 'train_samples_per_second': 3.44, 'train_steps_per_second': 0.86, 'train_loss': 0.13413777367543367, 'epoch': 10.0}                                                        
100%|█████████████████████████████████████████████████████████| 5900/5900 [1:54:21<00:00,  1.16s/it]
100%|███████████████████████████████████████████████████████████████| 67/67 [00:07<00:00,  8.54it/s]
Evaluation results: {'eval_loss': 0.2831324338912964, 'eval_runtime': 8.0614, 'eval_samples_per_second': 8.311, 'eval_steps_per_second': 2.109, 'epoch': 10.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}
再次测试对话
input question: Nice to meet you too. What are you interested in?
Attention type 'block_sparse' is not possible if sequence_length: 13 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...
output answer: User 2: I'm interested in a lot of things, but I'm especially interested in history and science.