# OCR Fine-tuning Examples

æœ¬ç›®å½•åŒ…å« Qwen2-VL OCR æ¨¡å‹å¾®è°ƒçš„å®Œæ•´ç¤ºä¾‹ã€‚

## ğŸ“š æ–‡ä»¶è¯´æ˜

- `finetune_example.py`: å®Œæ•´çš„å¾®è°ƒç¤ºä¾‹ä»£ç ï¼ŒåŒ…å« LoRAã€QLoRAã€å…¨å‚æ•°å¾®è°ƒã€è¯„ä¼°å’Œæ¨ç†
- `sample_data.json`: ç¤ºä¾‹è®­ç»ƒæ•°æ® (3ä¸ªæ ·æœ¬)
- `README.md`: æœ¬æ–‡ä»¶

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

æœ¬ç¤ºä¾‹å®ç°äº† [Issue #2379](https://github.com/mindspore-lab/mindnlp/issues/2379) çš„æ‰€æœ‰è¦æ±‚ï¼š

- âœ… LoRA å¾®è°ƒå®ç°
- âœ… QLoRA å¾®è°ƒå®ç°ï¼ˆä½èµ„æºåœºæ™¯ï¼‰
- âœ… å…¨å‚æ•°å¾®è°ƒå®ç°
- âœ… æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”æµç¨‹
- âœ… åœºæ™¯ç‰¹å®šè¯„ä¼°ï¼ˆè¡¨æ ¼ã€å…¬å¼ã€æ‰‹å†™ä½“ï¼‰
- âœ… æ•°æ®é›†å‡†å¤‡å·¥å…·
- âœ… è¯¦ç»†çš„æœ€ä½³å®è·µæ–‡æ¡£

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡æ•°æ®

å°†ä½ çš„å›¾ç‰‡æ”¾åœ¨ `images/` ç›®å½•ä¸‹,å¹¶å‡†å¤‡è®­ç»ƒæ•°æ® JSON æ–‡ä»¶:

```json
[
  {
    "image_path": "images/your_image.jpg",
    "conversations": [
      {
        "role": "user",
        "content": "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„æ–‡å­—"
      },
      {
        "role": "assistant",
        "content": "è¯†åˆ«ç»“æœæ–‡æœ¬"
      }
    ],
    "task_type": "general"
  }
]
```

### 2. LoRA å¾®è°ƒ

```bash
python -m mindnlp.ocr.finetune.train_lora \
  --model_name_or_path Qwen/Qwen2-VL-7B-Instruct \
  --data_path ./sample_data.json \
  --image_folder ./images \
  --output_dir ./output/lora_model \
  --lora_r 16 \
  --lora_alpha 32 \
  --num_epochs 3 \
  --batch_size 4 \
  --learning_rate 2e-4
```

### 3. QLoRA å¾®è°ƒ (ä½æ˜¾å­˜)

```bash
python -m mindnlp.ocr.finetune.train_qlora \
  --model_name_or_path Qwen/Qwen2-VL-7B-Instruct \
  --data_path ./sample_data.json \
  --image_folder ./images \
  --output_dir ./output/qlora_model \
  --load_in_4bit \
  --lora_r 16 \
  --num_epochs 3 \
  --batch_size 4
```

### 4. æ¨¡å‹è¯„ä¼°

```bash
python -m mindnlp.ocr.finetune.evaluate \
  --model_path ./output/lora_model/final_model \
  --test_data_path ./test_data.json \
  --image_folder ./images \
  --output_file ./evaluation_results.json
```

### 5. Python API ä½¿ç”¨

```python
from mindnlp.ocr.finetune import train_lora, evaluate_model

# è®­ç»ƒ
model, processor = train_lora(
    model_name_or_path="Qwen/Qwen2-VL-7B-Instruct",
    data_path="./sample_data.json",
    image_folder="./images",
    output_dir="./output/lora_model",
    lora_r=16,
    lora_alpha=32,
    num_epochs=3,
    batch_size=4,
    learning_rate=2e-4,
)

# è¯„ä¼°
metrics = evaluate_model(
    model=model,
    processor=processor,
    test_data_path="./test_data.json",
    image_folder="./images",
    device="cuda"
)

print(f"CER: {metrics['cer']:.4f}")
print(f"WER: {metrics['wer']:.4f}")
```

## å®Œæ•´æ–‡æ¡£

è¯¦ç»†çš„å¾®è°ƒæ–‡æ¡£è¯·æŸ¥çœ‹: [docs/ocr_finetuning.md](../../docs/ocr_finetuning.md)

## Issue #2379 è¦æ±‚

æ ¹æ® [Issue #2379](https://github.com/mindspore-ai/mindnlp/issues/2379) çš„è¦æ±‚:

- âœ… æ”¯æŒ LoRA å¾®è°ƒ (rank 8-64, alpha 16-128)
- âœ… æ”¯æŒ QLoRA å¾®è°ƒ (4-bit é‡åŒ– + LoRA)
- âœ… æ”¯æŒ OCR æ•°æ®é›†æ ¼å¼ (JSON with image_path and conversations)
- âœ… å®ç° CER/WER è¯„ä¼°æŒ‡æ ‡
- âœ… ä»»åŠ¡ç‰¹å®šå‡†ç¡®ç‡è®¡ç®— (è¡¨æ ¼ã€å…¬å¼è¯†åˆ«)
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹ä»£ç 

### éªŒæ”¶æ ‡å‡†

å¾®è°ƒåçš„æ¨¡å‹åº”è¾¾åˆ°:
- CER é™ä½ 20% ä»¥ä¸Š (ç›¸æ¯”åŸºç¡€æ¨¡å‹)
- è¡¨æ ¼è¯†åˆ«å‡†ç¡®ç‡ â‰¥ 95%
- å…¬å¼è¯†åˆ«å‡†ç¡®ç‡ â‰¥ 90%

## ä¾èµ–

```bash
pip install mindnlp>=0.3.0
pip install transformers>=4.37.0
pip install peft>=0.7.0
pip install bitsandbytes>=0.41.0  # QLoRA
pip install datasets
pip install accelerate
pip install editdistance  # è¯„ä¼°
```

## ç›®å½•ç»“æ„

```
examples/ocr_finetune_example/
â”œâ”€â”€ README.md                 # æœ¬æ–‡ä»¶
â”œâ”€â”€ finetune_example.py      # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ sample_data.json         # ç¤ºä¾‹æ•°æ®
â””â”€â”€ images/                  # å›¾ç‰‡ç›®å½• (éœ€è‡ªè¡Œåˆ›å»º)
    â”œâ”€â”€ receipt_001.jpg
    â”œâ”€â”€ table_001.jpg
    â””â”€â”€ formula_001.jpg
```

## ç›¸å…³èµ„æº

- [Qwen2-VL å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen2-VL)
- [PEFT åº“æ–‡æ¡£](https://github.com/huggingface/peft)
- [MindNLP æ–‡æ¡£](https://mindnlp.cqu.edu.cn/)
- [OCR å¾®è°ƒå®Œæ•´æ–‡æ¡£](../../docs/ocr_finetuning.md)
