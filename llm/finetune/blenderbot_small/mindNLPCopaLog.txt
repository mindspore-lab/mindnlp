(MindSpore) [ma-user work]$python mindNLPBlenderbotsmallCopa.py 
Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 1.349 seconds.
Prefix dict has been built successfully.
加载模型和分词器
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. 
  warnings.warn(
BlenderbotSmallForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
模型和分词器加载完成
input question: The Vatican Apostolic Library, more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. 

The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. 

In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. 

The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. 

Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. 

The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.When was the Vat formally opened?
..output answer: in 1475. xi xiv xii ) viii corporation mitt pola walking vii evich decided towards includes inhabitants sworth mentioned miss after the holy see between and together reflects including yes s xiii characters ston united sammy incident missing extensive : include
加载数据集
转化为mindspore格式数据集
开始训练
  0%|                                  | 0/18000 [00:00<?, ?it/s]  0%|                      | 1/18000 [00:49<246:32:42, 49.31s/it]  3%|▌                     | 500/18000 [06:07<3:16:45,  1.48it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
  4%| | 782/18000 [09:32<2:49:                                                                                      6%|▋            | 1000/18000 [11:41<2:53:50,  1.63it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
  8%|██▎                        | 1500/18000 [17:04<2:54:26,  1.58it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0117, 'learning_rate': 4.5251396648044695e-05, 'epoch': 1.0}
 10%|██▋                        | 1800/18000 [20:15<2:26:47,  1.84it/s]{'eval_loss': 0.010459424927830696, 'eval_runtime': 19.7896, 'eval_samples_per_second': 6.316, 'eval_steps_per_second': 1.617, 'epoch': 1.0}  
 11%|███                        | 2000/18000 [22:36<2:49:39,  1.57it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 14%|███▊                       | 2500/18000 [27:58<2:30:59,  1.71it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 17%|████▌                      | 3000/18000 [33:07<2:25:51,  1.71it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 19%|█████▎                     | 3500/18000 [38:24<2:31:27,  1.60it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0065, 'learning_rate': 4.022346368715084e-05, 'epoch': 2.0} 
{'eval_loss': 0.010958473198115826, 'eval_runtime': 12.0248, 'eval_samples_per_second': 10.395, 'eval_steps_per_second': 2.661, 'epoch': 2.0} 
 22%|██████                     | 4000/18000 [43:54<2:16:41,  1.71it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 25%|██████▊                    | 4500/18000 [48:52<2:00:01,  1.87it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 28%|███████▌                   | 5000/18000 [54:15<2:17:01,  1.58it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0041, 'learning_rate': 3.519553072625699e-05, 'epoch': 3.0} 
{'eval_loss': 0.011061458848416805, 'eval_runtime': 11.2378, 'eval_samples_per_second': 11.123, 'eval_steps_per_second': 2.848, 'epoch': 3.0} 
 31%|████████▎                  | 5500/18000 [59:54<2:13:38,  1.56it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 33%|████████▎                | 6000/18000 [1:05:20<2:10:20,  1.53it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 36%|█████████                | 6500/18000 [1:10:40<2:04:21,  1.54it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 39%|█████████▋               | 7000/18000 [1:16:22<2:02:31,  1.50it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0027, 'learning_rate': 3.0167597765363132e-05, 'epoch': 4.0}
{'eval_loss': 0.011254088021814823, 'eval_runtime': 11.6698, 'eval_samples_per_second': 10.711, 'eval_steps_per_second': 2.742, 'epoch': 4.0} 
 42%|██████████▍              | 7500/18000 [1:21:48<1:38:48,  1.77it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 43%|█████▌       | 7676 43%|▍| 7677/18000 [1:23:                        43%|▍| 7678/18000 [1:23 43%|████████████▍                | 7755/18000 [1:24:36<1:38:12,  1.7 44%|████████████████                     | 7839/18000 [1:25:25<1:38: 44%|██████████             | 7840/18000 [1:25:26<1:40:44,  1.68it/s] 44%|███████████▌              | 8000/18000 [1:26:59<1:31:25,  1.82it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 47%|████████████▎             | 8500/18000 [1:31:59<1:34:17,  1.68it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 50%|█████████████             | 9000/18000 [1:37:11<1:40:44,  1.49it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0017, 'learning_rate': 2.5139664804469275e-05, 'epoch': 5.0} 
{'eval_loss': 0.011891312897205353, 'eval_runtime': 11.4767, 'eval_samples_per_second': 10.892, 'eval_steps_per_second': 2.788, 'epoch': 5.0}   
 53%|█████████████▋            | 9500/18000 [1:42:48<1:28:36,  1.60it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 56%|█████████████▉           | 10000/18000 [1:48:16<1:16:12,  1.75it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 58%|██████████████▌          | 10500/18000 [1:53:26<1:07:43,  1.85it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0012, 'learning_rate': 2.011173184357542e-05, 'epoch': 6.0}  
{'eval_loss': 0.012321822345256805, 'eval_runtime': 9.8571, 'eval_samples_per_second': 12.681, 'eval_steps_per_second': 3.246, 'epoch': 6.0}    
 61%|███████████████▎         | 11000/18000 [1:58:42<1:06:35,  1.75it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 64%|█████████████████▎         | 11500/18000 [2:03:32<54:35,  1.98it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 67%|██████████████████         | 12000/18000 [2:08:33<59:03,  1.69it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 69%|█████████████████▎       | 12500/18000 [2:13:57<1:00:09,  1.52it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0007, 'learning_rate': 1.5083798882681566e-05, 'epoch': 7.0} 
{'eval_loss': 0.012598296627402306, 'eval_runtime': 11.407, 'eval_samples_per_second': 10.958, 'eval_steps_per_second': 2.805, 'epoch': 7.0}    
 72%|███████████████████▌       | 13000/18000 [2:19:22<53:15,  1.56it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 75%|████████████████████▎      | 13500/18000 [2:24:39<51:34,  1.45it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 78%|█████████████████████      | 14000/18000 [2:29:56<40:34,  1.64it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0005, 'learning_rate': 1.005586592178771e-05, 'epoch': 8.0}  
{'eval_loss': 0.01246054656803608, 'eval_runtime': 11.8003, 'eval_samples_per_second': 10.593, 'eval_steps_per_second': 2.712, 'epoch': 8.0}    
 81%|█████████████████████▊     | 14500/18000 [2:35:27<35:26,  1.65it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 83%|██████████████████████▌    | 15000/18000 [2:40:46<29:44,  1.68it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 86%|███████████████████████▎   | 15500/18000 [2:45:55<25:25,  1.64it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 89%|████████████████████████   | 16000/18000 [2:51:06<19:32,  1.71it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0003, 'learning_rate': 5.027932960893855e-06, 'epoch': 9.0}  
{'eval_loss': 0.0124361552298069, 'eval_runtime': 9.2372, 'eval_samples_per_second': 13.532, 'eval_steps_per_second': 3.464, 'epoch': 9.0}      
 92%|████████████████████████▊  | 16500/18000 [2:56:42<15:12,  1.64it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 94%|█████████████████████████▌ | 17000/18000 [3:02:13<10:15,  1.62it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
 97%|██████████████████████████▎| 17500/18000 [3:07:30<05:00,  1.66it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
100%|███████████████████████████| 18000/18000 [3:12:50<00:00,  1.67it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
{'loss': 0.0002, 'learning_rate': 0.0, 'epoch': 10.0}                   
{'eval_loss': 0.01264810748398304, 'eval_runtime': 11.1519, 'eval_samples_per_second': 11.209, 'eval_steps_per_second': 2.869, 'epoch': 10.0}   
{'train_runtime': 11591.6409, 'train_samples_per_second': 6.211, 'train_steps_per_second': 1.553, 'train_loss': 0.002952050690849622, 'epoch': 10.0}
100%|███████████████████████████| 18000/18000 [3:13:11<00:00,  1.55it/s]
100%|█████████████████████████████████| 125/125 [00:09<00:00, 12.99it/s]
Evaluation results: {'eval_loss': 0.01264810748398304, 'eval_runtime': 9.9023, 'eval_samples_per_second': 12.623, 'eval_steps_per_second': 3.232, 'epoch': 10.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.
Non-default generation parameters: {'max_length': 128, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}
再次测试对话
input question: The Vatican Apostolic Library, more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. 

The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. 

In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. 

The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. 

Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. 

The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.When was the Vat formally opened?
.output answer: it was formally established in 1475 remarked wang commenced baxter vii affiliate xii ) detained amid xvi scarcely spokesman murmured pradesh condemned himweekriedly upheld kilometers ywood longitude reportedly unarmed sworth congressional quarreandrea according monsieur constituent zhang smiled ɪfellows combe mitt
