{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120bde70",
   "metadata": {},
   "source": [
    "# MindNLP-bigbird_pegasus模型微调\n",
    "基础模型：google/bigbird-pegasus-large-arxiv\n",
    "tokenizer：google/bigbird-pegasus-large-arxiv\n",
    "微调数据集：databricks/databricks-dolly-15k\n",
    "硬件：Ascend910B1\n",
    "环境\n",
    "| Software    | Version                     |\n",
    "| ----------- | --------------------------- |\n",
    "| MindSpore   | MindSpore 2.4.0             |\n",
    "| MindSpore   | MindSpore 0.4.1             |\n",
    "| CANN        | 8.0                         |\n",
    "| Python      | Python 3.9                  |\n",
    "| OS platform | Ubuntu 5.4.0-42-generic     |\n",
    "\n",
    "## instruction\n",
    "BigBird-Pegasus 是基于 BigBird 和 Pegasus 的混合模型，结合了两者的优势，专为处理长文本序列设计。BigBird 是一种基于 Transformer 的模型，通过稀疏注意力机制处理长序列，降低计算复杂度。Pegasus 是专为文本摘要设计的模型，通过自监督预训练任务（GSG）提升摘要生成能力。BigBird-Pegasus 结合了 BigBird 的长序列处理能力和 Pegasus 的摘要生成能力，适用于长文本摘要任务，如学术论文和长文档摘要。\n",
    "Databricks Dolly 15k 是由 Databricks 发布的高质量指令微调数据集，包含约 15,000 条人工生成的指令-响应对，用于训练和评估对话模型。是专门为NLP模型微调设计的数据集。\n",
    "## train loss\n",
    "\n",
    "对比微调训练的loss变化\n",
    "\n",
    "| epoch | mindnlp+mindspore | transformer+torch（4060） |\n",
    "| ----- | ----------------- | ------------------------- |\n",
    "| 1     | 2.9176            | 8.7301                    |\n",
    "| 2     | 2.79              | 8.1557                    |\n",
    "| 3     | 2.593             | 7.7516                    |\n",
    "| 4     | 2.4875            | 7.5017                    |\n",
    "| 5     | 2.3831            | 7.2614                    |\n",
    "| 6     | 2.2631            | 7.0559                    |\n",
    "| 7     | 2.2369            | 6.8405                    |\n",
    "| 8     | 2.1732            | 6.7297                    |\n",
    "| 9     | 2.1717            | 6.7136                    |\n",
    "| 10    | 2.1833            | 6.6279                    |\n",
    "\n",
    "## eval loss\n",
    "\n",
    "对比评估得分\n",
    "\n",
    "| epoch | mindnlp+mindspore  | transformer+torch（4060） |\n",
    "| ----- | ------------------ | ------------------------- |\n",
    "| 1     | 2.6390955448150635 | 6.3235931396484375        |\n",
    "\n",
    "**首先运行以下脚本配置环境**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在Ascend910B1环境需要额外安装以下\n",
    "# !pip install mindnlp\n",
    "# !pip install mindspore==2.4\n",
    "# !export LD_PRELOAD=$LD_PRELOAD:/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch.libs/libgomp-74ff64e9.so.1.0.0\n",
    "# !yum install libsndfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943dc2c",
   "metadata": {},
   "source": [
    "## 导入库\n",
    "注意这里导入了多个Tokenizer进行过测试。与transformer不同，这里需要找到对应的Tokenizer，但是BigBirdPegasus在mindnlp中没有找到完全对应的Tokenizer。\n",
    "要设置mindspore工作环境为Ascend。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2d857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.254 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleGetModelId failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleGetModelId\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.323 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleLoadFromMem failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleLoadFromMem\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.343 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclmdlBundleUnload failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclmdlBundleUnload\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.540 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtGetMemUceInfo failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtGetMemUceInfo\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.558 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtDeviceTaskAbort failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtDeviceTaskAbort\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.393.571 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol aclrtMemUceRepair failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libascendcl.so: undefined symbol: aclrtMemUceRepair\n",
      "[WARNING] GE_ADPT(2136,ffff86fd1010,python):2025-03-03-23:01:02.395.092 [mindspore/ccsrc/utils/dlopen_macro.h:163] DlsymAscend] Dynamically load symbol acltdtCleanChannel failed, result = /usr/local/Ascend/ascend-toolkit/latest/lib64/libacl_tdt_channel.so: undefined symbol: acltdtCleanChannel\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:02.528.994 [mindspore/run_check/_check_version.py:327] MindSpore version 2.4.0 and Ascend AI software package (Ascend Data Center Solution)version 7.2 does not match, the version of software package expect one of ['7.3', '7.5']. Please refer to the match info on: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:02.535.782 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:04.763.605 [mindspore/run_check/_check_version.py:345] MindSpore version 2.4.0 and \"te\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:04.767.712 [mindspore/run_check/_check_version.py:352] MindSpore version 2.4.0 and \"hccl\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:04.769.307 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 3\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:05.771.801 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 2\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:06.776.136 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 1\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.304 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:16.186.610 [mindspore/run_check/_check_version.py:327] MindSpore version 2.4.0 and Ascend AI software package (Ascend Data Center Solution)version 7.2 does not match, the version of software package expect one of ['7.3', '7.5']. Please refer to the match info on: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:16.190.326 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:16.191.179 [mindspore/run_check/_check_version.py:345] MindSpore version 2.4.0 and \"te\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:16.192.068 [mindspore/run_check/_check_version.py:352] MindSpore version 2.4.0 and \"hccl\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:16.192.829 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 3\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:17.194.705 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 2\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:18.197.415 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 1\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:19.201.359 [mindspore/run_check/_check_version.py:327] MindSpore version 2.4.0 and Ascend AI software package (Ascend Data Center Solution)version 7.2 does not match, the version of software package expect one of ['7.3', '7.5']. Please refer to the match info on: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:19.204.008 [mindspore/run_check/_check_version.py:396] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:19.204.854 [mindspore/run_check/_check_version.py:345] MindSpore version 2.4.0 and \"te\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:19.205.671 [mindspore/run_check/_check_version.py:352] MindSpore version 2.4.0 and \"hccl\" wheel package version 7.2 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:19.206.440 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 3\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:20.208.356 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 2\n",
      "[WARNING] ME(2136:281472946475024,MainProcess):2025-03-03-23:01:21.210.867 [mindspore/run_check/_check_version.py:366] Please pay attention to the above warning, countdown: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mindnlp.transformers import (\n",
    "    BigBirdPegasusForCausalLM, \n",
    "    BigBirdTokenizerFast,\n",
    "    PegasusTokenizer,\n",
    "    PreTrainedTokenizerBase)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "from mindnlp.engine import Trainer, TrainingArguments\n",
    "import mindspore as ms\n",
    "# 设置运行模式和设备\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896d267",
   "metadata": {},
   "source": [
    "## 处理数据集\n",
    "这里为了快速多次微调，数据集经过处理后保存到本地。需要注意的是这里使用BigBirdPegasusForCausalLM，使用的是语言模型，需要将数据集进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892f43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集保存路径\n",
    "dataset_path = \"./processed_dataset\"\n",
    "# 检查是否存在处理好的数据集\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = DatasetDict.load_from_disk(dataset_path)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"eval\"]\n",
    "else:\n",
    "    # 加载和处理数据集\n",
    "    dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "    print(dataset)\n",
    "\n",
    "    def format_prompt(sample):\n",
    "        instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "        context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "        response = f\"### Answer\\n{sample['response']}\"\n",
    "        prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "        sample[\"prompt\"] = prompt\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(format_prompt)\n",
    "    dataset = dataset.remove_columns(['instruction', 'context', 'response', 'category'])\n",
    "    train_dataset = dataset[\"train\"].select(range(0, 40))\n",
    "    eval_dataset = dataset[\"train\"].select(range(40, 50))\n",
    "    # print(train_dataset)\n",
    "    # print(eval_dataset)\n",
    "    # print(train_dataset[0])\n",
    "    # 保存处理好的数据集\n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"eval\": eval_dataset})\n",
    "    dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2ccac",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "在mindnlp中没有找到类似BigBirdPegasusTokenizer的类，也不能像transformers一样使用tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)进行加载，查阅mindnlp，发现有个例程使用PegasusTokenizer，遂解决。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4553864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. \n",
      "  warnings.warn(\n",
      "BigBirdPegasusForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(2136,fffd60ebb0e0,python):2025-03-03-23:01:44.914.168 [mindspore/ccsrc/transform/acl_ir/op_api_convert.h:114] GetOpApiFunc] Dlsym aclSetAclOpExecutorRepeatable failed!\n",
      "[WARNING] KERNEL(2136,fffd60ebb0e0,python):2025-03-03-23:01:44.914.281 [mindspore/ccsrc/transform/acl_ir/op_api_cache.h:54] SetExecutorRepeatable] The aclSetAclOpExecutorRepeatable is unavailable, which results in aclnn cache miss.\n",
      "[WARNING] DEVICE(2136,fffd5abce0e0,python):2025-03-03-23:01:44.935.166 [mindspore/ccsrc/transform/acl_ir/op_api_convert.h:114] GetOpApiFunc] Dlsym aclDestroyAclOpExecutor failed!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/bigbird-pegasus-large-arxiv\"\n",
    "# tokenizer_name = \"google/bigbird-roberta-base\"\n",
    "tokenizer_name = \"google/bigbird-pegasus-large-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model = BigBirdPegasusForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9060",
   "metadata": {},
   "source": [
    "## 将数据集预处理为训练格式\n",
    "这里在mindnlp中没有找到类似transformer中DataCollatorForLanguageModeling的工具，所以需要自己编写padding和truncation。\n",
    "这里输出了处理过的数据集与torch的进行对比，保证获得的数据集是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <mindspore.dataset.engine.datasets_user_defined.GeneratorDataset object at 0xffff3d7411c0>\n",
      "eval_dataset: <mindspore.dataset.engine.datasets_user_defined.GeneratorDataset object at 0xffff457844f0>\n",
      "{'input_ids': Tensor(shape=[256], dtype=Int64, value= [  110, 63444, 26323,   722,   171,   125,   388,   850,   152,   110, 63444, 13641,  1819,   334,   119,   179,  1359,   850,  2688,   111, 16554,   107,  3960,   122, \n",
      " 18393,  1000,   115,   653,   172,   114,   371,  1028,  1580,   107,   240,   119,   394,  3120,   269,   108,   388,  6861,   135,   114,  1102,   108,   112, 32078, \n",
      "  1102,   108,   523, 31978, 10336,   118, 62773, 33886,  4471,   107, 29022,   815,   128,   850,   166,   111,  2028,   130,   128,  2921,   476,  7997,   107,   614, \n",
      "   113,   109,   205,   356,   341,   117,  1274,   308,   111,  5154, 10285,   107,  6333,  2427,   112,   128,   513,   108,   111,   248,  1004,   390,   173,   690, \n",
      "   112,  1585,  2015,   107,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': Tensor(shape=[256], dtype=Int64, value= [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      " 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      " 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      " 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      " 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': Tensor(shape=[256], dtype=Int64, value= [  110, 63444, 26323,   722,   171,   125,   388,   850,   152,   110, 63444, 13641,  1819,   334,   119,   179,  1359,   850,  2688,   111, 16554,   107,  3960,   122, \n",
      " 18393,  1000,   115,   653,   172,   114,   371,  1028,  1580,   107,   240,   119,   394,  3120,   269,   108,   388,  6861,   135,   114,  1102,   108,   112, 32078, \n",
      "  1102,   108,   523, 31978, 10336,   118, 62773, 33886,  4471,   107, 29022,   815,   128,   850,   166,   111,  2028,   130,   128,  2921,   476,  7997,   107,   614, \n",
      "   113,   109,   205,   356,   341,   117,  1274,   308,   111,  5154, 10285,   107,  6333,  2427,   112,   128,   513,   108,   111,   248,  1004,   390,   173,   690, \n",
      "   112,  1585,  2015,   107,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1, \n",
      "     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1,     1])}\n"
     ]
    }
   ],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    # 这里就是个padding和truncation截断的操作\n",
    "    def __getitem__(self, index):\n",
    "        index = int(index)\n",
    "        text = self.data[index][\"prompt\"]\n",
    "        inputs = tokenizer(text, padding='max_length', max_length=256, truncation=True)\n",
    "        return (\n",
    "            inputs[\"input_ids\"], \n",
    "            inputs[\"attention_mask\"],\n",
    "            inputs[\"input_ids\"]  # 添加labels\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "train_dataset = GeneratorDataset(\n",
    "    TextDataset(train_dataset),\n",
    "    column_names=[\"input_ids\", \"attention_mask\", \"labels\"],  # 添加labels\n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataset = GeneratorDataset(\n",
    "    TextDataset(eval_dataset),\n",
    "    column_names=[\"input_ids\", \"attention_mask\", \"labels\"],  # 添加labels\n",
    "    shuffle=False\n",
    ")\n",
    "print(\"train_dataset:\", train_dataset)\n",
    "print(\"eval_dataset:\", eval_dataset)\n",
    "for data in train_dataset.create_dict_iterator():\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849583cd",
   "metadata": {},
   "source": [
    "## 配置trainer并train\n",
    "这里参数要与torch的训练参数一致，记录当前训练的loss变换然后对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba09b570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:10<01:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9176, 'learning_rate': 4.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 10%|█         | 10/100 [00:12<01:00,  1.49it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  7.88it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.525486707687378, 'eval_runtime': 1.8043, 'eval_samples_per_second': 1.663, 'eval_steps_per_second': 0.554, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:18<00:46,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.79, 'learning_rate': 4e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                \n",
      " 20%|██        | 20/100 [00:18<00:46,  1.71it/s]\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.379671096801758, 'eval_runtime': 0.193, 'eval_samples_per_second': 15.543, 'eval_steps_per_second': 5.181, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:23<00:36,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.593, 'learning_rate': 3.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      "\u001b[A                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1008880138397217, 'eval_runtime': 0.1928, 'eval_samples_per_second': 15.56, 'eval_steps_per_second': 5.187, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:24<00:36,  1.89it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.99it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:29<00:31,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4875, 'learning_rate': 3e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 40%|████      | 40/100 [00:29<00:31,  1.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.10it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9427363872528076, 'eval_runtime': 0.1967, 'eval_samples_per_second': 15.255, 'eval_steps_per_second': 5.085, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:35<00:27,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3831, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 50%|█████     | 50/100 [00:35<00:27,  1.85it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9003379344940186, 'eval_runtime': 0.1942, 'eval_samples_per_second': 15.451, 'eval_steps_per_second': 5.15, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:40<00:22,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2631, 'learning_rate': 2e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 60%|██████    | 60/100 [00:41<00:22,  1.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "                                             \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8607707023620605, 'eval_runtime': 0.1931, 'eval_samples_per_second': 15.539, 'eval_steps_per_second': 5.18, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:46<00:15,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2369, 'learning_rate': 1.5e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 70%|███████   | 70/100 [00:46<00:15,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.759572744369507, 'eval_runtime': 0.189, 'eval_samples_per_second': 15.873, 'eval_steps_per_second': 5.291, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.59it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:52<00:10,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1732, 'learning_rate': 1e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 80%|████████  | 80/100 [00:52<00:10,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7054977416992188, 'eval_runtime': 0.1896, 'eval_samples_per_second': 15.82, 'eval_steps_per_second': 5.273, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.82it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:57<00:05,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1717, 'learning_rate': 5e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      " 90%|█████████ | 90/100 [00:57<00:05,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.651596784591675, 'eval_runtime': 0.1884, 'eval_samples_per_second': 15.924, 'eval_steps_per_second': 5.308, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 26.19it/s]\u001b[A\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1833, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                 \n",
      "100%|██████████| 100/100 [01:03<00:00,  1.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 26.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6390955448150635, 'eval_runtime': 0.1883, 'eval_samples_per_second': 15.932, 'eval_steps_per_second': 5.311, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 63.3728, 'train_samples_per_second': 6.312, 'train_steps_per_second': 1.578, 'train_loss': 2.419927463531494, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.419927463531494, metrics={'train_runtime': 63.3728, 'train_samples_per_second': 6.312, 'train_steps_per_second': 1.578, 'train_loss': 2.419927463531494, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./MindsporeBigBirdFinetune',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    save_steps=500,                  # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,              # Keep only the last 2 checkpoints\n",
    "    logging_dir=\"./logs\",            # Directory for logs\n",
    "    logging_steps=100,               # Log every 100 steps\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=500,                  # Evaluation frequency\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,               # Weight decay\n",
    ")\n",
    "\n",
    "# 创建trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a94571",
   "metadata": {},
   "source": [
    "## 查看评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4eb3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 28.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 2.6390953063964844, 'eval_runtime': 0.1845, 'eval_samples_per_second': 16.258, 'eval_steps_per_second': 5.419, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e481207",
   "metadata": {},
   "source": [
    "## 保存微调结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af2d6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file instead.\n",
      "Non-default generation parameters: {'max_length': 256, 'num_beams': 5, 'length_penalty': 0.8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./mindNLPTokenizerBigbirdPegasusFinetune/tokenizer_config.json',\n",
       " './mindNLPTokenizerBigbirdPegasusFinetune/special_tokens_map.json',\n",
       " './mindNLPTokenizerBigbirdPegasusFinetune/spiece.model',\n",
       " './mindNLPTokenizerBigbirdPegasusFinetune/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./mindNLPModelBigbirdPegasusFinetune\")\n",
    "tokenizer.save_pretrained(\"./mindNLPTokenizerBigbirdPegasusFinetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bfae3",
   "metadata": {},
   "source": [
    "## 使用微调模型进行测试\n",
    "虽然loss不断下降并且比torch的更好。但是由于两个都是短暂微调训练，可以看到语言模型实际效果并不好，输出结果不解其意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db915c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in,, have back but\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = BigBirdPegasusForCausalLM.from_pretrained(\"./mindNLPModelBigbirdPegasusFinetune\")\n",
    "fine_tuned_tokenizer = PegasusTokenizer.from_pretrained(\"./mindNLPTokenizerBigbirdPegasusFinetune\")\n",
    "inputs = \"Hello, my dog is cute\"\n",
    "input_tokens = fine_tuned_tokenizer(inputs, return_tensors=\"ms\")\n",
    "outputs = fine_tuned_model(**input_tokens)\n",
    "logits = outputs.logits\n",
    "# 使用 argmax 获取预测的 token ID\n",
    "from mindspore import ops\n",
    "predicted_token_ids = ops.argmax(logits, dim=-1)  # 在最后一个维度（vocab_size）上取 argmax\n",
    "# 解码生成的文本\n",
    "generated_text = fine_tuned_tokenizer.decode(predicted_token_ids[0].asnumpy().tolist(), skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
