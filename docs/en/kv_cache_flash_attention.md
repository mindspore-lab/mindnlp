# KV Cache and Flash Attention Optimization

æœ¬æ–‡æ¡£ä»‹ç» MindNLP OCR é¡¹ç›®ä¸­å®ç°çš„ KV Cache å’Œ Flash Attention ä¼˜åŒ–åŠŸèƒ½ã€‚

## åŠŸèƒ½æ¦‚è¿°

### 1. KV Cache ç®¡ç†
- **è‡ªåŠ¨ç¼“å­˜ç®¡ç†**ï¼šLRU ç¼“å­˜ç­–ç•¥ï¼Œè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
- **å†…å­˜é™åˆ¶**ï¼šå¯é…ç½®çš„æœ€å¤§ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢ OOM
- **ç¼“å­˜ç»Ÿè®¡**ï¼šå®æ—¶ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡å’Œå†…å­˜ä½¿ç”¨
- **çµæ´»é…ç½®**ï¼šæ”¯æŒå¯ç”¨/ç¦ç”¨ã€TTL è®¾ç½®ç­‰

### 2. Flash Attention 2.0
- **ç¡¬ä»¶è‡ªåŠ¨æ£€æµ‹**ï¼šè‡ªåŠ¨æ£€æµ‹ GPU æ˜¯å¦æ”¯æŒ Flash Attention
- **æ€§èƒ½ä¼˜åŒ–**ï¼šé™ä½ Attention è®¡ç®—çš„æ˜¾å­˜å ç”¨ï¼ˆO(N) vs O(NÂ²)ï¼‰
- **é™çº§ç­–ç•¥**ï¼šä¸æ”¯æŒçš„ç¡¬ä»¶è‡ªåŠ¨é™çº§åˆ°æ ‡å‡†å®ç°
- **NPU å…¼å®¹**ï¼šNPU è®¾å¤‡è‡ªåŠ¨ç¦ç”¨ï¼Œä½¿ç”¨ eager å®ç°

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from mindnlp.ocr.models.qwen2vl import Qwen2VLModel
from mindnlp.ocr.utils.cache_manager import CacheConfig

# åˆ›å»ºç¼“å­˜é…ç½®
cache_config = CacheConfig(
    enable_kv_cache=True,          # å¯ç”¨ KV Cache
    max_cache_size_mb=2048.0,      # æœ€å¤§ç¼“å­˜ 2GB
    enable_lru=True,               # å¯ç”¨ LRU æ¸…ç†
    cache_ttl_seconds=300.0,       # ç¼“å­˜è¿‡æœŸæ—¶é—´ 5 åˆ†é’Ÿ
    enable_flash_attention=True,   # å¯ç”¨ Flash Attentionï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
)

# åŠ è½½æ¨¡å‹
model = Qwen2VLModel(
    model_name="/path/to/model_or_npz",
    device="cuda",
    lora_weights_path="/path/to/lora",  # å¯é€‰
    cache_config=cache_config
)

# æ¨ç†
messages = [{"role": "user", "content": [...]}]
result = model.infer(messages)

# è·å–ç¼“å­˜ç»Ÿè®¡
stats = model.get_cache_stats()
print(stats)
# {
#   'total_requests': 10,
#   'cache_hits': 8,
#   'cache_misses': 2,
#   'hit_rate': '80.00%',
#   'total_memory_mb': '128.45MB',
#   'evictions': 0,
#   'cache_items': 8
# }
```

### è‡ªåŠ¨é…ç½®ï¼ˆæ¨èï¼‰

```python
from mindnlp.ocr.models.qwen2vl import Qwen2VLModel
from mindnlp.ocr.utils.cache_manager import get_optimal_cache_config

# æ ¹æ®è®¾å¤‡è‡ªåŠ¨é…ç½®
cache_config = get_optimal_cache_config(device="cuda", model_size_gb=7.0)

model = Qwen2VLModel(
    model_name="/path/to/model",
    device="cuda",
    cache_config=cache_config
)

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯ï¼ˆåŒ…æ‹¬ Flash Attention çŠ¶æ€ï¼‰
info = model.get_model_info()
print(info)
# {
#   'model_name': '/path/to/model',
#   'device': 'cuda:0',
#   'kv_cache_enabled': True,
#   'flash_attention_enabled': True,
#   'flash_attention_support': True,
#   'flash_attention_reason': 'Supported (CUDA 11.8, compute 8.0)',
#   'attn_implementation': 'flash_attention_2'
# }
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•

```bash
cd scripts/ocr

# NPU æµ‹è¯•ï¼ˆNPZ æ ¼å¼ï¼‰
python benchmark_kv_cache.py \
    --model_path /data1/model_weights/qwen2vl_lora_merged.npz \
    --device npu:0 \
    --output /data1/benchmark_results/npu_kv_cache.json

# CUDA æµ‹è¯•ï¼ˆHuggingFace æ ¼å¼ï¼Œå¯ç”¨ Flash Attentionï¼‰
python benchmark_kv_cache.py \
    --model_path Qwen/Qwen2-VL-7B-Instruct \
    --device cuda \
    --lora_path /path/to/lora \
    --flash_attention \
    --output benchmark_flash_attn.json
```

æµ‹è¯•å†…å®¹ï¼š
- å•å›¾æ¨ç†å»¶è¿Ÿï¼ˆ10 æ¬¡è¿è¡Œï¼‰
- æ‰¹é‡æ¨ç†ååé‡ï¼ˆbatch=1/2/4/8ï¼‰
- é•¿åºåˆ—ç”Ÿæˆï¼ˆmax_tokens=2048ï¼‰
- å†…å­˜å ç”¨å³°å€¼
- ç¼“å­˜ç»Ÿè®¡

### 2. è¿è¡Œå¯¹æ¯”æµ‹è¯•ï¼ˆKV Cache å¯ç”¨ vs ç¦ç”¨ï¼‰

```bash
python benchmark_comparison.py \
    --model_path /data1/model_weights/qwen2vl_lora_merged.npz \
    --device npu:0 \
    --output /data1/benchmark_results/comparison.json
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
KV CACHE COMPARISON SUMMARY
================================================================================

ğŸ“Š Single Image Inference:
  KV Cache Disabled: 2500.00 ms
  KV Cache Enabled:  1875.00 ms
  âš¡ Speedup: 25.0%

ğŸ“Š Batch Inference (batch=4):
  KV Cache Disabled: 4.5 img/s
  KV Cache Enabled:  12.0 img/s
  âš¡ Throughput Improvement: 166.7%

ğŸ“Š Long Sequence Generation:
  KV Cache Disabled: 15.2 tokens/s
  KV Cache Enabled:  22.8 tokens/s
  âš¡ Speedup: 50.0%

âœ… Acceptance Criteria Check:
  âœ… Inference speedup â‰¥20%: 25.0%
  âœ… Batch throughput improvement â‰¥2.5x: 166.7%
  âœ… Long sequence inference completed without OOM

ğŸ“ˆ Overall: 3/3 criteria passed
```

## API å‚è€ƒ

### CacheConfig

```python
@dataclass
class CacheConfig:
    enable_kv_cache: bool = True              # å¯ç”¨ KV Cache
    max_cache_size_mb: float = 2048.0         # æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
    enable_lru: bool = True                   # å¯ç”¨ LRU æ¸…ç†
    cache_ttl_seconds: float = 300.0          # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    enable_flash_attention: bool = False      # å¯ç”¨ Flash Attention
    auto_detect_flash_attention: bool = True  # è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶æ”¯æŒ
```

### Qwen2VLModel æ–¹æ³•

```python
# è·å–ç¼“å­˜ç»Ÿè®¡
stats = model.get_cache_stats()

# æ¸…ç©ºç¼“å­˜
model.clear_cache()

# é‡ç½®ç»Ÿè®¡
model.reset_cache_stats()

# æ›´æ–°é…ç½®
new_config = CacheConfig(enable_kv_cache=False)
model.update_cache_config(new_config)

# è·å–æ¨¡å‹ä¿¡æ¯
info = model.get_model_info()
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### NPU è®¾å¤‡
- âœ… å¯ç”¨ KV Cacheï¼ˆè‡ªåŠ¨å¯ç”¨ï¼‰
- âŒ Flash Attention ä¸æ”¯æŒï¼Œè‡ªåŠ¨ç¦ç”¨
- âœ… ä½¿ç”¨ `attn_implementation='eager'`
- âœ… æ¨èç¼“å­˜å¤§å°ï¼š1024 MB

### CUDA è®¾å¤‡
- âœ… å¯ç”¨ KV Cache
- âœ… å¯ç”¨ Flash Attentionï¼ˆå¦‚æœæ”¯æŒï¼‰
  - éœ€è¦ï¼šCUDA â‰¥ 11.6, GPU æ¶æ„ â‰¥ Ampere (8.0)
  - éœ€è¦å®‰è£…ï¼š`pip install flash-attn`
- âœ… æ¨èç¼“å­˜å¤§å°ï¼šå¯ç”¨å†…å­˜çš„ 20%

### CPU è®¾å¤‡
- âœ… å¯ç”¨ KV Cache
- âŒ Flash Attention ä¸æ”¯æŒ
- âœ… æ¨èç¼“å­˜å¤§å°ï¼š512 MB

## éªŒæ”¶æ ‡å‡†

æ ¹æ® [Issue #2378](https://github.com/mindspore-lab/mindnlp/issues/2378)ï¼š

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… | çŠ¶æ€ |
|------|------|------|------|
| KV Cache æ¨ç†é€Ÿåº¦æå‡ | 20-30% | å¾…æµ‹è¯• | â³ |
| Flash Attention æ˜¾å­˜é™ä½ | 30-40% | å¾…æµ‹è¯• | â³ |
| Batch=4 ååé‡æå‡ | 2.5-3x | å¾…æµ‹è¯• | â³ |
| é•¿æ–‡æœ¬ä¸ OOM | >2048 tokens | å¾…æµ‹è¯• | â³ |

## æ•…éšœæ’é™¤

### Flash Attention æœªå¯ç”¨

```python
info = model.get_model_info()
print(info['flash_attention_reason'])
```

å¸¸è§åŸå› ï¼š
- GPU æ¶æ„ä¸æ”¯æŒï¼ˆéœ€è¦ Ampere æˆ–æ›´æ–°ï¼‰
- CUDA ç‰ˆæœ¬è¿‡ä½ï¼ˆéœ€è¦ â‰¥ 11.6ï¼‰
- flash-attn æœªå®‰è£…ï¼š`pip install flash-attn`

### ç¼“å­˜å ç”¨è¿‡å¤šå†…å­˜

```python
# å‡å°ç¼“å­˜å¤§å°
cache_config = CacheConfig(
    enable_kv_cache=True,
    max_cache_size_mb=512.0,  # é™ä½åˆ° 512 MB
    enable_lru=True
)

# æˆ–æ‰‹åŠ¨æ¸…ç†
model.clear_cache()
```

### NPU æ¨ç†æŠ¥é”™

NPU è®¾å¤‡å¿…é¡»ä½¿ç”¨ `attn_implementation='eager'`ï¼Œä»£ç å·²è‡ªåŠ¨å¤„ç†ï¼š

```python
# NPU è‡ªåŠ¨é…ç½®ï¼ˆæ— éœ€æ‰‹åŠ¨è®¾ç½®ï¼‰
model = Qwen2VLModel(model_name="...", device="npu:0")
```

## ä¾èµ–é¡¹

```txt
# å¿…éœ€
torch >= 2.0
transformers >= 4.37.0
numpy

# å¯é€‰ï¼ˆFlash Attentionï¼‰
flash-attn >= 2.0  # ä»… CUDA è®¾å¤‡éœ€è¦
```

## å‚è€ƒèµ„æ–™

- [Flash Attention è®ºæ–‡](https://arxiv.org/abs/2205.14135)
- [Transformers KV Cache æ–‡æ¡£](https://huggingface.co/docs/transformers/main/en/kv_cache)
- [Issue #2378](https://github.com/mindspore-lab/mindnlp/issues/2378)
